conf_name: conf1
run_name: run1 # Variable per run of the same config
algo:
  name: ppo
  runner: rlkit.runners.ppo.PPORunner

# 1. Environment
defaults:
  - env: 3dball
  - info
  - _self_

env:
  params:
    time_scale: 10

# 2. Agent
agent:
  builder: rlkit.runners.ppo.PPOAgentBuilder
  policy_net:
    _target_: rlkit.models.MLP
    in_features: ${env.observation.dim}
    out_features: ${eval:'2 * ${env.action.dim}'}
    n_blocks: 1
    hidden_dim: 8
  value_net:
    _target_: rlkit.models.MLP
    in_features: ${env.observation.dim}
    out_features: 1
    n_blocks: 1
    hidden_dim: 8

# 3. Loss
loss:
  builder: rlkit.runners.ppo.PPOLossBuilder
  loss: torchrl.objectives.ClipPPOLoss
  loss_params:
    epsilon: 0.2
    entropy_coeff: 1e-2
  value_estimator_params:
    gamma: 0.99
    lmbda: 0.95

# 4. Trainer
trainer:
  builder: rlkit.runners.ppo.PPOTrainerBuilder
  params:
    generations: 100
    generation_size: 6_000
    slice_len: 128
    n_slices: 1024
    epochs: 5
    minibatch_size: 256

    workers: 5
    env_batch_dim: ${env.info.batch_dims}

    kl_soft_clip: 0.02
    early_stop_threshold: 32
    kl_hard_clip: 0.05

# 5. State
state:
  builder: rlkit.runners.ppo.PPOStateBuilder
  components:
    optimizer:
      _target_: torch.optim.Adam
      lr: ${state.components.lr_scheduler.initial_lr}

    logger:
      _target_: rlkit.utils.HFTBLogger
      log_dir: ${dir}/logs/${run_name}
      repo_id: ${repo_id}
      repo_subfolder: ${dir}/logs/${run_name}

    checkpointer:
      _target_: rlkit.utils.HFCheckpointer
      ckpt_path: ${dir}/ckpts
      name: ${run_name}
      repo_id: ${repo_id}

    lr_scheduler:
      _target_: rlkit.utils.CosineWithLinearWarmupSchedule
      warmup_epochs: 5
      total_epochs: ${trainer.params.generations}
      initial_lr: 1e-5
      max_lr: 1e-3

# Change working directory
dir: experiments/${env.name}/${algo.name}/${conf_name}
hydra:
  output_subdir: config
  run:
    dir: ${dir}
model_path: ${dir}/models/${run_name}.pt
results_path: ${dir}/results/${run_name}.png