conf_name: conf1
run_name: run1 # Variable per run of the same config

# 1. Environment
defaults:
  - env: walker
  - info
  - _self_

env:
  params:
    time_scale: 10

# 2. Algorithm
algo:
  name: ppo
  _target_: runners.ppo.PPORunner
  params:
    epsilon: 0.2
    entropy_coef: 1e-2
    gamma: 0.99

trainer:
  _target_: rlkit.templates.PPOBasic
  params:
    generations: 20_000
    generation_size: 80_000
    slice_len: 1000
    n_slices: 1000
    epochs: 10
    minibatch_size: 1024

    workers: 8
    env_batch_dim: ${env.info.batch_dims}

    kl_soft_clip: 0.02
    early_stop_threshold: 32
    kl_hard_clip: 0.05

# 3. Model
model:
  _target_: rlkit.models.MLP
  params:
    in_features: "${env.observation.dim}"
    out_features: "${env.action.dim}"
    n_blocks: 1
    hidden_dim: 128

# 4. State
optimizer:
  _target_: torch.optim.Adam
  params:
    lr: 1e-7 # Matches initial lr schedule


### Utility

# Change working directory
dir: experiments/${env.name}/${algo.name}/${conf_name}
hydra:
  output_subdir: config
  run:
    dir: ${dir}
model_path: ${dir}/models/${run_name}.pt
results_path: ${dir}/results/${run_name}.png

# Logger
logger:
  _target_: rlkit.utils.HFTBLogger
  log_dir: ${dir}/logs/${run_name}
  repo_id: ${repo_id}
  repo_subfolder: ${dir}/logs/${run_name}

# Checkpointer
checkpointer:
  _target_: rlkit.utils.HFCheckpointer
  ckpt_path: ${dir}/ckpts
  name: ${run_name}
  repo_id: ${repo_id}

# LR Scheduler
lr_scheduler:
  _target_: rlkit.utils.CosineWithLinearWarmupSchedule
  warmup_epochs: 5
  initial_lr: 1e-7
  max_lr: 3e-5
