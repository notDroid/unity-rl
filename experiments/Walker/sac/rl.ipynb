{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Walker SAC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), torch.float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rlkit.util import MultiVersionCheckpointer, Checkpointer, Logger, SimpleMetricModule, Stopwatch\n",
    "from config import *\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from env import create_env\n",
    "\n",
    "# Import Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchinfo import summary\n",
    "\n",
    "# Models and Loss\n",
    "from rlkit.models import MLP, CatWrapper\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "# Data\n",
    "from tensordict import TensorDict\n",
    "from torchrl.collectors import SyncDataCollector, MultiaSyncDataCollector, aSyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "# Util\n",
    "from torchrl.objectives import SACLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if str(device).startswith(\"cuda\") else \"cpu\"\n",
    "amp_dtype   = torch.float16 if device_type == \"cuda\" else torch.float32\n",
    "device, amp_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(model_config):\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"out_features\"] *= 2\n",
    "    model = MLP(**model_config)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        model,\n",
    "        NormalParamExtractor()\n",
    "    )\n",
    "    model = TensorDictModule(model, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "    \n",
    "    policy = ProbabilisticActor(\n",
    "        module=model,  \n",
    "        distribution_class=TanhNormal,\n",
    "\n",
    "        in_keys=[\"loc\", \"scale\"],\n",
    "        out_keys=[\"action\"],\n",
    "\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=\"log_prob\",\n",
    "        cache_dist=True,\n",
    "    )\n",
    "\n",
    "    return policy\n",
    "\n",
    "def create_qvalue(model_config):\n",
    "    # Remove out_features from config\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"in_features\"] = model_config[\"in_features\"] + model_config[\"out_features\"]\n",
    "    model_config[\"out_features\"] = 1\n",
    "\n",
    "    model = MLP(**model_config)\n",
    "    model = CatWrapper(model)\n",
    "    qvalue = TensorDictModule(model, in_keys=[\"observation\", \"action\"], out_keys=[\"state_action_value\"])\n",
    "    return qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = create_policy(MODEL_CONFIG)\n",
    "qvalue = create_qvalue(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TensorDictModule                              [1, 1]                    --\n",
       "├─CatWrapper: 1-1                             [1, 1]                    --\n",
       "│    └─MLP: 2-1                               [1, 1]                    --\n",
       "│    │    └─Linear: 3-1                       [1, 128]                  36,224\n",
       "│    │    └─ModuleList: 3-2                   --                        393,216\n",
       "│    │    └─Sequential: 3-3                   [1, 1]                    257\n",
       "===============================================================================================\n",
       "Total params: 516,993\n",
       "Trainable params: 516,993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.43\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 1.72\n",
       "Estimated Total Size (MB): 1.74\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = (torch.randn(1, MODEL_CONFIG[\"in_features\"]), torch.randn(1, MODEL_CONFIG[\"out_features\"]))\n",
    "summary(qvalue, input_data=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TensorDictModule                              [1, 39]                   --\n",
       "├─Sequential: 1-1                             [1, 39]                   --\n",
       "│    └─MLP: 2-1                               [1, 78]                   --\n",
       "│    │    └─Linear: 3-1                       [1, 128]                  31,232\n",
       "│    │    └─ModuleList: 3-2                   --                        393,216\n",
       "│    │    └─Sequential: 3-3                   [1, 78]                   10,190\n",
       "│    └─NormalParamExtractor: 2-2              [1, 39]                   --\n",
       "│    │    └─biased_softplus: 3-4              [1, 39]                   --\n",
       "===============================================================================================\n",
       "Total params: 521,934\n",
       "Trainable params: 521,934\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.43\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 1.74\n",
       "Estimated Total Size (MB): 1.76\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = torch.randn(1 ,MODEL_CONFIG[\"in_features\"])\n",
    "summary(policy.module[0], input_data=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_module = SACLoss(\n",
    "#     actor_network=policy, qvalue_network=qvalue, value_network=None,\n",
    "#     num_qvalue_nets=2,\n",
    "#     alpha_init=0.1, fixed_alpha=True, \n",
    "#     delay_actor=False, delay_qvalue=True\n",
    "# )\n",
    "# target_updater = SoftUpdate(loss_module, tau=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: env.close()\n",
    "# except: pass\n",
    "# env = create_env(graphics=False, time_scale=10)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     data = env.rollout(100, policy=policy, break_when_any_done=False)\n",
    "# data, loss_module(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SCALE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS = os.cpu_count() // 2\n",
    "TIMESTEPS = 100_000_000\n",
    "BUFFER_SIZE = 3_000_000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# ENTROPY_TARGET = 1e-3\n",
    "ENTROPY_COEF = 3e-4\n",
    "# print(\"Entropy Target:\", ENTROPY_TARGET)\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "\n",
    "LR = 1e-5\n",
    "\n",
    "TRAIN_STEPS = 512\n",
    "ENV_STEPS = 1000 * WORKERS\n",
    "# Steps = N*K, train_iters = K. K helps with buffering async collected data with less stalls\n",
    "\n",
    "CKPT_EVAL_INTERVAL = 100\n",
    "EVAL_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'run0'\n",
    "CONTINUE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = create_policy(MODEL_CONFIG).to(device)\n",
    "qvalue = create_qvalue(MODEL_CONFIG).to(device)\n",
    "\n",
    "loss_module = SACLoss(\n",
    "    actor_network=policy, qvalue_network=qvalue, value_network=None,\n",
    "    num_qvalue_nets=2,\n",
    "    alpha_init=ENTROPY_COEF, fixed_alpha=True,\n",
    "    # target_entropy=ENTROPY_TARGET, fixed_alpha=False,\n",
    "    delay_actor=False, delay_qvalue=True\n",
    ")\n",
    "loss_module.make_value_estimator(gamma=GAMMA)\n",
    "target_updater = SoftUpdate(loss_module, tau=TAU)\n",
    "\n",
    "optimizer = optim.Adam(loss_module.parameters(), lr=LR)\n",
    "\n",
    "replay_buffer = TensorDictReplayBuffer(\n",
    "    storage = LazyMemmapStorage(max_size=BUFFER_SIZE, ndim=2),\n",
    "    batch_size = BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint at timestep: 72555000.0\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(keys = LOG_KEYS, log_path=LOG_PATH, name=NAME)\n",
    "checkpointer = MultiVersionCheckpointer(ckpt_path=CKPT_PATH, name=NAME, metric_key=\"score\", levels=10, base_interval=10, interval_scale=1.75)\n",
    "\n",
    "# Continue/Reset\n",
    "start_timestep = 0\n",
    "if not CONTINUE:\n",
    "    logger.reset()\n",
    "    checkpointer.reset()\n",
    "else:\n",
    "    checkpoint = checkpointer.load_progress()\n",
    "    if checkpoint:\n",
    "        start_timestep = checkpoint['timestep']\n",
    "        print(f\"Found checkpoint at timestep: {start_timestep}\")\n",
    "        logger.revert(\"timestep\", start_timestep)\n",
    "        loss_module.load_state_dict(checkpoint[\"loss_module\"], strict=False)\n",
    "    else:\n",
    "        print(\"Checkpoint not found, restarting\")\n",
    "        logger.reset()\n",
    "loss_module.log_alpha = torch.tensor(math.log(ENTROPY_COEF), device=device)\n",
    "loss_module._value_estimator.gamma = torch.tensor(GAMMA, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/collectors/collectors.py:2137: UserWarning: total_frames (27445000.0) is not exactly divisible by frames_per_batch (7000). This means 2000.0 additional frames will be collected. To silence this message, set the environment variable RL_WARNINGS to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "Forcing GfxDevice: Null\n",
      "Forcing GfxDevice: Null\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "Forcing GfxDevice: Null\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "NullGfxDevice:\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "NullGfxDevice:\n",
      "    Renderer: Null Device\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Vendor:   Unity Technologies\n",
      "    Vendor:   Unity Technologies\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Loaded All Assemblies, in  0.104 seconds\n",
      "- Loaded All Assemblies, in  0.104 seconds\n",
      "- Loaded All Assemblies, in  0.104 seconds\n",
      "- Loaded All Assemblies, in  0.104 seconds\n",
      "- Loaded All Assemblies, in  0.104 seconds\n",
      "- Loaded All Assemblies, in  0.104 seconds\n",
      "- Loaded All Assemblies, in  0.105 seconds\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Finished resetting the current domain, in  0.002 seconds\n",
      "- Finished resetting the current domain, in  0.002 seconds\n",
      "- Finished resetting the current domain, in  0.002 seconds\n",
      "- Finished resetting the current domain, in  0.002 seconds\n",
      "- Finished resetting the current domain, in  0.002 seconds\n",
      "- Finished resetting the current domain, in  0.002 seconds\n",
      "- Finished resetting the current domain, in  0.002 seconds\n",
      "ERROR: Shader ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.373459 ms\n",
      "UnloadTime: 0.339000 ms\n",
      "UnloadTime: 0.275791 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.283833 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.307584 ms\n",
      "UnloadTime: 0.304708 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.309917 ms\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Thread 0x171953000 may have been prematurely finalized\n",
      "Thread 0x17152b000 may have been prematurely finalized\n",
      "Thread 0x171a8b000 may have been prematurely finalized\n",
      "Thread 0x16d8b3000 may have been prematurely finalized\n",
      "Thread 0x1712c3000 may have been prematurely finalized\n",
      "Thread 0x1711c3000 may have been prematurely finalized\n",
      "Thread 0x16f7db000 may have been prematurely finalized\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Thread 0x17029b000 may have been prematurely finalized\n",
      "Thread 0x170327000 may have been prematurely finalized\n",
      "Thread 0x1703b3000 may have been prematurely finalized\n",
      "Thread 0x17043f000 may have been prematurely finalized\n",
      "Thread 0x17066f000 may have been prematurely finalized\n",
      "Thread 0x1706fb000 may have been prematurely finalized\n",
      "Thread 0x170557000 may have been prematurely finalized\n",
      "Thread 0x17089f000 may have been prematurely finalized\n",
      "Thread 0x170787000 may have been prematurely finalized\n",
      "Thread 0x1705e3000 may have been prematurely finalized\n",
      "Thread 0x170813000 may have been prematurely finalized\n",
      "Thread 0x1704cb000 may have been prematurely finalized\n",
      "Thread 0x17092b000 may have been prematurely finalized\n",
      "Thread 0x16e1af000 may have been prematurely finalized\n",
      "Thread 0x16e2c7000 may have been prematurely finalized\n",
      "Thread 0x16e583000 may have been prematurely finalized\n",
      "Thread 0x16e46b000 may have been prematurely finalized\n",
      "Thread 0x16e3df000 may have been prematurely finalized\n",
      "Thread 0x16e60f000 may have been prematurely finalized\n",
      "Thread 0x16e353000 may have been prematurely finalized\n",
      "Thread 0x16e7b3000 may have been prematurely finalized\n",
      "Thread 0x16e83f000 may have been prematurely finalized\n",
      "Thread 0x16e69b000 may have been prematurely finalized\n",
      "Thread 0x16e4f7000 may have been prematurely finalized\n",
      "Thread 0x16e23b000 may have been prematurely finalized\n",
      "Thread 0x16e727000 may have been prematurely finalized\n",
      "Thread 0x16fc97000 may have been prematurely finalized\n",
      "Thread 0x16fc0b000 may have been prematurely finalized\n",
      "Thread 0x16fd23000 may have been prematurely finalized\n",
      "Thread 0x16fdaf000 may have been prematurely finalized\n",
      "Thread 0x16fe3b000 may have been prematurely finalized\n",
      "Thread 0x16fec7000 may have been prematurely finalized\n",
      "Thread 0x16ff53000 may have been prematurely finalized\n",
      "Thread 0x16ffdf000 may have been prematurely finalized\n",
      "Thread 0x17006b000 may have been prematurely finalized\n",
      "Thread 0x17029b000 may have been prematurely finalized\n",
      "Thread 0x170183000 may have been prematurely finalized\n",
      "Thread 0x17020f000 may have been prematurely finalized\n",
      "Thread 0x1700f7000 may have been prematurely finalized\n",
      "Thread 0x17045f000 may have been prematurely finalized\n",
      "Thread 0x170577000 may have been prematurely finalized\n",
      "Thread 0x1704eb000 may have been prematurely finalized\n",
      "Thread 0x170603000 may have been prematurely finalized\n",
      "Thread 0x17071b000 may have been prematurely finalized\n",
      "Thread 0x17068f000 may have been prematurely finalized\n",
      "Thread 0x17094b000 may have been prematurely finalized\n",
      "Thread 0x1707a7000 may have been prematurely finalized\n",
      "Thread 0x1709d7000 may have been prematurely finalized\n",
      "Thread 0x170a63000 may have been prematurely finalized\n",
      "Thread 0x170833000 may have been prematurely finalized\n",
      "Thread 0x1708bf000 may have been prematurely finalized\n",
      "Thread 0x170aef000 may have been prematurely finalized\n",
      "Thread 0x16feff000 may have been prematurely finalized\n",
      "Thread 0x16c1fb000 may have been prematurely finalized\n",
      "Thread 0x170017000 may have been prematurely finalized\n",
      "Thread 0x16c313000 may have been prematurely finalized\n",
      "Thread 0x16c287000 may have been prematurely finalized\n",
      "Thread 0x16ff8b000 may have been prematurely finalized\n",
      "Thread 0x16c39f000 may have been prematurely finalized\n",
      "Thread 0x1700a3000 may have been prematurely finalized\n",
      "Thread 0x16c42b000 may have been prematurely finalized\n",
      "Thread 0x17012f000 may have been prematurely finalized\n",
      "Thread 0x1701bb000 may have been prematurely finalized\n",
      "Thread 0x16c4b7000 may have been prematurely finalized\n",
      "Thread 0x170247000 may have been prematurely finalized\n",
      "Thread 0x16c543000 may have been prematurely finalized\n",
      "Thread 0x17035f000 may have been prematurely finalized\n",
      "Thread 0x16c5cf000 may have been prematurely finalized\n",
      "Thread 0x16c65b000 may have been prematurely finalized\n",
      "Thread 0x1702d3000 may have been prematurely finalized\n",
      "Thread 0x1703eb000 may have been prematurely finalized\n",
      "Thread 0x16c6e7000 may have been prematurely finalized\n",
      "Thread 0x170477000 may have been prematurely finalized\n",
      "Thread 0x17058f000 may have been prematurely finalized\n",
      "Thread 0x16c773000 may have been prematurely finalized\n",
      "Thread 0x16c7ff000 may have been prematurely finalized\n",
      "Thread 0x16c88b000 may have been prematurely finalized\n",
      "Thread 0x16fb0b000 may have been prematurely finalized\n",
      "Thread 0x16fd3b000 may have been prematurely finalized\n",
      "Thread 0x16fcaf000 may have been prematurely finalized\n",
      "Thread 0x16fc23000 may have been prematurely finalized\n",
      "Thread 0x16fdc7000 may have been prematurely finalized\n",
      "Thread 0x16fe53000 may have been prematurely finalized\n",
      "Thread 0x16fb97000 may have been prematurely finalized\n",
      "Thread 0x16fedf000 may have been prematurely finalized\n",
      "Thread 0x16ff6b000 may have been prematurely finalized\n",
      "Thread 0x16fff7000 may have been prematurely finalized\n",
      "Thread 0x170083000 may have been prematurely finalized\n",
      "Thread 0x17019b000 may have been prematurely finalized\n",
      "Memory Statistics:\n",
      "Memory Statistics:\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "Memory Statistics:\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count:     [ALLOC_TEMP_MAIN]\n",
      "  StackAllocators : \n",
      "      Peak usage frame count:     [ALLOC_TEMP_MAIN]\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 1 frames[1.0 KB-2.0 KB]: 1 frames, [2.0 MB-4.0 MB]: 1 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "\n",
      "      Initial Block Size 4.0 MB\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "  StackAllocators : \n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "[1.0 KB-2.0 KB]: 1 frames      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Peak usage frame count:       Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Current Block Size 32.0 KB\n",
      "[1.0 KB-2.0 KB]: 1 frames    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: , [2.0 MB-4.0 MB]: 1 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "Memory Statistics:\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Current Block Size 4.0 MB\n",
      "[1.0 KB-2.0 KB]: 1 frames      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      ", [2.0 MB-4.0 MB]: 1 frames    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "  StackAllocators : \n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak usage frame count:       Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "[1.0 KB-2.0 KB]: 1 frames      Initial Block Size 32.0 KB\n",
      ", [2.0 MB-4.0 MB]: 1 frames      Current Block Size 32.0 KB\n",
      "\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 5\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "Memory Statistics:\n",
      "      Current Block Size 32.0 KB\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "      Peak Allocated Bytes 0 B\n",
      "  StackAllocators : \n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_MAIN]\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Peak usage frame count:       Initial Block Size 32.0 KB\n",
      "[1.0 KB-2.0 KB]: 1 frames      Current Block Size 32.0 KB\n",
      ", [2.0 MB-4.0 MB]: 1 frames      Peak Allocated Bytes 0 B\n",
      "\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 4.0 MB\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Current Block Size 4.0 MB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 4.0 MB\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Current Block Size 4.0 MB\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 5\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 5\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 272 B\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Overflow Count 5\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 5\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Peak Allocated Bytes 446 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 240 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 272 B\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 80 B\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 250 B\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 176 B\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 144 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Peak Allocated Bytes 450 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 240 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 80 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Peak Allocated Bytes 446 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Initial Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 144 B\n",
      "      Current Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 203 B\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 450 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Current Block Size 64.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Overflow Count 0\n",
      "      Current Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 251 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 199 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Peak Allocated Bytes 440 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 385 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Peak Allocated Bytes 450 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 42 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 176 B\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 144 B\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Current Block Size 64.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 450 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 450 B\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 446 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Current Block Size 64.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Peak Allocated Bytes 297 B\n",
      "      Current Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Current Block Size 256.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 440 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 450 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 32.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Current Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Current Block Size 64.0 KB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 176 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Initial Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 64.0 KB\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Initial Block Size 32.0 KB\n",
      "      Overflow Count 0\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Current Block Size 256.0 KB\n",
      "  Peak main deferred allocation count 52\n",
      "      Peak Allocated Bytes 272 B\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_BUCKET]\n",
      "      Current Block Size 256.0 KB\n",
      "      Large Block size 4.0 MB\n",
      "      Peak Allocated Bytes 450 B\n",
      "      Used Block count 1\n",
      "      Overflow Count 0\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak usage frame count:       Current Block Size 32.0 KB\n",
      "[16.0 MB-32.0 MB]: 2 frames      Overflow Count 0\n",
      "\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "  Peak main deferred allocation count 52\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "    [ALLOC_BUCKET]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Large Block size 4.0 MB\n",
      "      Overflow Count 0\n",
      "      Used Block count 1\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 2 frames      Peak Allocated Bytes 0 B\n",
      "\n",
      "      Overflow Count 0\n",
      "      Requested Block Size 16.0 MB\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Peak Block count 1\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Peak usage frame count:       Requested Block Size 16.0 MB\n",
      "[4.0 MB-8.0 MB]: 2 frames      Current Block Size 256.0 KB\n",
      "\n",
      "      Current Block Size 256.0 KB\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Allocated memory 5.1 MB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Overflow Count 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "      Initial Block Size 256.0 KB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak usage frame count:       Peak Allocated Bytes 272 B\n",
      "[4.0 MB-8.0 MB]: 2 frames      Overflow Count 0\n",
      "\n",
      "      Initial Block Size 256.0 KB\n",
      "      Requested Block Size 16.0 MB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated Bytes 464 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "  Initial Block Size 2.0 MB\n",
      "      Initial Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Overflow Count 0\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 203 B\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "  Used Block Count 1\n",
      "      Initial Block Size 32.0 KB\n",
      "  Overflow Count (too large) 0\n",
      "      Current Block Size 32.0 KB\n",
      "  Overflow Count (full) 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "      Peak Allocated memory 5.1 MB\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 32.0 KB\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "  Initial Block Size 2.0 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "  Initial Block Size 2.0 MB\n",
      "      Initial Block Size 64.0 KB\n",
      "  Used Block Count 0\n",
      "      Current Block Size 64.0 KB\n",
      "  Overflow Count (too large) 0\n",
      "      Initial Block Size 64.0 KB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 52\n",
      "  Used Block Count 1\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Peak Allocated Bytes 272 B\n",
      "  Overflow Count (too large) 0\n",
      "      Initial Block Size 256.0 KB\n",
      "  Overflow Count (full) 0\n",
      "      Current Block Size 256.0 KB\n",
      "      Overflow Count 0\n",
      "  Overflow Count (full) 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "      Initial Block Size 256.0 KB\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "      Peak Allocated Bytes 158 B\n",
      "  Initial Block Size 2.0 MB\n",
      "      Overflow Count 0\n",
      "  Used Block Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "  Overflow Count (too large) 0\n",
      "      Initial Block Size 64.0 KB\n",
      "  Overflow Count (full) 0\n",
      "      Current Block Size 64.0 KB\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "      Peak Allocated Bytes 0 B\n",
      "  Initial Block Size 2.0 MB\n",
      "      Current Block Size 64.0 KB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Current Block Size 256.0 KB\n",
      "    [ALLOC_BUCKET]\n",
      "      Overflow Count 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 52\n",
      "  Initial Block Size 2.0 MB\n",
      "    [ALLOC_BUCKET]\n",
      "  Used Block Count 1\n",
      "      Large Block size 4.0 MB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Used Block count 1\n",
      "      Large Block size 4.0 MB\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Overflow Count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "  Overflow Count (too large) 0\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Used Block count 1\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "  Initial Block Size 1.0 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "  Used Block Count 1\n",
      "      Overflow Count 0\n",
      "  Overflow Count (too large) 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "  Overflow Count (full) 0\n",
      "      Initial Block Size 256.0 KB\n",
      "  Overflow Count (full) 0\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak usage frame count:       Peak Allocated Bytes 464 B\n",
      "      Overflow Count 0\n",
      "[16.0 MB-32.0 MB]: 2 frames    [ALLOC_TEMP_OSX HID Input]\n",
      "\n",
      "      Initial Block Size 64.0 KB\n",
      "      Requested Block Size 16.0 MB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated Bytes 0 B\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "  Peak main deferred allocation count 52\n",
      "      Peak Allocated Bytes 176 B\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "      Overflow Count 0\n",
      "  Initial Block Size 1.0 MB\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "  Used Block Count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count:   Overflow Count (too large) 0\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "  Overflow Count (full) 0\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "      Peak usage frame count:   Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "[4.0 MB-8.0 MB]: 2 frames      Peak Allocated Bytes 0 B\n",
      "  Peak main deferred allocation count 0\n",
      "      Overflow Count 0\n",
      "    [ALLOC_BUCKET]\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Large Block size 4.0 MB\n",
      "      Initial Block Size 64.0 KB\n",
      "      Used Block count 1\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Allocated Bytes 158 B\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Overflow Count 0\n",
      "      Peak usage frame count:     [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 32.0 KB\n",
      "[16.0 MB-32.0 MB]: 2 frames      Current Block Size 32.0 KB\n",
      "\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Large Block size 4.0 MB\n",
      "      Overflow Count 0\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Requested Block Size 16.0 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Initial Block Size 64.0 KB\n",
      "[32.0 KB-64.0 KB]: 1 frames      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      ", [64.0 KB-128.0 KB]: 1 frames    [ALLOC_BUCKET]\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Peak Block count 1\n",
      "      Large Block size 4.0 MB\n",
      "      Peak usage frame count:       Used Block count 1\n",
      "[32.0 KB-64.0 KB]: 1 frames      Peak Allocated bytes 1.3 MB\n",
      ", [64.0 KB-128.0 KB]: 1 frames    [ALLOC_DEFAULT_MAIN]\n",
      "\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Requested Block Size 16.0 MB\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "      Peak Block count 1\n",
      "  Peak main deferred allocation count 52\n",
      "      Peak Allocated memory 69.7 KB\n",
      "    [ALLOC_BUCKET]\n",
      "      Initial Block Size 256.0 KB\n",
      "\n",
      "      Current Block Size 256.0 KB\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Allocated Bytes 385 B\n",
      "      Peak Block count 1\n",
      "      Overflow Count 0\n",
      "      Peak Allocated memory 5.4 MB\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Initial Block Size 256.0 KB\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak usage frame count:       Peak Allocated Bytes 272 B\n",
      "      Large Block size 4.0 MB\n",
      "      Overflow Count 0\n",
      "[16.0 MB-32.0 MB]: 2 frames    [ALLOC_TEMP_Background Job.worker 14]\n",
      "\n",
      "      Requested Block Size 16.0 MB\n",
      "      Initial Block Size 32.0 KB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Allocated Bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Overflow Count 0\n",
      "      Peak usage frame count:     [ALLOC_TEMP_Loading.AsyncRead]\n",
      "[64.0 KB-128.0 KB]: 2 frames      Initial Block Size 64.0 KB\n",
      "\n",
      "      Current Block Size 64.0 KB\n",
      "      Used Block count 1\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Peak usage frame count:       Requested Block Size 16.0 MB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Overflow Count 0\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 52\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Used Block count 1\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "  Initial Block Size 2.0 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "  Used Block Count 1\n",
      "      Peak usage frame count:   Overflow Count (too large) 0\n",
      "[16.0 MB-32.0 MB]: 2 frames  Overflow Count (full) 0\n",
      "\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "      Requested Block Size 16.0 MB\n",
      "  Initial Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "  Used Block Count 0\n",
      "      Peak Allocated memory 29.6 MB\n",
      "  Overflow Count (too large) 0\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "  Overflow Count (full) 0\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count:       Peak usage frame count: [4.0 MB-8.0 MB]: 2 frames[4.0 MB-8.0 MB]: 2 frames      Large Block size 4.0 MB\n",
      "\n",
      "      Requested Block Size 16.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Block count 1\n",
      "[16.0 MB-32.0 MB]: 2 frames      Peak Allocated memory 5.1 MB\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "\n",
      "\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak usage frame count:       Peak Block count 1\n",
      "[4.0 MB-8.0 MB]: 2 frames      Peak Block count 1\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "  Initial Block Size 2.0 MB\n",
      "      Requested Block Size 16.0 MB\n",
      "  Used Block Count 1\n",
      "      Peak Large allocation bytes 0 B\n",
      "\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "      Peak usage frame count:   Initial Block Size 2.0 MB\n",
      "[64.0 KB-128.0 KB]: 2 frames  Used Block Count 1\n",
      "\n",
      "  Overflow Count (too large) 0\n",
      "      Peak usage frame count:       Requested Block Size 16.0 MB\n",
      "[0.5 MB-1.0 MB]: 2 frames      Peak Block count 1\n",
      "\n",
      "  Initial Block Size 2.0 MB\n",
      "  Overflow Count (too large) 0\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "      Requested Block Size 4.0 MB\n",
      "  Overflow Count (full) 0\n",
      "      Peak Block count 1\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "      Peak Allocated memory 0.7 MB\n",
      "  Initial Block Size 2.0 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "  Used Block Count 0\n",
      "  Overflow Count (full) 0\n",
      "  Overflow Count (too large) 0\n",
      "      Peak Allocated memory 82.5 KB\n",
      "  Overflow Count (full) 0\n",
      "      Peak Large allocation bytes 0 B\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Peak main deferred allocation count 0\n",
      "  Initial Block Size 1.0 MB\n",
      "    [ALLOC_BUCKET]\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "  Overflow Count (full) 0\n",
      "      Large Block size 4.0 MB\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "      Peak Allocated memory 5.2 MB\n",
      "  Peak main deferred allocation count 0\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_BUCKET]\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "      Large Block size 4.0 MB\n",
      "  Initial Block Size 2.0 MB\n",
      "      Used Block count 1\n",
      "      Peak usage frame count:       Peak Allocated bytes 1.3 MB\n",
      "[0.5 MB-1.0 MB]: 1 frames    [ALLOC_CACHEOBJECTS_MAIN]\n",
      ", [4.0 MB-8.0 MB]: 1 frames      Peak usage frame count:       Peak Allocated memory 29.6 MB\n",
      "  Used Block Count 1\n",
      "      Used Block count 1\n",
      "  Overflow Count (too large) 0\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "  Overflow Count (full) 0\n",
      "    [ALLOC_GFX_MAIN]\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Used Block Count 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Overflow Count (too large) 0\n",
      "      Peak usage frame count:   Overflow Count (full) 0\n",
      "\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "      Requested Block Size 4.0 MB\n",
      "[32.0 KB-64.0 KB]: 1 frames      Peak Block count 2\n",
      ", [64.0 KB-128.0 KB]: 1 frames      Peak Allocated memory 4.9 MB\n",
      "\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Requested Block Size 16.0 MB\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "      Peak Block count 1\n",
      "  Peak main deferred allocation count 0\n",
      "      Peak Allocated memory 69.7 KB\n",
      "    [ALLOC_BUCKET]\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Large Block size 4.0 MB\n",
      "    [ALLOC_GFX_THREAD]\n",
      "  Initial Block Size 2.0 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "  Initial Block Size 2.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "  Used Block Count 1\n",
      "      Peak usage frame count:   Overflow Count (too large) 0\n",
      "[4.0 MB-8.0 MB]: 2 frames  Overflow Count (full) 0\n",
      "\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "      Peak Allocated memory 5.1 MB\n",
      "  Initial Block Size 2.0 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "  Used Block Count 0\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Overflow Count (too large) 0\n",
      "  Initial Block Size 2.0 MB\n",
      "  Overflow Count (full) 0\n",
      "  Used Block Count 1\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Overflow Count (too large) 0\n",
      "  Initial Block Size 2.0 MB\n",
      "  Overflow Count (full) 0\n",
      "  Used Block Count 1\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Overflow Count (too large) 0\n",
      "      Used Block count 1\n",
      "  Overflow Count (full) 0\n",
      "  Used Block Count 1\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Overflow Count (too large) 0\n",
      "  Initial Block Size 1.0 MB\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "  Overflow Count (full) 0\n",
      "[0.5 MB-1.0 MB]: 2 frames[ALLOC_GFX] Dual Thread Allocator\n",
      "\n",
      "  Peak main deferred allocation count 0\n",
      "      Requested Block Size 4.0 MB\n",
      "    [ALLOC_BUCKET]\n",
      "      Peak Block count 1\n",
      "      Large Block size 4.0 MB\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Used Block count 1\n",
      "  Used Block Count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count:     [ALLOC_TYPETREE_MAIN]\n",
      "[32.0 KB-64.0 KB]: 1 frames      Peak usage frame count:   Overflow Count (too large) 0\n",
      ", [64.0 KB-128.0 KB]: 1 frames  Initial Block Size 2.0 MB\n",
      "\n",
      "  Used Block Count 0\n",
      "      Requested Block Size 16.0 MB\n",
      "  Overflow Count (too large) 0\n",
      "      Peak usage frame count:   Overflow Count (full) 0\n",
      "[64.0 KB-128.0 KB]: 2 frames\n",
      "  Overflow Count (too large) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Requested Block Size 16.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Allocated memory 82.5 KB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "  Overflow Count (full) 0\n",
      "      Peak Block count 1\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "    [ALLOC_BUCKET]\n",
      "      Peak usage frame count:       Large Block size 4.0 MB\n",
      "[0.5 MB-1.0 MB]: 1 frames      Used Block count 1\n",
      ", [4.0 MB-8.0 MB]: 1 frames  Initial Block Size 1.0 MB\n",
      "\n",
      "  Used Block Count 1\n",
      "      Requested Block Size 4.0 MB\n",
      "  Overflow Count (too large) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "      Peak usage frame count:       Peak Allocated bytes 1.3 MB\n",
      "[32.0 KB-64.0 KB]: 1 frames    [ALLOC_CACHEOBJECTS_MAIN]\n",
      ", [64.0 KB-128.0 KB]: 1 frames      Peak usage frame count: \n",
      "[0.5 MB-1.0 MB]: 2 frames      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "\n",
      "      Peak usage frame count:       Requested Block Size 4.0 MB\n",
      "[64.0 KB-128.0 KB]: 2 frames      Peak Block count 1\n",
      "  Initial Block Size 2.0 MB\n",
      "      Peak Allocated memory 0.7 MB\n",
      "  Peak main deferred allocation count 0\n",
      "      Peak Large allocation bytes 0 B\n",
      "\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Requested Block Size 16.0 MB\n",
      "[0-1.0 KB]: 2 frames      Peak usage frame count: \n",
      "[0.5 MB-1.0 MB]: 1 frames      Requested Block Size 2.0 MB\n",
      ", [4.0 MB-8.0 MB]: 1 frames      Peak Block count 1\n",
      "\n",
      "      Peak Allocated memory 112 B\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "  Used Block Count 1\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Block count 1\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Block count 2\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_BUCKET]\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Large Block size 4.0 MB\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Used Block count 1\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_MAIN]\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "      Peak usage frame count:       Peak usage frame count: [32.0 KB-64.0 KB]: 1 frames  Overflow Count (too large) 0\n",
      "  Peak main deferred allocation count 0\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "[4.0 KB-8.0 KB]: 2 frames  Peak main deferred allocation count 0\n",
      "\n",
      "    [ALLOC_BUCKET]\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "      Large Block size 4.0 MB\n",
      "    [ALLOC_BUCKET]\n",
      "      Used Block count 1\n",
      "      Large Block size 4.0 MB\n",
      "  Overflow Count (full) 0\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count:   Peak main deferred allocation count 0\n",
      "[0-1.0 KB]: 2 frames      Requested Block Size 2.0 MB\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak Block count 1\n",
      "      Peak usage frame count:       Peak Allocated memory 7.9 KB\n",
      "[64.0 KB-128.0 KB]: 2 frames      Peak Large allocation bytes 0 B\n",
      "\n",
      ", [64.0 KB-128.0 KB]: 1 frames      Requested Block Size 16.0 MB\n",
      "\n",
      "  Initial Block Size 1.0 MB\n",
      "\n",
      "    [ALLOC_BUCKET]\n",
      "      Used Block count 1\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Block count 1\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Large Block size 4.0 MB\n",
      "      Peak usage frame count:       Used Block count 1\n",
      "      Peak Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Allocated memory 82.5 KB\n",
      "  Used Block Count 1\n",
      "      Peak Large allocation bytes 0 B\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "      Peak Allocated memory 69.7 KB\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count:     [ALLOC_BUCKET]\n",
      "[0.5 MB-1.0 MB]: 2 frames\n",
      "      Large Block size 4.0 MB\n",
      "      Requested Block Size 4.0 MB\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak usage frame count:       Peak Allocated memory 0.7 MB\n",
      "[64.0 KB-128.0 KB]: 2 frames      Used Block count 1\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count:       Peak usage frame count: [0.5 MB-1.0 MB]: 1 frames[32.0 KB-64.0 KB]: 1 frames[0-1.0 KB]: 2 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "\n",
      "      Peak Block count 1\n",
      "\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Block count 1\n",
      "      Requested Block Size 16.0 MB\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Block count 1\n",
      "      Peak usage frame count:       Peak Large allocation bytes 0 B\n",
      "      Peak Allocated memory 82.5 KB\n",
      "[64.0 KB-128.0 KB]: 2 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.6 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "  Peak main deferred allocation count 0\n",
      "      Peak usage frame count:     [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "[4.0 KB-8.0 KB]: 2 frames      Used Block count 1\n",
      "\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "      Requested Block Size 2.0 MB\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Peak Block count 1\n",
      "      Large Block size 4.0 MB\n",
      ", [4.0 MB-8.0 MB]: 1 frames      Used Block count 1\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Block count 1\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "\n",
      "      Peak usage frame count:       Peak usage frame count: [0.5 MB-1.0 MB]: 2 frames[0.5 MB-1.0 MB]: 2 frames\n",
      "\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 1 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "      Requested Block Size 4.0 MB\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count:     [ALLOC_CACHEOBJECTS_MAIN]\n",
      "[4.0 KB-8.0 KB]: 2 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak usage frame count:       Peak Block count 1\n",
      "[0.5 MB-1.0 MB]: 2 frames      Peak Allocated memory 7.9 KB\n",
      "\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Block count 1\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count:       Peak Allocated memory 0.7 MB\n",
      "[0.5 MB-1.0 MB]: 1 frames      Peak Large allocation bytes 0 B\n",
      ", [4.0 MB-8.0 MB]: 1 frames    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "\n",
      "      Peak usage frame count:       Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "[0.5 MB-1.0 MB]: 1 frames      Peak Allocated memory 4.9 MB\n",
      ", [4.0 MB-8.0 MB]: 1 frames      Peak Large allocation bytes 0 B\n",
      "\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "  Peak main deferred allocation count 0\n",
      "      Peak Allocated memory 4.9 MB\n",
      "    [ALLOC_BUCKET]\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Large Block size 4.0 MB\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "      Used Block count 1\n",
      "  Peak main deferred allocation count 0\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "    [ALLOC_BUCKET]\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Large Block size 4.0 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Used Block count 1\n",
      "      Peak usage frame count:       Peak Allocated bytes 1.3 MB\n",
      "[0-1.0 KB]: 2 frames\n",
      "      Peak Block count 2\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Requested Block Size 2.0 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak Block count 1\n",
      "      Peak usage frame count:       Peak Allocated memory 112 B\n",
      "[0-1.0 KB]: 2 frames      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count:       Peak usage frame count:       Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "      Requested Block Size 2.0 MB\n",
      "  Peak main deferred allocation count 0\n",
      "      Peak Block count 1\n",
      "    [ALLOC_BUCKET]\n",
      "      Peak Allocated memory 112 B\n",
      "      Large Block size 4.0 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Used Block count 1\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "[0-1.0 KB]: 2 frames      Peak usage frame count: \n",
      "[4.0 KB-8.0 KB]: 2 frames      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "[4.0 KB-8.0 KB]: 2 frames      Peak Large allocation bytes 0 B\n",
      "\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "      Peak usage frame count:     [ALLOC_TYPETREE_MAIN]\n",
      "[4.0 KB-8.0 KB]: 2 frames      Peak usage frame count: \n",
      "[0-1.0 KB]: 2 frames      Requested Block Size 2.0 MB\n",
      "\n",
      "      Peak Block count 1\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Block count 1\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Allocated memory 112 B\n",
      "\n",
      "      Peak Block count 1\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 2 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n"
     ]
    }
   ],
   "source": [
    "collector = MultiaSyncDataCollector(\n",
    "    [lambda: create_env(graphics=False, time_scale=TIME_SCALE)]*WORKERS, policy, \n",
    "    frames_per_batch = ENV_STEPS, \n",
    "    total_frames = TIMESTEPS - start_timestep, \n",
    "    env_device=\"cpu\", device=device, storing_device=\"cpu\",\n",
    "    reset_at_each_iter=False,\n",
    ")\n",
    "short_watch, long_watch = Stopwatch(), Stopwatch()\n",
    "metric_module = SimpleMetricModule(mode=\"approx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "New input system (experimental) initialized\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Loaded All Assemblies, in  0.049 seconds\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.254375 ms\n",
      "Registered Communicator in Agent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/transforms/transforms.py:587: FutureWarning: The key 'continuous_action' is unaccounted for by the transform (expected keys ['VectorSensor_size243', 'done', 'terminated', 'truncated', 'group_reward', 'reward']). Every new entry in the tensordict resulting from a call to a transform must be registered in the specs for torchrl rollouts to be consistently built. Make sure transform_output_spec/transform_observation_spec/... is coded correctly. This warning will trigger a KeyError in v0.9, make sure to adapt your code accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0x16d727000 may have been prematurely finalized\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Thread 0x16c213000 may have been prematurely finalized\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 93 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 94 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [1.0 MB-2.0 MB]: 1 frames, [2.0 MB-4.0 MB]: 93 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 3.3 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 93 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 94 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 94 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 93 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 94 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 94 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10365  72562000.0  255692.101376         242.627643  253159.776248  72.306946   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10365     -2.41164     0.001181  0.0003 -16.091123  0.021518  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10365          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10366  72569000.0  255717.951878         242.644314  253185.609933  73.470928   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10366    -2.442843     0.001108  0.0003 -18.39247  0.021285  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10366          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10367  72576000.0  255744.22088          242.66559  253211.857582  85.539296   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10367    -2.533053     0.001245  0.0003 -11.725533  0.020227  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10367          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10368  72583000.0  255769.512737         242.679501  253237.135359  30.458827   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10368    -2.402718     0.002034  0.0003 -11.605381  0.021446  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10368          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10369  72590000.0  255795.767601         242.697649  253263.372026  23.058159   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10369    -2.279236        0.002  0.0003 -10.598261  0.020341  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10369          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10370  72597000.0  255821.926862         242.712534  253289.516347  46.506397   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10370     -2.22009     0.001985  0.0003 -7.87852  0.019291  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10370          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10371  72604000.0  255847.912036         242.729112  253315.484864  63.469015   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10371    -2.197505     0.002271  0.0003 -7.885133  0.019666  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10371          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10372  72611000.0  255874.666817         242.745214  253342.223494  35.234615   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10372    -2.140818      0.00113  0.0003 -5.647039  0.017387  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10372          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10373  72618000.0  255900.210414         242.759533  253367.752696  50.858747   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10373    -2.128241     0.001714  0.0003 -31.440938  0.025365  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10373          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10374  72625000.0  255923.211587         242.776142  253390.737208  44.678368   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10374    -2.224451     0.002998  0.0003 -25.859749   0.03492  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10374          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10375  72632000.0  255947.239967         242.790819  253414.750857  51.380064   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10375    -2.281191     0.002034  0.0003 -17.991078  0.030322  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10375          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "10376  72639000.0  255971.21337          242.80911  253438.705919  100.375906   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10376    -2.290724     0.002522  0.0003 -17.641154  0.028471  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10376          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10377  72646000.0  255994.991218         242.823383  253462.469433  60.437664   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10377    -2.257147     0.002416  0.0003 -14.950482  0.027918  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10377          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10378  72653000.0  256019.215654         242.837828  253486.679378  56.758102   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10378    -2.216174     0.002393  0.0003 -14.818302  0.026805  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10378          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10379  72660000.0  256043.104329         242.853709  253510.552124  65.730251   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10379    -2.227257     0.002815  0.0003 -13.779635  0.027765  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10379          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10380  72667000.0  256067.425741         242.868164  253534.859029  52.815795   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10380    -2.205956     0.002075  0.0003 -13.585659   0.02608  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10380          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10381  72674000.0  256090.961152         242.882733  253558.379771  28.146977   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10381    -2.180626     0.002044  0.0003 -11.845097  0.024539  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10381          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10382  72681000.0  256115.651355         242.897184  253583.055466  78.714721   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10382    -2.175116      0.00217  0.0003 -11.937165  0.024361  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10382          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10383  72688000.0  256139.799129         242.914794  253607.185572  44.56744   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10383    -2.146303     0.002312  0.0003 -10.382123  0.023479  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10383          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10384  72695000.0  256163.550613         242.929965  253630.92183  29.185057   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10384    -2.125766     0.001866  0.0003  -9.3054  0.021413  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10384          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10385  72702000.0  256187.345131         242.944121  253654.702131  47.924787   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10385    -2.106099      0.00283  0.0003 -8.888228  0.021862  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10385          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10386  72709000.0  256211.659277          242.95983  253679.000513  38.715236   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10386    -2.069499     0.001504  0.0003 -7.984493  0.021001  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10386          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time    train_time      score  \\\n",
      "10387  72716000.0  256235.65371         242.974192  253702.98053  76.786414   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10387    -2.086291     0.001858  0.0003 -7.263407  0.020277  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10387          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10388  72723000.0  256258.077941         242.989366  253725.389533  52.119497   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10388    -2.075684     0.002431  0.0003 -5.37643  0.019836  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10388          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10389  72730000.0  256279.318897         243.003139  253746.616667  21.106819   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10389    -2.040288     0.001935  0.0003 -3.748423  0.018494  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10389          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10390  72737000.0  256300.563307         243.018599  253767.845557  27.290771   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10390    -2.027708     0.001255  0.0003 -2.95502  0.017537  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10390          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10391  72744000.0  256321.863838         243.032138  253789.132499  46.272852   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10391    -1.999023     0.001349  0.0003 -1.727418  0.017352  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10391          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10392  72751000.0  256343.177605         243.045756  253810.432598  44.276796   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10392    -1.972344     0.002252  0.0003  0.354187  0.017655  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10392          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10393  72758000.0  256364.318067          243.06122  253831.557547  76.800987   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10393    -1.948782     0.003482  0.0003 -1.193062   0.01905  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10393          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10394  72765000.0  256385.878468         243.076832  253853.102275  84.600493   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10394    -1.948086     0.001962  0.0003 -1.237512  0.017042  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10394          1000.0    -53.618839  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "10395  72772000.0  256407.1148         243.092608  253874.322773  98.119497   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10395    -1.967832     0.002731  0.0003 -0.997771  0.018079  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10395          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10396  72779000.0  256430.407221         243.109753  253897.597995  82.773507   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10396    -1.932904     0.002052  0.0003 -0.938427  0.017608  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10396          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10397  72786000.0  256454.949712         243.128432  253922.12175  82.363315   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10397     -1.94104     0.003542  0.0003 -1.299308  0.018956  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10397          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10398  72793000.0  256478.642836         243.143976  253945.799253  76.235794   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10398    -1.951111     0.001958  0.0003 -1.300686   0.01779  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10398          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10399  72800000.0  256502.673277         243.159609  253969.814012  74.503101   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10399    -1.951512     0.002401  0.0003 -1.554643  0.018148  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10399          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10400  72807000.0  256527.136214         243.174597  253994.261882  82.979336   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10400    -1.932871     0.003876  0.0003 -1.961057  0.020163  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10400          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10401  72814000.0  256551.559933         243.193938  254018.666207  89.911565   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10401    -1.926945       0.0021  0.0003 -2.810532    0.0185  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10401          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10402  72821000.0  256575.121922         243.211294  254042.210757  83.499685   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10402    -1.931718     0.002476  0.0003 -2.924531  0.019301  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10402          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10403  72828000.0  256599.655819         243.225804  254066.730083  91.362208   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10403    -1.932745     0.001997  0.0003 -3.19329  0.018692  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10403          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10404  72835000.0  256624.552727         243.241033  254091.611687  89.360803   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10404    -1.925934      0.00193  0.0003 -2.338104  0.018413  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10404          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "10405  72842000.0  256648.88777         243.255346  254115.932362  88.81595   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10405     -1.93791     0.002119  0.0003 -2.95669  0.018858  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10405          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10406  72849000.0  256673.892023         243.270877  254140.921024  64.519785   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10406    -1.916367     0.002082  0.0003 -3.990589  0.019317  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10406          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10407  72856000.0  256697.724355         243.285946  254164.738204  94.226509   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10407    -1.924344     0.001744  0.0003 -3.399515  0.017988  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10407          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10408  72863000.0  256722.040081         243.301035  254189.038771  92.081308   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10408    -1.936809     0.002443  0.0003 -2.532862  0.018489  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10408          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10409  72870000.0  256746.386762         243.314822  254213.37161  75.363301   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10409    -1.920984     0.002711  0.0003 -2.752554  0.018395  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10409          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10410  72877000.0  256770.473246          243.32977  254237.443094  70.739441   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10410    -1.993597      0.00295  0.0003 -34.128443  0.033392  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10410          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10411  72884000.0  256794.976171         243.346125  254261.929565  90.53202   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10411    -2.042858     0.003676  0.0003 -18.674718  0.034096  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10411          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10412  72891000.0  256818.510126         243.360656  254285.448932  82.729451   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10412    -2.073743     0.002532  0.0003 -15.941561  0.031187  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10412          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10413  72898000.0  256843.963852         243.374715  254310.888546   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha    entropy  td_error  \\\n",
      "10413  110.860787    -2.101565     0.003484  0.0003 -13.601206  0.029853   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10413  14.868451          1000.0    -53.618839  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "10414  72905000.0  256869.1203         243.389425  254336.030233  65.003522   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10414    -2.114491     0.002532  0.0003 -10.520428  0.028148  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10414          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10415  72912000.0  256893.849353         243.404384  254360.744272  90.157017   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10415      -2.1258      0.00269  0.0003 -10.388046  0.028122  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10415          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10416  72919000.0  256917.754859         243.419627  254384.634478  73.968396   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10416    -2.127157     0.002944  0.0003 -10.702063  0.027316  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10416          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10417  72926000.0  256942.308883         243.434325  254409.173724  69.675334   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10417    -2.122301     0.003966  0.0003 -9.740597   0.02724  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10417          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10418  72933000.0  256967.096775         243.450293  254433.945592  36.049366   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10418    -2.145021     0.001983  0.0003 -8.169664  0.024845  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10418          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10419  72940000.0  256992.565531         243.465289  254459.399298  46.455152   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10419    -2.121544     0.001906  0.0003 -7.141403  0.023568  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10419          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10420  72947000.0  257017.321419         243.480762  254484.139632  35.850074   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10420    -2.103051     0.003537  0.0003 -5.043998  0.023191  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10420          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10421  72954000.0  257041.746876         243.497087  254508.548679  67.833066   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10421    -2.093989     0.001647  0.0003 -4.449531  0.021197  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10421          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10422  72961000.0  257063.945634         243.510591  254530.733882  52.327722   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10422    -2.106937     0.001707  0.0003 -3.099367  0.019827  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10422          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10423  72968000.0  257085.725885          243.52437  254552.500302  40.380169   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10423    -2.092213     0.002281  0.0003 -3.191242   0.01971  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10423          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10424  72975000.0  257107.979579         243.539161  254574.739151  36.605116   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10424    -2.086061      0.00235  0.0003 -1.611418  0.020087  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10424          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10425  72982000.0  257129.860579         243.553962  254596.605293  53.810947   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10425    -2.055889     0.002297  0.0003 -0.560708  0.018149  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10425          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10426  72989000.0  257152.269684         243.568461  254618.999839  61.298855   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10426    -2.044171     0.002301  0.0003  0.516787  0.017882  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10426          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10427  72996000.0  257174.716366         243.582607  254641.432324  74.245326   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10427     -2.03798     0.002807  0.0003  0.477536  0.017097  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10427          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10428  73003000.0  257196.892844         243.598334  254663.593026  79.957634   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10428    -2.030885     0.001642  0.0003  0.161811  0.016629  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10428          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10429  73010000.0  257218.905866         243.612746  254685.591582  69.590844   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10429    -2.020466     0.002234  0.0003 -0.076929  0.018063  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10429          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10430  73017000.0  257241.263735         243.628028  254707.934112  82.829975   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10430    -1.994121     0.001698  0.0003 -0.394076  0.016692  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10430          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10431  73024000.0  257263.427675         243.642883  254730.083137  89.657106   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10431    -2.010266     0.001876  0.0003 -0.345695  0.017288  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10431          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10432  73031000.0  257286.008789         243.658476  254752.648603  88.750891   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10432    -1.980435     0.002526  0.0003 -1.005211  0.019049  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10432          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10433  73038000.0  257308.052705         243.672572  254774.678369  98.166227   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10433    -2.016314     0.002567  0.0003 -0.789972  0.017934  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10433          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10434  73045000.0  257330.708745         243.687216  254797.319708  92.44857   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10434    -2.008401     0.001327  0.0003 -1.484617  0.017719  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10434          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10435  73052000.0  257352.676302         243.701865  254819.272544  84.744543   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10435    -2.014207     0.001835  0.0003 -2.371858  0.018138  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10435          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10436  73059000.0  257375.019423         243.715965  254841.601491  90.224855   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10436    -2.005807     0.002163  0.0003 -4.375184  0.018673  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10436          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10437  73066000.0  257397.01928         243.732443  254863.584821  99.956773   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10437    -2.038377     0.003112  0.0003 -19.447722  0.024695  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10437          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10438  73073000.0  257419.560642         243.746671  254886.111891  85.986711   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10438    -2.059723     0.002841  0.0003 -15.683484  0.029688  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10438          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time   train_time      score  \\\n",
      "10439  73080000.0  257442.172167         243.763747  254908.7063  87.503508   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error     return  \\\n",
      "10439    -2.110157     0.002349  0.0003 -11.913223  0.028185  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10439          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10440  73087000.0  257464.633449         243.778171  254931.153099  82.229346   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10440    -2.076326     0.003445  0.0003 -9.63504  0.028237  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10440          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10441  73094000.0  257487.059641         243.791995  254953.565415  66.697486   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10441     -2.09063     0.003168  0.0003 -9.013603  0.026561  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10441          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10442  73101000.0  257509.621333         243.806836  254976.112216  26.874013   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10442    -2.086179     0.002696  0.0003 -8.673626  0.025908  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10442          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10443  73108000.0  257531.630478         243.821123  254998.107013  43.466266   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10443    -2.086383     0.002115  0.0003 -7.84493  0.023886  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10443          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10444  73115000.0  257554.898923         243.835287  255021.361243  53.820468   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10444    -2.080731     0.002607  0.0003 -6.418825  0.022961  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10444          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10445  73122000.0  257579.321675         243.848799  255045.770422  45.029163   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10445    -2.083641     0.002113  0.0003 -5.34662  0.022098  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10445          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10446  73129000.0  257602.07538         243.863436  255068.509435  16.025232   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10446    -2.054097     0.001473  0.0003 -5.012093  0.020026  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10446          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "10447  73136000.0  257624.059332         243.877822  255090.478952  6.59512   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10447    -2.056351     0.002302  0.0003 -2.612489  0.019754  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10447          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10448  73143000.0  257646.835644         243.891907  255113.241129  38.54211   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10448    -2.052506     0.002106  0.0003 -2.739391  0.019717  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10448          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10449  73150000.0  257668.670424         243.906395  255135.061371  31.867541   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10449    -2.052118     0.002046  0.0003 -1.069049  0.018901  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10449          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10450  73157000.0  257691.652183         243.920287  255158.029182  24.629904   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10450    -2.007171      0.00198  0.0003 -0.889223   0.01874  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10450          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10451  73164000.0  257713.800651         243.934452  255180.163429  32.876924   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10451    -1.993157     0.001336  0.0003  0.869935  0.016605  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10451          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10452  73171000.0  257736.42029         243.949084  255202.768383  71.390949   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10452     -2.00652     0.001732  0.0003  0.398138  0.016879  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10452          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10453  73178000.0  257758.93653         243.964365  255225.269282  72.669826   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10453    -1.986433     0.001532  0.0003 -0.210689   0.01692  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10453          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10454  73185000.0  257780.867686         243.978405  255247.186341   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10454  103.316277    -1.984806     0.002178  0.0003  0.233356   0.01677   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10454  14.868451          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10455  73192000.0  257803.156768         243.992676  255269.461104  87.606445   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10455     -2.00933     0.001025  0.0003 -0.578677  0.015827  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10455          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10456  73199000.0  257825.415588         244.008058  255291.704492   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10456  110.159636    -1.980316     0.002367  0.0003 -0.746627  0.017306   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10456  14.868451          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10457  73206000.0  257848.031691         244.022988  255314.305612  95.637321   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10457    -1.980173     0.002091  0.0003 -1.528356  0.018356  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10457          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10458  73213000.0  257870.514409         244.037399  255336.773871  97.689271   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10458    -2.011133     0.002423  0.0003 -1.018642  0.018102  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10458          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "10459  73220000.0  257892.377298         244.051297  255358.622816  92.2639   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10459     -2.00817     0.001904  0.0003 -0.615445  0.017719  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10459          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10460  73227000.0  257914.976808          244.06617  255381.207402  71.017958   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10460    -2.016101     0.002397  0.0003 -0.824469  0.018677  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10460          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10461  73234000.0  257937.349409         244.080628  255403.565492  88.536359   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10461    -2.000413     0.003073  0.0003 -1.342562  0.019067  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10461          1000.0    -53.618839  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10462  73241000.0  257960.328147         244.096097  255426.528704  73.329069   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10462    -1.999778     0.002569  0.0003 -1.641326  0.018063  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10462          1000.0    -53.618839  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10463  73248000.0  257982.68158         244.112243  255448.865935  78.494228   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10463    -2.009825     0.002299  0.0003 -1.139265  0.019028  14.868451   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10463          1000.0    -53.618839  \n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10464  73255000.0  258022.30937         244.126044  255471.416572  62.872991   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10464     -2.00249     0.001786  0.0003 -1.872646  0.017592  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10464          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10465  73262000.0  258048.187371          244.14238  255497.278168  77.181801   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10465    -1.999954     0.001722  0.0003 -1.826521  0.017894  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10465          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10466  73269000.0  258073.689634         244.157822  255522.764909  92.083149   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10466    -1.999702     0.001952  0.0003 -1.391613  0.017959  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10466          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10467  73276000.0  258100.157824         244.173746  255549.217121  84.671542   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10467    -1.992854     0.002371  0.0003 -1.241193  0.018739  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10467          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10468  73283000.0  258125.190698         244.188163  255574.23553  80.095619   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10468    -1.978704     0.001977  0.0003 -0.883247   0.01878  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10468          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10469  73290000.0  258147.591223         244.203257  255596.620913  81.241854   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10469     -1.99221     0.002578  0.0003 -1.144048  0.018693  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10469          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "10470  73297000.0  258169.361317          244.21767  255618.37654  90.05063   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10470    -2.009084     0.001565  0.0003 -0.762562   0.01737  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10470          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "10471  73304000.0  258191.944753         244.232997  255640.944596  83.4857   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10471    -1.985977      0.00223  0.0003 -2.378076  0.018877  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10471          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10472  73311000.0  258214.467192         244.248808  255663.451174   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10472  104.023218    -1.987826     0.002016  0.0003 -2.077252   0.01813   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10472  25.69932          1000.0     -9.310139  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10473  73318000.0  258236.67443         244.265445  255685.641723  95.552415   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error    return  \\\n",
      "10473     -2.00577     0.002258  0.0003 -12.210511   0.02078  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10473          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10474  73325000.0  258259.244611         244.280274  255708.197021  91.484368   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error    return  \\\n",
      "10474    -2.050298     0.002157  0.0003 -14.514765  0.027793  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10474          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10475  73332000.0  258281.817979         244.294068  255730.756539  99.409111   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10475    -2.091032     0.002971  0.0003 -9.22796  0.027657  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10475          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10476  73339000.0  258304.164928         244.308664  255753.088841  70.560358   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10476    -2.090847     0.003029  0.0003 -7.869162  0.027265  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10476          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10477  73346000.0  258326.661366         244.323512  255775.57038  97.217493   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10477     -2.09037     0.002433  0.0003 -6.959876  0.024773  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10477          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10478  73353000.0  258348.761787         244.337311  255797.656943  64.08263   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10478    -2.079098      0.00263  0.0003 -6.920175  0.025023  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10478          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10479  73360000.0  258371.135094         244.351759  255820.015751  32.205816   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10479      -2.0721     0.004028  0.0003 -6.320176  0.024951  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10479          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10480  73367000.0  258394.535094          244.36552  255843.401942  35.141289   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10480    -2.072334     0.002498  0.0003 -5.262976  0.023454  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10480          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10481  73374000.0  258417.316219         244.380385  255866.168151  34.835849   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10481    -2.044691     0.001808  0.0003 -4.512936  0.020592  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10481          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10482  73381000.0  258439.926992         244.394587  255888.76467  61.783124   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10482    -2.033759     0.002039  0.0003 -3.859536  0.020972  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10482          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10483  73388000.0  258462.420235         244.408594  255911.243847  53.860944   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10483    -2.041567     0.002552  0.0003 -2.889725  0.020237  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10483          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10484  73395000.0  258484.868173         244.423731  255933.676591  20.912047   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10484    -2.032613     0.002608  0.0003 -2.738676  0.019453  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10484          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10485  73402000.0  258507.053622          244.43762  255955.848102  41.656237   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10485    -2.022793     0.001747  0.0003 -2.274683  0.018791  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10485          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10486  73409000.0  258529.086901         244.452703  255977.866247  36.533915   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10486    -2.032893     0.002215  0.0003 -1.467945  0.019633  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10486          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10487  73416000.0  258551.482342         244.466807  256000.247531  32.054905   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10487    -2.000184     0.002535  0.0003 -0.157422  0.018319  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10487          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10488  73423000.0  258573.429701         244.479525  256022.182122  59.351061   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10488    -1.978282      0.00159  0.0003 -0.037656  0.016818  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10488          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10489  73430000.0  258595.628333         244.494285  256044.365943  80.380186   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10489    -2.012734     0.002261  0.0003 -0.094607  0.017525  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10489          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10490  73437000.0  258617.379338         244.508697  256066.102486  94.887204   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10490    -1.973377     0.001865  0.0003 -0.313246  0.016718  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10490          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10491  73444000.0  258639.725517         244.524576  256088.432731  83.423689   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10491    -1.980334     0.002457  0.0003 -2.107251  0.017858  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10491          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10492  73451000.0  258661.130391         244.539108  256109.823018  97.940184   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10492    -2.002803     0.002795  0.0003 -2.425225  0.019065  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10492          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10493  73458000.0  258683.100078         244.554355  256131.777406  86.740427   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha    entropy  td_error    return  \\\n",
      "10493    -2.022628     0.003605  0.0003 -18.586074   0.02368  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10493          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10494  73465000.0  258705.059138          244.56899  256153.72178  100.862198   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10494    -2.019255     0.002554  0.0003 -8.897838  0.022343  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10494          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10495  73472000.0  258726.815717         244.583662  256175.463637   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10495  118.121788    -2.034866     0.001906  0.0003 -9.580994  0.025104   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10495  25.69932          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10496  73479000.0  258748.806531         244.599458  256197.438588  91.097534   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10496    -2.063767     0.003575  0.0003 -9.625264   0.02731  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10496          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10497  73486000.0  258771.086391         244.614464  256219.703381  72.405308   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10497    -2.038173     0.002277  0.0003 -7.173457  0.025173  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10497          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10498  73493000.0  258792.792581         244.628997  256241.394988  60.934626   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10498    -2.052206      0.00241  0.0003 -6.872262  0.024029  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10498          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10499  73500000.0  258814.070606         244.642164  256262.659795  64.019673   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10499    -2.037865     0.002854  0.0003 -6.895842  0.024727  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10499          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10500  73507000.0  258835.683557         244.657611  256284.257239  85.177705   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10500    -2.037624     0.002693  0.0003 -6.860721  0.023209  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10500          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10501  73514000.0  258857.491724         244.672129  256306.050836  47.644503   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10501    -2.028598     0.002053  0.0003 -5.352654  0.021753  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10501          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10502  73521000.0  258880.108379         244.685913  256328.653626  63.16147   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10502    -2.025845     0.003094  0.0003 -4.318754  0.021901  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10502          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10503  73528000.0  258905.404964         244.701892  256353.934179  58.56324   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10503    -2.019666     0.002336  0.0003 -4.530189  0.021276  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10503          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10504  73535000.0  258929.652815         244.715957  256378.167913  42.891484   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10504    -2.012212     0.003896  0.0003 -3.387701  0.021493  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10504          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10505  73542000.0  258951.144899         244.729841  256399.646057  59.189588   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10505    -2.004984     0.002949  0.0003 -3.067185  0.020274  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10505          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10506  73549000.0  258973.172077         244.743009  256421.660014  36.375776   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10506    -1.999579     0.002707  0.0003 -2.892285  0.019265  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10506          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10507  73556000.0  258994.580655         244.757395  256443.054154  87.591574   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10507    -1.992229     0.002458  0.0003 -1.785585  0.019053  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10507          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10508  73563000.0  259016.129605         244.772418  256464.588029  76.134406   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10508    -1.994818     0.002252  0.0003 -1.716853  0.017618  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10508          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10509  73570000.0  259038.141361         244.787442  256486.58471  106.965907   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10509    -2.003903     0.001464  0.0003 -1.657361  0.017688  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10509          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10510  73577000.0  259059.870111          244.80287  256508.297979  91.915876   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10510    -1.990742     0.001824  0.0003 -2.615718  0.018344  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10510          1000.0     -9.310139  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10511  73584000.0  259081.34961         244.818158  256529.762132  110.02323   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10511    -2.003112      0.00149  0.0003 -4.632355  0.018872  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10511          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10512  73591000.0  259102.708058         244.832884  256551.105787   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10512  101.537444    -2.016724     0.002571  0.0003 -5.647209  0.019275   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10512  25.69932          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10513  73598000.0  259124.159385          244.84627  256572.543673  84.668852   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10513    -2.019999     0.002301  0.0003 -6.560753  0.020915  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10513          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "10514  73605000.0  259145.882556         244.860502  256594.25256  82.17968   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10514    -2.022029     0.001768  0.0003 -4.855344   0.02104  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10514          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10515  73612000.0  259167.634018         244.875057  256615.989416  85.964829   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10515    -2.016696     0.002383  0.0003 -5.259724  0.023339  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10515          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10516  73619000.0  259189.217907         244.888657  256637.559653  85.618876   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10516    -2.035809     0.002073  0.0003 -5.921543  0.022148  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10516          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10517  73626000.0  259211.003756         244.904666  256659.329438  73.089346   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10517     -2.01789     0.002828  0.0003 -5.153121  0.024322  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10517          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10518  73633000.0  259232.623741         244.919122  256680.934911  76.622717   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10518    -2.027108     0.003487  0.0003 -4.727042  0.023566  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10518          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10519  73640000.0  259254.236252          244.93603  256702.530459  78.087121   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10519    -2.022531     0.003508  0.0003 -4.148295  0.023776  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10519          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10520  73647000.0  259275.818052         244.951278  256724.096948  50.776571   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10520    -2.018066     0.002416  0.0003 -4.759073  0.021653  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10520          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10521  73654000.0  259297.946247         244.965922  256746.210451  58.213286   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10521    -2.012892     0.002137  0.0003 -3.653459  0.021508  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10521          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10522  73661000.0  259319.585764         244.979767  256767.836064  50.700303   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10522    -2.020377     0.002939  0.0003 -3.443021  0.021426  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10522          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10523  73668000.0  259341.964204         244.994352  256790.199868  72.710648   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10523    -2.007461     0.001985  0.0003 -4.023008  0.020437  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10523          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10524  73675000.0  259364.647931         245.008491  256812.869383  57.197567   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10524    -2.003334     0.003024  0.0003 -2.449917  0.020415  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10524          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10525  73682000.0  259388.459723         245.022751  256836.666863  72.070442   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10525    -1.994437     0.002357  0.0003 -2.689056  0.019237  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10525          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10526  73689000.0  259412.691055         245.038277  256860.882618  81.869073   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10526     -2.00531     0.002097  0.0003 -2.003255  0.018709  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10526          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10527  73696000.0  259436.258206         245.052087  256884.435911   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10527  107.839793    -1.997781     0.003103  0.0003 -2.324651  0.019194   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10527  25.69932          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time   train_time      score  \\\n",
      "10528  73703000.0  259457.656502          245.06784  256905.8184  94.327115   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10528    -2.023753     0.002486  0.0003 -2.199126  0.020037  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10528          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10529  73710000.0  259478.595246           245.0815  256926.743431   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10529  102.320537    -2.018645     0.003469  0.0003 -3.369719  0.021028   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10529  25.69932          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10530  73717000.0  259499.983728         245.094937  256948.118421  92.014052   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10530    -2.012523      0.00364  0.0003 -4.342576  0.022194  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10530          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10531  73724000.0  259521.755575          245.11046  256969.874678  91.721773   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10531    -2.017664     0.003414  0.0003 -5.080322  0.023062  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10531          1000.0     -9.310139  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "10532  73731000.0  259543.76129         245.126653  256991.864145  81.49644   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10532    -2.026033     0.002044  0.0003 -2.987865  0.021109  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10532          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10533  73738000.0  259565.448616         245.142093  257013.535973  79.996563   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10533    -2.024423     0.004188  0.0003 -3.358999  0.023266  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10533          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10534  73745000.0  259587.772697         245.156746  257035.845347  83.777219   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10534    -2.013578     0.001901  0.0003 -2.787976  0.020602  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10534          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10535  73752000.0  259609.725774         245.171404  257057.783716  76.828666   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10535    -2.017038     0.001955  0.0003 -3.275282  0.020925  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10535          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10536  73759000.0  259631.618375         245.185931  257079.661735  77.088408   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10536    -2.014562     0.002806  0.0003 -2.629695  0.020767  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10536          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10537  73766000.0  259653.495987         245.200422  257101.524778  69.282293   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10537    -2.033579     0.001649  0.0003 -1.910668    0.0191  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10537          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10538  73773000.0  259675.715416          245.21766  257123.726918   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10538  103.278145    -2.016082     0.002021  0.0003 -2.438156  0.019296   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10538  25.69932          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10539  73780000.0  259698.015569         245.231756  257146.012925   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10539  113.281913    -2.023938     0.001769  0.0003 -1.820892  0.019764   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10539  25.69932          1000.0     -9.310139  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10540  73787000.0  259719.91905         245.245952  257167.902158  90.300843   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10540    -2.008135      0.00164  0.0003 -1.819602  0.018712  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10540          1000.0     -9.310139  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10541  73794000.0  259742.16858         245.260349  257190.137243  89.578018   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10541    -2.027966      0.00271  0.0003 -2.308677  0.020489  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10541          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10542  73801000.0  259764.814375         245.274856  257212.768464  75.093992   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10542    -2.033202     0.002418  0.0003 -2.523409  0.019658  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10542          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10543  73808000.0  259787.374816         245.290604  257235.313103  96.084252   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10543    -2.019103     0.003257  0.0003 -3.30601  0.020534  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10543          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10544  73815000.0  259809.356486         245.304269  257257.281057  69.502905   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10544    -2.019242     0.002395  0.0003 -3.431115  0.020487  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10544          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10545  73822000.0  259831.827852         245.320004  257279.736636  72.10096   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10545    -2.035105     0.002424  0.0003 -3.008161  0.020558  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10545          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10546  73829000.0  259856.874132         245.334519  257304.768346  73.588036   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10546    -2.031182     0.002768  0.0003 -3.202718  0.020873  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10546          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time   train_time      score  \\\n",
      "10547  73836000.0  259881.747166         245.348643  257329.6272  91.227278   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10547    -2.032792     0.003069  0.0003 -3.88594  0.021392  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10547          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10548  73843000.0  259905.689572         245.362338  257353.555855  87.698832   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10548     -2.02534     0.002205  0.0003 -3.849644   0.02065  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10548          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10549  73850000.0  259931.069127         245.382305  257378.915392  85.92689   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10549    -2.033445     0.002884  0.0003 -3.241292  0.021831  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10549          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10550  73857000.0  259955.523177         245.398434  257403.353237  69.435596   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10550    -2.027264     0.002223  0.0003 -3.142468  0.020978  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10550          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10551  73864000.0  259980.285711         245.414955  257428.099197  91.000952   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10551    -2.038793     0.002176  0.0003 -2.418716   0.02019  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10551          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10552  73871000.0  260004.489441         245.430866  257452.286963  84.737994   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10552    -2.015919     0.002586  0.0003 -2.421374   0.02103  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10552          1000.0     -9.310139  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "10553  73878000.0  260026.75527         245.445266  257474.538338  114.600822   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10553     -2.02324     0.002975  0.0003 -2.865735  0.021971  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10553          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10554  73885000.0  260049.188368         245.461773  257496.954875  83.857022   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10554    -2.016786     0.002315  0.0003 -2.832645  0.020408  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10554          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10555  73892000.0  260070.660266          245.47676  257518.411738  88.798746   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10555    -2.010589     0.002394  0.0003 -2.888499  0.020277  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10555          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10556  73899000.0  260092.641715         245.491913  257540.377981  94.854593   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10556    -2.036745      0.00352  0.0003 -2.164579  0.021177  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10556          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10557  73906000.0  260114.873854          245.50631  257562.595663   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10557  115.903549    -2.014785     0.002667  0.0003 -2.645673  0.020753   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10557  25.69932          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10558  73913000.0  260136.844807         245.520015  257584.552855  94.990738   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10558    -2.017994     0.002127  0.0003 -2.81388  0.020236  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10558          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10559  73920000.0  260158.916434         245.534755  257606.609695  97.438224   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10559    -2.032488     0.002762  0.0003 -1.965481  0.020008  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10559          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10560  73927000.0  260180.923894         245.547401  257628.604454  78.19882   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10560    -2.028566     0.001954  0.0003 -1.765464  0.019804  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10560          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10561  73934000.0  260203.035158         245.561938  257650.70112  106.033519   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10561    -2.024188     0.003232  0.0003 -2.021386  0.021104  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10561          1000.0     -9.310139  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10562  73941000.0  260224.93169         245.577216  257672.582319  94.908446   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10562    -2.029003     0.002343  0.0003 -1.505465  0.020388  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10562          1000.0     -9.310139  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10563  73948000.0  260246.777964         245.592478  257694.413277  66.805139   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10563     -2.00147     0.002659  0.0003 -1.400848  0.020743  25.69932   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10563          1000.0     -9.310139  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10564  73955000.0  260285.914773         245.606105  257716.478584  89.777969   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10564    -2.030131     0.002816  0.0003 -1.776932  0.020598  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10564          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10565  73962000.0  260308.865483         245.620464  257739.414878  96.945196   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10565      -2.0422     0.002173  0.0003 -2.688835   0.01976  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10565          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10566  73969000.0  260331.022698         245.636678  257761.555815  75.425409   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10566     -2.01682     0.002609  0.0003 -2.133217  0.020056  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10566          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10567  73976000.0  260352.761019         245.652519  257783.278235  86.844459   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10567    -2.025677     0.002349  0.0003 -2.206533  0.020038  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10567          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10568  73983000.0  260374.116884         245.668305  257804.618248  80.930836   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10568    -2.031548      0.00296  0.0003 -2.131382  0.020382  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10568          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10569  73990000.0  260395.074846         245.688093  257825.556297   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10569  115.568578    -2.027422     0.002645  0.0003 -2.053838  0.020235   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10569  28.165409          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10570  73997000.0  260415.729081         245.707754  257846.190654  89.018129   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10570    -2.006921      0.00214  0.0003 -2.716761  0.020733  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10570          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10571  74004000.0  260436.843858          245.72759  257867.285538  82.905591   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10571    -2.041442     0.002431  0.0003 -2.643451  0.022008  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10571          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10572  74011000.0  260458.174978         245.743114  257888.601075  78.539513   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10572    -2.031725     0.002443  0.0003 -2.531479  0.020961  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10572          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10573  74018000.0  260479.842254         245.759831  257910.251573  96.406087   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10573    -2.044717     0.002775  0.0003 -2.216635  0.020527  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10573          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10574  74025000.0  260501.514948         245.774257  257931.90976  85.528828   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10574    -2.049347     0.003239  0.0003 -2.309086  0.020818  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10574          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10575  74032000.0  260523.005573         245.791373  257953.383214  77.112705   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10575    -2.031201     0.002101  0.0003 -2.294826  0.020281  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10575          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10576  74039000.0  260544.622497         245.805738  257974.985723  80.341019   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10576    -2.022519     0.002117  0.0003 -2.608204  0.020464  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10576          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10577  74046000.0  260568.444725         245.821126  257998.792512  82.136981   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10577    -2.049394     0.003289  0.0003 -2.241483  0.020948  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10577          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "10578  74053000.0  260592.548264         245.836082  258022.88104  88.70855   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10578    -2.011325     0.002615  0.0003 -1.538256  0.020099  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10578          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10579  74060000.0  260617.366443         245.851863  258047.683383  73.60816   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10579    -2.030897     0.002093  0.0003 -2.106901  0.019343  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10579          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10580  74067000.0  260642.365899         245.868373  258072.66627  69.964081   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10580    -2.024687      0.00304  0.0003 -1.816567  0.020806  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10580          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10581  74074000.0  260666.854959         245.882545  258097.141106  67.312784   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10581     -2.01638     0.002038  0.0003 -1.354565  0.019467  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10581          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10582  74081000.0  260692.016846         245.897809  258122.287645  95.108844   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10582    -2.037451     0.002428  0.0003 -1.91616  0.019513  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10582          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10583  74088000.0  260715.590738          245.91321  258145.846085  71.26376   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10583    -2.038082     0.002277  0.0003 -1.49596  0.019698  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10583          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10584  74095000.0  260739.690053         245.927971  258169.930583  90.422362   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10584    -2.029712      0.00196  0.0003 -1.176526  0.019988  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10584          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10585  74102000.0  260763.673781          245.94317  258193.899059  86.720213   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10585    -2.028142     0.001763  0.0003 -1.376264  0.019261  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10585          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10586  74109000.0  260787.557021         245.958891  258217.766518  60.430132   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10586    -2.018468     0.002296  0.0003 -1.549734  0.018993  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10586          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10587  74116000.0  260811.489799         245.973859  258241.684252  83.600082   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10587    -2.033399     0.002462  0.0003 -0.950689  0.020033  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10587          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10588  74123000.0  260836.304033         245.988455  258266.483825  92.922755   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10588    -2.035527     0.002514  0.0003 -1.082411  0.019879  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10588          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10589  74130000.0  260860.934686         246.016101  258291.08678  91.366418   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10589      -2.0283     0.002743  0.0003 -1.290041  0.019032  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10589          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10590  74137000.0  260883.705907         246.032156  258313.841889  94.158694   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10590    -2.044578     0.002617  0.0003 -1.178826  0.019301  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10590          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10591  74144000.0  260904.879913         246.046979  258335.001019  88.964842   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10591    -2.026983     0.003033  0.0003 -1.801465  0.020442  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10591          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10592  74151000.0  260926.016817         246.062371  258356.122459  62.969618   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10592    -2.051028     0.002315  0.0003 -2.487755  0.019788  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10592          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10593  74158000.0  260947.661896         246.079025  258377.750831   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "10593  104.204223    -2.035403     0.002393  0.0003 -2.47351    0.0205   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10593  28.165409          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10594  74165000.0  260969.494287         246.094271  258399.567904  96.136734   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10594    -2.042655     0.003261  0.0003 -1.944607  0.021187  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10594          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10595  74172000.0  260990.877658         246.108292  258420.937201  99.22085   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10595    -2.057588     0.002497  0.0003 -2.206459  0.019689  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10595          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10596  74179000.0  261012.051459          246.12257  258442.096672  98.595858   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10596    -2.041172     0.002266  0.0003 -1.450777  0.019242  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10596          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10597  74186000.0  261033.981706         246.137289  258464.012148   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10597  104.292341    -2.043954     0.002818  0.0003 -1.419091  0.019743   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10597  28.165409          1000.0     -8.101072  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10598  74193000.0  261055.52292         246.151829  258485.538763  94.478466   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10598     -2.03413     0.002729  0.0003 -1.422955  0.020575  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10598          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10599  74200000.0  261077.510059         246.167084  258507.510592  85.824199   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10599    -2.035078      0.00282  0.0003 -1.303919  0.019632  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10599          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10600  74207000.0  261100.191866         246.181484  258530.177954  87.660924   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10600    -2.038801     0.002891  0.0003 -1.993577  0.021227  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10600          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10601  74214000.0  261122.398793         246.196806  258552.369503  94.423585   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10601    -2.043266     0.003161  0.0003 -1.26777  0.020123  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10601          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10602  74221000.0  261144.220798          246.21085  258574.17741  69.462121   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10602    -2.034098     0.003046  0.0003 -1.044146  0.020378  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10602          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10603  74228000.0  261166.006762         246.226684  258595.947484  98.756626   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10603     -2.04549     0.003171  0.0003 -0.842051  0.020115  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10603          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10604  74235000.0  261187.575381         246.241359  258617.501374  94.381034   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10604    -2.020962     0.003591  0.0003 -1.145138  0.021244  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10604          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10605  74242000.0  261210.363809         246.255803  258640.275308  80.319703   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10605    -2.029059     0.002309  0.0003 -1.155675  0.019163  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10605          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10606  74249000.0  261232.180546         246.271668  258662.076125  94.359532   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10606    -2.046381     0.002586  0.0003 -0.54581  0.019358  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10606          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10607  74256000.0  261254.524327         246.288515  258684.403007  96.253887   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10607    -2.029727     0.001864  0.0003 -0.913828  0.018667  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10607          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10608  74263000.0  261276.059684         246.304114  258705.922701  90.671107   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10608      -2.0372     0.003735  0.0003 -0.806886  0.020224  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10608          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10609  74270000.0  261297.839596         246.320271  258727.686402  85.85988   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10609    -2.035557     0.003317  0.0003 -1.150187  0.019862  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10609          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10610  74277000.0  261319.455604         246.351836  258749.270793   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10610  107.330486    -2.052074     0.001984  0.0003 -0.911926  0.019402   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10610  28.165409          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10611  74284000.0  261340.559884         246.368086  258770.358764   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10611  103.595003    -2.035784      0.00293  0.0003 -1.318213  0.019951   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10611  28.165409          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10612  74291000.0  261361.915948         246.384196  258791.698663  92.37285   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10612    -2.051338     0.003985  0.0003 -1.496614  0.020956  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10612          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10613  74298000.0  261383.119887           246.3992  258812.88754  104.328386   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10613    -2.039162      0.00392  0.0003 -1.006007  0.020875  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10613          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10614  74305000.0  261404.882051         246.413871  258834.63496  77.905275   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10614    -2.060802     0.002243  0.0003 -1.009308  0.019836  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10614          1000.0     -8.101072  \n",
      "         timestep          time  collect_wait_time     train_time   score  \\\n",
      "10615  74312000.0  261426.45186         246.429639  258856.188952  93.695   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10615     -2.03928     0.002286  0.0003 -1.173992   0.01919  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10615          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10616  74319000.0  261448.734805         246.444625  258878.45685  105.634362   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10616    -2.053106     0.002402  0.0003 -0.535326  0.018364  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10616          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10617  74326000.0  261470.457482          246.45972  258900.164377  63.459657   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10617    -2.060006     0.002589  0.0003 -0.628859  0.019385  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10617          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10618  74333000.0  261492.492746         246.473987  258922.185322  85.931815   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10618    -2.044638     0.002476  0.0003 -0.407308  0.018591  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10618          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10619  74340000.0  261514.596118         246.489661  258944.272967  96.572258   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10619    -2.052476     0.002919  0.0003 -0.298526   0.01926  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10619          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10620  74347000.0  261536.354208         246.518492  258966.002172  71.344994   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10620    -2.051103     0.002332  0.0003 -0.171284  0.018123  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10620          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10621  74354000.0  261558.411233         246.533398  258988.044237  91.184773   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10621    -2.058088     0.001745  0.0003 -0.053018  0.018305  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10621          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10622  74361000.0  261580.071478         246.549564  259009.688262  82.811289   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10622     -2.02467     0.001791  0.0003 -0.098677  0.017708  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10622          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10623  74368000.0  261604.202699           246.5654  259033.803572  71.43569   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10623    -2.042481     0.002396  0.0003 -0.75935  0.019634  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10623          1000.0     -8.101072  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10624  74375000.0  261628.98665         246.581026  259058.571795  95.276155   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10624    -2.046711     0.002205  0.0003 -0.788882  0.018905  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10624          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10625  74382000.0  261653.367127         246.602747  259082.93049  99.955812   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10625    -2.051264     0.002772  0.0003 -1.156596   0.01962  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10625          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10626  74389000.0  261675.434609         246.618427  259104.982238  95.847294   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10626    -2.046554     0.002319  0.0003 -1.160443  0.019153  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10626          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10627  74396000.0  261697.116814         246.633427  259126.64939  87.398291   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10627    -2.055128     0.002043  0.0003 -0.871683  0.019294  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10627          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10628  74403000.0  261719.385089         246.647543  259148.903497  93.465649   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10628    -2.053191     0.002177  0.0003 -1.423222  0.019129  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10628          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10629  74410000.0  261742.007535         246.663171  259171.51026  120.644115   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10629      -2.0451     0.002727  0.0003 -1.266938  0.019797  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10629          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10630  74417000.0  261763.905041         246.678234  259193.392646  95.296666   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10630    -2.069097     0.002859  0.0003 -1.276369  0.020079  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10630          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10631  74424000.0  261785.638427         246.694453  259215.109762  73.442891   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10631    -2.067364     0.002119  0.0003 -0.709706  0.019948  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10631          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10632  74431000.0  261807.646353         246.708382  259237.103703  91.483362   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10632    -2.048729     0.002065  0.0003 -1.023926  0.018896  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10632          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10633  74438000.0  261829.987303         246.722825  259259.430133  99.655807   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10633     -2.07818     0.002514  0.0003 -1.281706  0.019534  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10633          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10634  74445000.0  261852.218115         246.738123  259281.645624  83.169416   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10634    -2.050464     0.002603  0.0003 -1.286189  0.019705  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10634          1000.0     -8.101072  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "10635  74452000.0  261874.11793         246.752766  259303.530741  104.651399   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10635    -2.057979      0.00268  0.0003 -1.902317  0.020926  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10635          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10636  74459000.0  261895.976398         246.768309  259325.373606  103.43837   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10636    -2.081071     0.003768  0.0003 -1.305816  0.020699  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10636          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10637  74466000.0  261917.301646         246.783251  259346.683854   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10637  114.259563    -2.068447      0.00283  0.0003 -1.507754  0.019663   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10637  28.165409          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10638  74473000.0  261939.251351          246.79942  259368.617337  83.664112   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10638    -2.049911     0.003099  0.0003 -1.069811  0.020041  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10638          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10639  74480000.0  261960.799085         246.815054  259390.149382  74.731678   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10639    -2.076948      0.00252  0.0003 -0.702685  0.019348  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10639          1000.0     -8.101072  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "10640  74487000.0  261982.69786         246.827575  259412.035587  83.50087   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10640    -2.076306       0.0024  0.0003 -2.062683  0.020258  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10640          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10641  74494000.0  262004.148474         246.843865  259433.469855   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10641  100.149609    -2.086196     0.002204  0.0003 -1.212359  0.019362   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10641  28.165409          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10642  74501000.0  262025.725158         246.858548  259455.031803  82.035765   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10642    -2.064424     0.003168  0.0003 -1.040441  0.019917  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10642          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10643  74508000.0  262047.196131         246.875608  259476.485661  70.66489   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10643     -2.08008     0.003263  0.0003 -1.093265  0.020129  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10643          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10644  74515000.0  262069.141885         246.895459  259498.411474  99.538043   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10644    -2.066506     0.002932  0.0003 -0.821479  0.020357  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10644          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10645  74522000.0  262091.108747         246.912171  259520.361569  67.38238   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10645    -2.065249      0.00401  0.0003 -1.660136  0.020992  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10645          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10646  74529000.0  262115.364178         246.927091  259544.602021  62.939696   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10646    -2.067278     0.002658  0.0003 -0.988225  0.019838  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10646          1000.0     -8.101072  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "10647  74536000.0  262139.73921         246.941768  259568.962312  94.80004   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10647    -2.077975     0.002266  0.0003 -1.101271  0.018981  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10647          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10648  74543000.0  262163.298475         246.958972  259592.504317  83.985746   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10648    -2.081074       0.0028  0.0003 -0.559846  0.020692  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10648          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10649  74550000.0  262187.525657         246.973863  259616.71655  79.076186   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10649    -2.063492     0.002702  0.0003 -0.694066  0.019323  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10649          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10650  74557000.0  262211.378443         246.988528  259640.554617  95.974885   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10650    -2.091524     0.002035  0.0003 -0.514807  0.018148  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10650          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10651  74564000.0  262235.474026         247.001785  259664.636863  81.384584   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10651    -2.065179     0.002192  0.0003  0.087888  0.018219  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10651          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10652  74571000.0  262259.833657         247.023664  259688.974561   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10652  110.013172     -2.07085      0.00278  0.0003  0.277739  0.018783   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10652  28.165409          1000.0     -8.101072  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "10653  74578000.0  262284.64362         247.039481  259713.768656  75.78066   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10653    -2.073111     0.002189  0.0003 -0.071011  0.019025  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10653          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10654  74585000.0  262308.975899         247.054735  259738.085623  80.302209   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10654     -2.07371     0.003453  0.0003 -0.403723  0.019138  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10654          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10655  74592000.0  262333.368097         247.069711  259762.462791  92.570819   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10655    -2.058938     0.002331  0.0003 -0.390123  0.018514  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10655          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10656  74599000.0  262358.175143         247.084771  259787.25467  111.379728   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10656    -2.062744     0.003463  0.0003  0.155898  0.019161  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10656          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10657  74606000.0  262382.412282         247.100044  259811.476482  80.21038   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10657    -2.074501     0.003662  0.0003  0.267717  0.019908  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10657          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10658  74613000.0  262406.972113         247.114522  259836.021767   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10658  112.453945     -2.07124     0.006315  0.0003 -0.260787  0.021675   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10658  28.165409          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10659  74620000.0  262431.717129         247.130456  259860.750786  96.518122   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10659     -2.07149     0.002666  0.0003 -0.224418  0.019119  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10659          1000.0     -8.101072  \n",
      "         timestep         time  collect_wait_time    train_time      score  \\\n",
      "10660  74627000.0  262453.5209         247.145103  259882.53986  78.436613   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10660    -2.068597     0.002415  0.0003 -0.78475  0.019529  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10660          1000.0     -8.101072  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10661  74634000.0  262475.74187         247.160807  259904.745072  94.441727   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10661    -2.084006     0.002881  0.0003 -0.503806  0.019781  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10661          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10662  74641000.0  262497.717671         247.175936  259926.70569  107.728951   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10662    -2.069492     0.002339  0.0003 -1.049546  0.020066  28.165409   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10662          1000.0     -8.101072  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10663  74648000.0  262519.859688         247.191926  259948.831662   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10663  117.643833    -2.080183      0.00181  0.0003 -1.348626  0.019099   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10663  28.165409          1000.0     -8.101072  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10664  74655000.0  262559.037558         247.207226  259971.114903  63.108996   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10664    -2.086335     0.002004  0.0003 -1.399882  0.019545  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10664          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10665  74662000.0  262583.617856         247.224065  259995.678299  77.339806   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10665    -2.089674     0.002685  0.0003 -1.145194   0.01988  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10665          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10666  74669000.0  262608.354114         247.240059  260020.398506  82.424119   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10666    -2.093615     0.002602  0.0003 -1.296725  0.021234  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10666          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10667  74676000.0  262632.450104         247.255644  260044.478856   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10667  106.317386    -2.087904     0.001808  0.0003 -1.155406  0.019538   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10667  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10668  74683000.0  262656.817088         247.270797  260068.830637  91.82243   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10668    -2.093562     0.001899  0.0003 -0.440935  0.018977  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10668          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10669  74690000.0  262681.186501         247.287041  260093.183753  74.677892   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10669    -2.105249     0.003358  0.0003 -0.572379  0.020611  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10669          1000.0     -4.032137  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10670  74697000.0  262705.09468         247.301788  260117.077118  91.153413   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10670    -2.085414     0.003217  0.0003 -0.780326  0.021131  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10670          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10671  74704000.0  262726.806002         247.316395  260138.773782  80.556832   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10671    -2.081205     0.002515  0.0003 -1.21655  0.019996  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10671          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10672  74711000.0  262748.537429         247.331204  260160.490346  83.574548   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10672    -2.093654     0.003543  0.0003 -0.417728  0.021087  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10672          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "10673  74718000.0  262770.710268         247.347281  260182.64705  95.49287   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10673    -2.093247     0.002187  0.0003 -0.180235  0.018912  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10673          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10674  74725000.0  262792.797041         247.361515  260204.719536  66.503696   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10674     -2.08965     0.003157  0.0003  0.231468  0.020069  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10674          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10675  74732000.0  262815.049305         247.377184  260226.956078  98.810561   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10675    -2.079121     0.002375  0.0003  0.167657   0.01841  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10675          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10676  74739000.0  262837.149909         247.391812  260249.042006  78.186236   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10676    -2.066541     0.004615  0.0003 -0.147621  0.020774  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10676          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "10677  74746000.0  262859.611305         247.406068  260271.48909  94.20342   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10677    -2.092323     0.002834  0.0003 -0.045186  0.019856  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10677          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10678  74753000.0  262883.265073          247.42227  260295.126603   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10678  101.109356    -2.096539     0.002256  0.0003 -0.852436  0.019208   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10678  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10679  74760000.0  262907.639569         247.437188  260319.486123  71.461737   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10679    -2.090487     0.003102  0.0003 -0.897551  0.019925  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10679          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10680  74767000.0  262929.749405          247.45265  260341.580434   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10680  109.723531    -2.105094     0.002489  0.0003 -0.504799  0.020701   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10680  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10681  74774000.0  262951.652148         247.468672  260363.46709  86.204231   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10681    -2.067449     0.003393  0.0003 -1.805211  0.021614  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10681          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10682  74781000.0  262973.435633          247.48195  260385.237245  77.626415   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10682     -2.10172     0.003314  0.0003 -0.711058  0.020866  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10682          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10683  74788000.0  262995.763199         247.496455  260407.550251  84.107682   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10683    -2.107349     0.002562  0.0003 -0.665207  0.020103  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10683          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10684  74795000.0  263017.831368         247.511484  260429.603336  89.509785   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10684    -2.074252      0.00292  0.0003 -0.501408  0.021233  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10684          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10685  74802000.0  263040.288613         247.527025  260452.044988  82.478933   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10685    -2.087459     0.002799  0.0003 -0.631625  0.019882  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10685          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10686  74809000.0  263062.983323         247.541136  260474.725535  84.203288   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10686     -2.07591     0.002174  0.0003 -0.490567  0.019641  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10686          1000.0     -4.032137  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10687  74816000.0  263084.50095         247.556188  260496.228054  92.444226   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10687    -2.089472     0.002894  0.0003  0.009264   0.02016  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10687          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10688  74823000.0  263106.095487         247.577661  260517.801063  85.097536   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10688    -2.102482     0.002199  0.0003  0.092323   0.01791  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10688          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10689  74830000.0  263128.249822         247.593078  260539.939928   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "10689  106.315866    -2.092867     0.002165  0.0003  0.02709  0.018869   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10689  21.006897          1000.0     -4.032137  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10690  74837000.0  263150.46248         247.608262  260562.137348  105.80308   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10690    -2.095814       0.0028  0.0003  0.131077  0.018946  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10690          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10691  74844000.0  263172.804217         247.623077  260584.464216   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10691  104.888007    -2.098776     0.001842  0.0003 -0.073529  0.017922   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10691  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10692  74851000.0  263195.127621         247.640189  260606.770422   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10692  106.920056    -2.100094     0.002665  0.0003 -0.240868   0.01853   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10692  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10693  74858000.0  263217.512627         247.657683  260629.137879  81.944041   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10693    -2.091004     0.002364  0.0003 -1.079584  0.019045  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10693          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10694  74865000.0  263240.008228         247.671501  260651.619609  86.498953   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10694     -2.10127     0.003363  0.0003 -1.834704  0.021495  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10694          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10695  74872000.0  263261.825694         247.686213  260673.422314  72.863035   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10695    -2.096344     0.002824  0.0003 -1.559923  0.021193  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10695          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10696  74879000.0  263283.829292          247.70032  260695.411752  87.27397   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10696    -2.092509     0.002439  0.0003 -1.952302  0.021243  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10696          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10697  74886000.0  263305.839339         247.715476  260717.406571  82.049653   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10697    -2.090328     0.002898  0.0003 -2.01734  0.021634  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10697          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10698  74893000.0  263328.246044          247.72987  260739.79883  97.059578   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10698    -2.107289     0.002937  0.0003 -1.874421  0.021789  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10698          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10699  74900000.0  263352.480187         247.745298  260764.017493   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10699  102.493629    -2.116338       0.0021  0.0003 -2.060192  0.020754   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10699  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10700  74907000.0  263376.859481         247.760183  260788.381843   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10700  102.507256    -2.109428     0.002081  0.0003 -2.723471   0.02151   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10700  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10701  74914000.0  263401.812181         247.775231  260813.319444  87.722771   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10701    -2.105539     0.002495  0.0003 -2.883144  0.021642  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10701          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10702  74921000.0  263426.427157         247.788376  260837.921194  69.374248   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10702    -2.128086     0.003858  0.0003 -5.972576  0.023887  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10702          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10703  74928000.0  263450.848383         247.802945  260862.327799  72.193138   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10703     -2.14008     0.003197  0.0003 -4.636756  0.023262  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10703          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10704  74935000.0  263475.603324         247.819114  260887.066518  67.107707   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10704    -2.129504     0.002972  0.0003 -4.82661  0.022527  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10704          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10705  74942000.0  263497.812407          247.83514  260909.25952  67.694269   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10705    -2.134125     0.002314  0.0003 -3.693135  0.021775  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10705          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10706  74949000.0  263519.992702         247.850656  260931.424228  59.259765   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10706    -2.145667     0.002788  0.0003 -3.911574  0.021828  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10706          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10707  74956000.0  263542.108847         247.865259  260953.525709  65.717779   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10707    -2.157028     0.002421  0.0003 -2.58387  0.021616  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10707          1000.0     -4.032137  \n",
      "         timestep        time  collect_wait_time     train_time      score  \\\n",
      "10708  74963000.0  263563.715         247.879987  260975.117043  49.829293   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10708    -2.133665     0.002213  0.0003 -2.627229  0.021063  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10708          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10709  74970000.0  263585.630491         247.896746  260997.015718  41.834667   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10709    -2.123502     0.002236  0.0003 -2.461497  0.020196  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10709          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10710  74977000.0  263607.737874         247.911924  261019.107857  37.79212   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10710    -2.128689      0.00293  0.0003 -2.273054  0.020559  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10710          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10711  74984000.0  263629.684691         247.928061  261041.038489  51.868059   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10711    -2.127043     0.002102  0.0003 -2.46036  0.020101  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10711          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10712  74991000.0  263651.497388         247.942052  261062.837146  41.099608   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10712    -2.099926     0.002254  0.0003 -2.214196  0.020007  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10712          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10713  74998000.0  263673.602872         247.956271  261084.928359  57.111297   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10713    -2.113262     0.002574  0.0003 -1.432684  0.019165  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10713          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10714  75005000.0  263695.752971         247.970951  261107.063723  28.979614   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10714    -2.106238     0.001974  0.0003 -0.832628   0.01899  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10714          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10715  75012000.0  263717.885858         247.988373  261129.179135  84.698051   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10715      -2.1082     0.003559  0.0003 -0.310838  0.019041  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10715          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10716  75019000.0  263740.086834         248.003786  261151.364644  37.077621   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10716    -2.106585      0.00249  0.0003 -0.561046  0.018973  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10716          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10717  75026000.0  263762.434278         248.021759  261173.694065  97.020403   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10717    -2.135307     0.002397  0.0003 -1.695548  0.018724  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10717          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10718  75033000.0  263784.666929         248.036937  261195.911483  94.763309   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10718    -2.118433     0.002043  0.0003 -1.840681    0.0192  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10718          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10719  75040000.0  263806.777253         248.052218  261218.006467  87.800257   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10719    -2.118602     0.002074  0.0003 -3.444466  0.020769  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10719          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10720  75047000.0  263830.189776          248.07166  261241.399496  89.563452   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10720    -2.123371     0.002707  0.0003 -3.742877  0.021531  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10720          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10721  75054000.0  263852.208372         248.086401  261263.403284  94.353355   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10721    -2.114372      0.00344  0.0003 -2.947816  0.022148  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10721          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10722  75061000.0  263874.192618         248.101648  261285.372229  95.755495   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10722    -2.137706     0.002507  0.0003 -2.531039  0.021503  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10722          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10723  75068000.0  263895.922577         248.116948  261307.086832   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10723  119.707741    -2.136974     0.003454  0.0003 -2.101415  0.022072   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10723  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10724  75075000.0  263918.181532         248.131305  261329.331378  70.667259   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10724    -2.142582      0.00418  0.0003 -2.417277  0.021817  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10724          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10725  75082000.0  263939.633161         248.147016  261350.76724  69.924921   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10725    -2.133359     0.002768  0.0003 -2.080104  0.021189  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10725          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10726  75089000.0  263960.850791         248.163861  261371.967967  62.561058   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10726    -2.122858     0.004182  0.0003 -2.033659  0.021609  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10726          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10727  75096000.0  263982.815705          248.17948  261393.917164  79.227567   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10727    -2.121659     0.002666  0.0003 -0.975935   0.02052  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10727          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10728  75103000.0  264004.341085         248.209793  261415.412175  70.027821   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10728    -2.132844     0.002938  0.0003 -1.269466  0.019933  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10728          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10729  75110000.0  264025.729539         248.223753  261436.786619  35.224885   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10729     -2.12898     0.004029  0.0003 -1.302708  0.020425  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10729          1000.0     -4.032137  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10730  75117000.0  264047.00922         248.240474  261458.049531  68.318099   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10730    -2.112802     0.003506  0.0003 -1.331016  0.020201  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10730          1000.0     -4.032137  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10731  75124000.0  264068.39596         248.268506  261479.408169  94.458565   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10731    -2.133394     0.001811  0.0003 -1.982932  0.019558  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10731          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "10732  75131000.0  264089.599878         248.284619  261500.595918  79.1049   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10732    -2.123862     0.003096  0.0003 -2.126351  0.020572  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10732          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10733  75138000.0  264111.253798         248.299078  261522.235328  90.845831   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10733    -2.117177     0.002265  0.0003 -1.960004  0.020368  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10733          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10734  75145000.0  264132.726329          248.31325  261543.693636  81.845775   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10734    -2.119426     0.003219  0.0003 -1.435897  0.022195  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10734          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10735  75152000.0  264154.008739         248.330405  261564.958823  91.624252   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10735    -2.130027     0.002673  0.0003 -1.651659  0.021181  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10735          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10736  75159000.0  264175.185359         248.346181  261586.11961  91.540389   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10736    -2.113109     0.002782  0.0003 -1.278566  0.020575  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10736          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10737  75166000.0  264196.480275         248.360649  261607.399999  82.33957   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10737    -2.117073     0.002154  0.0003 -0.691642  0.019948  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10737          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10738  75173000.0  264217.781447          248.37594  261628.685828   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10738  103.194833    -2.132507     0.001983  0.0003 -1.554652  0.019227   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10738  21.006897          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10739  75180000.0  264239.417754         248.391119  261650.306904  94.407462   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10739    -2.128722     0.002612  0.0003 -0.971577  0.019877  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10739          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10740  75187000.0  264261.097121         248.406621  261671.970711  85.443534   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10740    -2.134637     0.002349  0.0003 -0.800269  0.019439  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10740          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10741  75194000.0  264283.507104         248.420069  261694.367187  56.502298   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10741    -2.129704     0.002338  0.0003 -1.035321  0.019258  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10741          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10742  75201000.0  264307.113068         248.438647  261717.954434  82.309939   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10742    -2.117987     0.002876  0.0003 -0.621954   0.01918  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10742          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10743  75208000.0  264328.491382          248.45434  261739.316998  82.760908   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10743    -2.120259     0.002674  0.0003 -0.71918  0.019809  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10743          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10744  75215000.0  264349.425985         248.469899  261760.235984  92.946693   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10744    -2.120488     0.002178  0.0003 -1.055569  0.019858  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10744          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10745  75222000.0  264370.740083          248.48592  261781.534003  86.269923   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10745    -2.107069      0.00303  0.0003 -1.054046  0.020076  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10745          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10746  75229000.0  264391.842191         248.519576  261802.60239  87.092198   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10746     -2.11921     0.003296  0.0003 -0.73934  0.020937  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10746          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10747  75236000.0  264413.070382         248.534313  261823.815785  79.687357   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10747    -2.134194     0.003222  0.0003 -0.806196  0.020907  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10747          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10748  75243000.0  264434.204176         248.550686  261844.933154  99.823825   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10748    -2.118903     0.002516  0.0003 -0.653557  0.019939  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10748          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10749  75250000.0  264455.795268         248.564283  261866.51056  108.235411   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10749    -2.117628     0.002404  0.0003  0.09472  0.019523  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10749          1000.0     -4.032137  \n",
      "         timestep          time  collect_wait_time    train_time      score  \\\n",
      "10750  75257000.0  264477.19479         248.581727  261887.89258  90.220772   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10750    -2.112153     0.001916  0.0003 -0.329629  0.019111  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10750          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10751  75264000.0  264498.889113         248.597344  261909.571236  95.506802   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10751    -2.123506      0.00214  0.0003 -0.205715  0.019259  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10751          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10752  75271000.0  264520.005487         248.612078  261930.672825  68.05969   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10752     -2.10586     0.002661  0.0003 -0.133503  0.019508  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10752          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10753  75278000.0  264541.553084         248.627146  261952.205303  70.272245   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10753    -2.102279     0.001873  0.0003  0.129413  0.018382  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10753          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10754  75285000.0  264562.982086         248.642127  261973.619276  88.930823   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10754     -2.11584     0.002773  0.0003  -0.0424  0.019522  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10754          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10755  75292000.0  264584.042589         248.655738  261994.666111  95.127754   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10755    -2.108168     0.002351  0.0003 -0.013589  0.018676  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10755          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10756  75299000.0  264605.216547         248.670911  262015.824844  89.615233   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10756    -2.114571      0.00256  0.0003  0.109524  0.018775  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10756          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10757  75306000.0  264627.056836         248.685203  262037.650785  94.184168   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10757    -2.137257     0.003061  0.0003  0.320548  0.019103  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10757          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time   train_time      score  \\\n",
      "10758  75313000.0  264648.420732         248.701232  262058.9986  93.999021   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10758     -2.12884     0.003454  0.0003  0.41788  0.019551  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10758          1000.0     -4.032137  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10759  75320000.0  264669.63967         248.715522  262080.203199  99.027678   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10759    -2.121231     0.002812  0.0003  0.249277  0.019056  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10759          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10760  75327000.0  264690.875616         248.729527  262101.42509  86.861782   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10760    -2.114878     0.002686  0.0003  0.31397  0.019383  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10760          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10761  75334000.0  264712.140764         248.743393  262122.676315  89.562401   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10761    -2.110106     0.003157  0.0003  0.098464  0.018665  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10761          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10762  75341000.0  264733.675025         248.759628  262144.194283  82.19327   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10762    -2.115914     0.003403  0.0003   0.5786  0.019801  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10762          1000.0     -4.032137  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10763  75348000.0  264755.344108         248.773866  262165.849053  82.283705   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10763    -2.114585     0.004006  0.0003  0.774048  0.020056  21.006897   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10763          1000.0     -4.032137  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10764  75355000.0  264793.65785         248.790795  262187.337307  91.279648   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10764    -2.120149     0.002007  0.0003  0.909243  0.017982  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10764          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10765  75362000.0  264817.761899          248.80718  262211.424883  92.145331   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10765    -2.118616     0.002673  0.0003  0.526887  0.018347  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10765          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10766  75369000.0  264842.461821         248.823254  262236.108672  82.26081   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10766    -2.092846     0.002568  0.0003  0.550515   0.01874  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10766          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10767  75376000.0  264867.205832         248.843029  262260.832849  77.241935   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10767    -2.121148      0.00244  0.0003  0.211858  0.018585  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10767          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10768  75383000.0  264890.783914         248.858719  262284.395172  88.65808   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10768     -2.13055     0.002086  0.0003 -0.742133  0.018493  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10768          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10769  75390000.0  264915.115221         248.876369  262308.708765   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10769  100.821853    -2.118626     0.003248  0.0003 -2.238574  0.021617   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10769  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10770  75397000.0  264938.305283         248.891158  262331.883983  78.254409   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10770    -2.131371     0.001994  0.0003 -3.617095  0.020333  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10770          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10771  75404000.0  264962.031297         248.905479  262355.595626  80.547921   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10771    -2.132485     0.004077  0.0003 -2.645479  0.022282  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10771          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10772  75411000.0  264985.964793         248.921058  262379.513455  99.803075   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10772    -2.137112     0.002667  0.0003 -2.215185  0.021433  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10772          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10773  75418000.0  265009.684703         248.935068  262403.21929  83.956763   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10773     -2.13992     0.003589  0.0003 -1.554911  0.021865  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10773          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10774  75425000.0  265033.274917         248.951502  262426.792928  52.188095   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10774    -2.135665     0.003165  0.0003 -1.126709  0.020567  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10774          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10775  75432000.0  265057.415785         248.966849  262450.918371  83.81895   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10775     -2.11887     0.001976  0.0003 -0.696016  0.018991  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10775          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10776  75439000.0  265080.727021         248.984826  262474.211569  62.91943   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10776    -2.118663     0.002458  0.0003 -0.769819  0.020224  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10776          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10777  75446000.0  265104.643715         248.999179  262498.113854  87.140262   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10777    -2.134575     0.002092  0.0003 -1.033572  0.019141  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10777          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10778  75453000.0  265128.097521         249.013578  262521.553206  84.605828   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10778    -2.111589      0.00291  0.0003 -0.265489  0.019464  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10778          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10779  75460000.0  265151.087783          249.03669  262544.520282  90.516232   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10779    -2.111236     0.002137  0.0003 -0.285688  0.019288  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10779          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10780  75467000.0  265175.22019         249.059155  262568.630166  87.916598   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10780    -2.127637     0.002857  0.0003 -1.053238  0.019342  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10780          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10781  75474000.0  265198.820629         249.084143  262592.205563  77.905938   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10781    -2.118981     0.002571  0.0003 -0.279069  0.019471  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10781          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10782  75481000.0  265221.786403         249.107149  262615.148267  85.897721   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10782    -2.108076     0.001752  0.0003  0.423082  0.017556  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10782          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10783  75488000.0  265243.054233          249.13112  262636.39207  85.638568   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10783    -2.122267     0.001965  0.0003  0.639216  0.018129  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10783          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10784  75495000.0  265264.251256         249.155688  262657.564468  113.68949   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10784     -2.12963     0.003259  0.0003  1.138105  0.019078  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10784          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10785  75502000.0  265285.945685         249.177502  262679.237031  97.152382   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10785    -2.119297     0.002874  0.0003  1.141517  0.018709  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10785          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10786  75509000.0  265307.11834         249.200259  262700.386871  81.621863   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10786    -2.115405     0.002481  0.0003  1.099424  0.018108  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10786          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10787  75516000.0  265328.170382         249.221584  262721.417536  88.26071   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10787    -2.121884     0.001914  0.0003  1.378075  0.017504  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10787          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10788  75523000.0  265349.749937         249.245978  262742.972648  85.578717   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10788    -2.123419     0.002403  0.0003  1.065976   0.01759  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10788          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10789  75530000.0  265371.14552         249.268633  262764.345522  73.808841   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10789     -2.11812     0.003267  0.0003   1.0921  0.019417  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10789          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10790  75537000.0  265392.772493         249.290885  262785.95019  67.629017   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10790    -2.113932     0.001336  0.0003  1.323077  0.016279  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10790          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10791  75544000.0  265414.405251         249.312689  262807.561088  80.109738   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10791    -2.101769     0.001903  0.0003  0.973852  0.018008  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10791          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10792  75551000.0  265435.801522         249.349604  262828.920378  94.91764   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10792    -2.105808     0.001892  0.0003  0.750468  0.017778  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10792          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "10793  75558000.0  265457.09996         249.382484  262850.185882  106.824644   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10793    -2.119568     0.002132  0.0003  0.689898  0.017443  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10793          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10794  75565000.0  265478.366814         249.392279  262871.442893   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10794  109.231956    -2.134431     0.002283  0.0003  0.500673  0.017807   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10794  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10795  75572000.0  265499.787222         249.401464  262892.854068   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10795  107.483119    -2.126323     0.002891  0.0003  1.449955  0.018482   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10795  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10796  75579000.0  265521.582211         249.411828  262914.638638  89.934386   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10796    -2.116542     0.001388  0.0003  1.249843  0.016738  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10796          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10797  75586000.0  265542.998109         249.421308  262936.045005   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10797  108.262233    -2.110814     0.002234  0.0003  1.235981  0.017407   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10797  21.788311          1000.0     -6.059991  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "10798  75593000.0  265564.2722         249.430843  262957.309509  98.754317   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10798    -2.129076     0.002443  0.0003  0.416072  0.018363  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10798          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10799  75600000.0  265585.805669         249.439888  262978.833882  88.322006   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10799    -2.119171     0.002134  0.0003  1.154238  0.018064  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10799          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10800  75607000.0  265609.81133         249.449095  263002.830283  72.150879   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10800    -2.123408      0.00335  0.0003  1.809168  0.018382  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10800          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10801  75614000.0  265633.902332         249.458986  263026.911304   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10801  124.270536    -2.143475     0.003686  0.0003  1.289619  0.018786   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10801  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10802  75621000.0  265656.758756         249.471054  263049.755606   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10802  107.607804    -2.133844     0.003321  0.0003  1.268168  0.018792   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10802  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10803  75628000.0  265678.147068         249.481458  263071.133432   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10803  100.902326    -2.122263     0.003954  0.0003  1.044378  0.019755   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10803  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10804  75635000.0  265699.427701         249.490987  263092.404477  94.169162   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10804    -2.143892     0.003008  0.0003  0.266845  0.019318  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10804          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10805  75642000.0  265721.086961         249.500537  263114.054087  91.724388   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10805    -2.122007     0.003238  0.0003  0.974178  0.019092  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10805          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10806  75649000.0  265742.155113         249.511615  263135.111113  98.460965   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10806    -2.147646     0.001963  0.0003  1.662734  0.017277  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10806          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10807  75656000.0  265763.494612          249.52091  263156.441264  83.32362   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10807    -2.120858     0.002726  0.0003  1.226372   0.01786  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10807          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10808  75663000.0  265784.695002         249.531754  263177.630758  92.739046   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10808    -2.131626     0.002011  0.0003  1.072618  0.018298  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10808          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10809  75670000.0  265805.885411         249.542948  263198.809902  79.356186   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10809    -2.132937     0.001986  0.0003  1.392429  0.017985  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10809          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10810  75677000.0  265827.211453         249.552315  263220.126517  87.48471   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10810    -2.138503     0.001806  0.0003  1.389393  0.017231  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10810          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10811  75684000.0  265849.096496            249.562  263242.001804   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "10811  103.217475    -2.126281     0.003657  0.0003  1.36768  0.019146   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10811  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10812  75691000.0  265870.823908         249.571919  263263.719173  86.14061   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10812    -2.144277      0.00254  0.0003  1.845063  0.017267  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10812          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10813  75698000.0  265892.963801         249.582863  263285.84807  98.050572   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10813    -2.124125     0.001759  0.0003  0.998866  0.017592  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10813          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10814  75705000.0  265915.000438         249.592144  263307.875372  65.158501   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10814    -2.127222     0.002905  0.0003  1.189676  0.018523  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10814          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10815  75712000.0  265936.687926         249.601833  263329.553104  106.45514   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10815     -2.14374     0.001674  0.0003  1.135724  0.017414  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10815          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10816  75719000.0  265959.004683         249.612949  263351.858679  79.729386   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10816    -2.145497     0.002181  0.0003  0.813506   0.01768  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10816          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10817  75726000.0  265980.80605         249.622096  263373.650839  82.084656   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10817    -2.131424     0.002741  0.0003  1.437271  0.018183  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10817          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10818  75733000.0  266003.178933         249.631351  263396.014417   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10818  118.878938    -2.145965     0.001734  0.0003  1.328027    0.0174   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10818  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10819  75740000.0  266025.059751         249.641488  263417.885043  99.308573   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10819    -2.141541      0.00219  0.0003  0.978842  0.018022  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10819          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10820  75747000.0  266046.782309         249.652193  263439.596844  85.653357   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10820    -2.142006     0.002816  0.0003  0.946199  0.018291  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10820          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10821  75754000.0  266068.885565         249.661646  263461.690593  64.083621   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10821    -2.156367     0.002871  0.0003  0.855522  0.018024  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10821          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10822  75761000.0  266090.815704         249.671372  263483.610956  78.966394   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10822    -2.151721     0.003205  0.0003  1.334268  0.018331  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10822          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10823  75768000.0  266113.33193         249.682642  263506.115863  99.535279   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10823    -2.154771     0.003284  0.0003  1.012867  0.018533  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10823          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10824  75775000.0  266134.933674         249.693336  263527.706822  87.077826   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10824    -2.155474     0.003514  0.0003  0.777519  0.019812  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10824          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10825  75782000.0  266156.553638         249.704474  263549.315561  96.201427   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10825    -2.152396     0.002333  0.0003  0.931362  0.018587  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10825          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10826  75789000.0  266178.106421          249.71611  263570.856656  89.692563   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10826    -2.140884     0.003517  0.0003  0.482848  0.019175  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10826          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10827  75796000.0  266200.002512         249.725288  263592.74352  100.955233   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10827    -2.146719      0.00301  0.0003  0.370975  0.019814  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10827          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10828  75803000.0  266221.963022         249.734711  263614.694553  109.94456   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10828    -2.160896     0.003543  0.0003  0.078682  0.019542  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10828          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10829  75810000.0  266243.544638         249.743803  263636.267011  79.245046   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10829    -2.150591     0.002175  0.0003  0.982959  0.017253  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10829          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10830  75817000.0  266264.655135          249.75417  263657.367075  83.501212   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10830    -2.146634     0.002975  0.0003  0.959004  0.018181  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10830          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10831  75824000.0  266286.027486         249.764979  263678.728565   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10831  101.231948     -2.16206     0.002286  0.0003  1.286662  0.017561   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10831  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10832  75831000.0  266307.384371         249.774347  263700.076034  95.875025   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10832    -2.146616     0.002687  0.0003  0.535099  0.018521  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10832          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10833  75838000.0  266328.780451          249.78416  263721.462252  70.711091   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10833    -2.164412     0.002824  0.0003  0.821149  0.017727  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10833          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10834  75845000.0  266350.739818         249.793748  263743.411983  98.155044   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10834    -2.170833     0.002223  0.0003  0.563926  0.017555  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10834          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10835  75852000.0  266372.359857         249.802672  263765.023042  93.880467   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10835    -2.161764     0.002765  0.0003  0.780233  0.017791  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10835          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10836  75859000.0  266394.260887         249.812511  263786.914178  78.893408   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10836    -2.165611     0.002155  0.0003  0.615318  0.017717  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10836          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10837  75866000.0  266415.64271         249.822476  263808.285985  78.528345   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10837    -2.154834     0.002834  0.0003  0.480817   0.01855  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10837          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10838  75873000.0  266437.842295         249.832555  263830.475434  72.057217   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10838    -2.148456     0.002541  0.0003  0.676128  0.018367  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10838          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10839  75880000.0  266460.187959         249.842868  263852.810735  70.651054   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10839     -2.17624     0.002397  0.0003  1.226958  0.017869  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10839          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10840  75887000.0  266482.036788         249.852447  263874.649939  91.318026   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10840    -2.159114     0.002669  0.0003  0.834567  0.018549  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10840          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10841  75894000.0  266504.023893         249.862626  263896.626817  86.87637   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10841    -2.162049     0.002569  0.0003  0.326849  0.018902  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10841          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "10842  75901000.0  266526.18181         249.872626  263918.774682  75.97445   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10842    -2.144811     0.002343  0.0003  1.255926  0.017295  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10842          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10843  75908000.0  266548.448543           249.8824  263941.031588  87.816261   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10843    -2.132534     0.002319  0.0003  0.909401   0.01791  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10843          1000.0     -6.059991  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10844  75915000.0  266570.64151         249.892872  263963.214025  90.157486   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10844    -2.146163     0.002443  0.0003  1.467669  0.018335  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10844          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10845  75922000.0  266592.565787         249.903316  263985.127801  91.246486   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10845    -2.151196     0.002189  0.0003  0.616467  0.018217  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10845          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10846  75929000.0  266615.170657         249.913073  264007.722858  78.300692   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10846    -2.169512     0.003048  0.0003  0.712725  0.018609  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10846          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10847  75936000.0  266639.874231         249.923836  264032.415514  77.333391   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10847    -2.170228     0.002458  0.0003  0.667408  0.018198  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10847          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10848  75943000.0  266663.727386         249.934972  264056.257464   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10848  111.156404    -2.157195     0.003393  0.0003  0.942048  0.018574   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10848  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10849  75950000.0  266687.511684         249.945102  264080.031573   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10849  112.316914    -2.140469     0.003194  0.0003  0.804652  0.019255   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10849  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10850  75957000.0  266711.495906         249.955603  264104.005238  91.636792   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10850    -2.148543       0.0041  0.0003  1.48752  0.019968  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10850          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10851  75964000.0  266734.638434         249.965287  264127.137985  71.621306   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10851    -2.165109     0.002404  0.0003 -1.388958  0.018689  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10851          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10852  75971000.0  266758.894435          249.97651  264151.382704  74.344166   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10852    -2.151211     0.002344  0.0003  1.729133  0.017949  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10852          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10853  75978000.0  266782.965198         249.987056  264175.442863  75.943045   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10853    -2.156179     0.002397  0.0003  1.663343  0.018373  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10853          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10854  75985000.0  266806.599477         249.996831  264199.067283  98.197833   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10854    -2.162779     0.002473  0.0003  1.051178  0.017809  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10854          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10855  75992000.0  266831.079252         250.008499  264223.535333  84.748253   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10855    -2.153138     0.001776  0.0003  1.064084  0.017881  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10855          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10856  75999000.0  266855.217655         250.017889  264247.664298  94.461203   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10856    -2.155883     0.003718  0.0003  0.667034   0.01945  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10856          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10857  76006000.0  266879.799224         250.027516  264272.235911  81.255719   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10857    -2.143002     0.002103  0.0003  1.318994   0.01771  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10857          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10858  76013000.0  266904.490416         250.039092  264296.915452  88.821255   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10858     -2.15056      0.00278  0.0003  1.015141  0.017894  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10858          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10859  76020000.0  266928.519115         250.049503  264320.933671  77.195048   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10859     -2.16117     0.002569  0.0003  0.76006  0.018065  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10859          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10860  76027000.0  266953.013492         250.059968  264345.41752  109.783486   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10860    -2.170569     0.003272  0.0003  0.801815  0.019712  21.788311   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10860          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10861  76034000.0  266977.332955         250.069914  264369.726984   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10861  104.115136    -2.184333     0.002164  0.0003  1.379666  0.018549   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10861  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10862  76041000.0  267001.409787         250.079503  264393.794173   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10862  101.010166    -2.176413     0.004595  0.0003  1.650117  0.020045   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10862  21.788311          1000.0     -6.059991  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10863  76048000.0  267024.965152         250.092396  264417.336592   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10863  109.807797    -2.153986     0.002615  0.0003  1.381202  0.019004   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10863  21.788311          1000.0     -6.059991  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10864  76055000.0  267064.336575         250.101864  264439.843718  89.961119   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10864    -2.160132     0.002671  0.0003  1.408613  0.018346  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10864          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10865  76062000.0  267088.672255         250.112555  264464.168648  96.936494   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10865    -2.169782     0.002703  0.0003  1.478777  0.019269  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10865          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10866  76069000.0  267112.546241         250.122616  264488.032514  65.221027   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10866     -2.16483     0.004323  0.0003  1.408298  0.019209  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10866          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10867  76076000.0  267134.233716         250.131852  264509.710704  87.000631   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10867    -2.153786     0.002633  0.0003  1.566167  0.018266  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10867          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10868  76083000.0  267156.310673         250.140951  264531.778473   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10868  104.250282    -2.151409     0.003079  0.0003  1.627463  0.018559   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10868  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10869  76090000.0  267178.035012         250.151928  264553.491775   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10869  100.211523    -2.160471     0.002265  0.0003  1.317917  0.017622   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10869  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10870  76097000.0  267200.168378         250.161533  264575.615488   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10870  101.634011    -2.168819     0.003018  0.0003  1.789029  0.018982   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10870  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10871  76104000.0  267221.843799         250.170728  264597.281664  98.727137   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10871    -2.150614     0.003066  0.0003  1.077646   0.01895  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10871          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10872  76111000.0  267243.562578         250.181485  264618.989623   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "10872  105.199717     -2.17583      0.00419  0.0003  0.90525  0.019671   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10872  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10873  76118000.0  267265.369774         250.191514  264640.786741  86.005263   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10873    -2.160406     0.002211  0.0003  1.180129  0.017496  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10873          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10874  76125000.0  267287.523587         250.200741  264662.931274   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10874  109.811291    -2.170429     0.002685  0.0003  0.945165  0.018557   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10874  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10875  76132000.0  267308.840961         250.211033  264684.238304   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10875  100.778356     -2.17652      0.00264  0.0003  0.844768  0.018875   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10875  20.429419          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10876  76139000.0  267330.49144         250.221235  264705.878522  87.057486   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10876    -2.161947     0.002588  0.0003  0.485953   0.01819  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10876          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10877  76146000.0  267352.025936         250.232159  264727.402018  93.330674   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10877    -2.181172     0.002981  0.0003  0.921064  0.018685  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10877          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10878  76153000.0  267374.539371         250.242717  264749.904839  87.081507   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10878     -2.16659     0.002953  0.0003  0.964287  0.018176  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10878          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10879  76160000.0  267396.111677          250.25459  264771.465222  93.789056   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10879     -2.16156     0.003137  0.0003  0.78457  0.019418  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10879          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10880  76167000.0  267418.016823         250.263772  264793.361134  83.973296   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10880    -2.172003     0.002857  0.0003  0.970163  0.019253  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10880          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10881  76174000.0  267439.820746         250.272801  264815.155974  93.245395   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10881    -2.178937     0.002443  0.0003  0.850317   0.01819  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10881          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10882  76181000.0  267464.092295         250.288785  264839.411468  87.621965   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10882    -2.159641     0.002971  0.0003  0.745693  0.019367  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10882          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10883  76188000.0  267487.183475         250.301184  264862.490185  70.123605   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10883    -2.166426      0.00189  0.0003  0.940481  0.017641  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10883          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10884  76195000.0  267509.203027         250.309868  264884.501006   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10884  117.562614    -2.177284     0.003199  0.0003  0.875434  0.018899   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10884  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10885  76202000.0  267530.963198         250.319593  264906.251381  88.086285   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10885     -2.18852     0.003762  0.0003  0.530285  0.020164  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10885          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10886  76209000.0  267552.821368         250.328133  264928.100952   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10886  109.688878    -2.167332     0.003491  0.0003  0.172057  0.019603   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10886  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10887  76216000.0  267575.232002         250.338472  264950.501199  71.561478   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10887    -2.161758     0.003567  0.0003  0.877731  0.019625  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10887          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10888  76223000.0  267596.923228          250.34798  264972.182854   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10888  115.796037    -2.173893      0.00203  0.0003  1.160745  0.017483   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10888  20.429419          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10889  76230000.0  267618.47818         250.360406  264993.725323  92.475519   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10889    -2.173125     0.003372  0.0003  0.839735   0.01882  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10889          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10890  76237000.0  267640.010441         250.370847  265015.24707  81.608683   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10890    -2.165299     0.003114  0.0003  0.783568  0.018371  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10890          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10891  76244000.0  267661.284479          250.37989  265036.511995  90.631522   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10891    -2.178159     0.001697  0.0003  1.076634  0.017983  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10891          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10892  76251000.0  267683.047543         250.389393  265058.265504  95.968314   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10892    -2.171614     0.002784  0.0003  1.372425  0.018608  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10892          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10893  76258000.0  267704.611928         250.398876  265079.820344  93.15028   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10893     -2.18651     0.002993  0.0003  1.40193  0.019203  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10893          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10894  76265000.0  267726.032147         250.409585  265101.229801  69.090806   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10894    -2.176046     0.002598  0.0003  0.829579  0.018444  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10894          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time   train_time      score  \\\n",
      "10895  76272000.0  267747.728878         250.419964  265122.9161  84.054187   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10895    -2.167263     0.002756  0.0003  1.638705  0.018896  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10895          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10896  76279000.0  267769.064872         250.429127  265144.242833  80.821335   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10896      -2.1661      0.00247  0.0003  1.055347  0.018481  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10896          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10897  76286000.0  267790.601235         250.439131  265165.769144  75.78972   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10897    -2.175014     0.002318  0.0003  1.415886  0.018235  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10897          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10898  76293000.0  267811.765345         250.448937  265186.923393  97.783081   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10898    -2.162943     0.002482  0.0003  0.831407  0.018677  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10898          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10899  76300000.0  267834.088923         250.458931  265209.236923  62.207904   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10899    -2.162785     0.001999  0.0003   0.7417  0.018211  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10899          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10900  76307000.0  267857.663106         250.469084  265232.80087  83.911493   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10900    -2.172396     0.002533  0.0003  1.303726  0.017605  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10900          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10901  76314000.0  267880.161133         250.481365  265255.286561  98.996878   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10901    -2.177165     0.003097  0.0003  0.903034  0.019251  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10901          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10902  76321000.0  267901.225461         250.490763  265276.341432   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "10902  102.221154    -2.166055      0.00236  0.0003  0.88804  0.018531   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10902  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10903  76328000.0  267922.497174         250.500119  265297.603725  93.489751   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10903    -2.172495     0.002396  0.0003  1.07041  0.017768  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10903          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10904  76335000.0  267943.667176         250.509961  265318.763829  73.237427   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10904    -2.173374      0.00223  0.0003  1.130202  0.018319  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10904          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10905  76342000.0  267965.29655         250.519767  265340.383347  96.864939   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10905    -2.175706     0.002272  0.0003  0.755385   0.01825  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10905          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10906  76349000.0  267986.568962         250.529027  265361.646449  78.195117   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10906     -2.15533     0.001858  0.0003  1.317677  0.017192  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10906          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10907  76356000.0  268008.358785         250.538239  265383.427009  75.301006   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10907     -2.15756     0.001765  0.0003  1.004593  0.016772  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10907          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10908  76363000.0  268030.089775         250.548611  265405.147561  79.519689   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10908     -2.16278     0.001989  0.0003  1.438612  0.017441  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10908          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10909  76370000.0  268051.176181         250.558079  265426.224448  91.975689   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10909     -2.17851     0.002683  0.0003  0.710766  0.018507  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10909          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10910  76377000.0  268073.016951         250.568019  265448.055225  93.752146   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10910    -2.172677     0.002633  0.0003  0.910176  0.018667  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10910          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10911  76384000.0  268094.542588         250.578336  265469.570491   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10911  110.236749    -2.181586     0.003526  0.0003  0.412559  0.019539   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10911  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10912  76391000.0  268115.842689         250.587899  265490.860979  87.957203   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10912    -2.181935     0.001978  0.0003  1.316195  0.017717  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10912          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10913  76398000.0  268137.452678         250.596875  265512.461938   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10913  110.687584    -2.170313     0.003271  0.0003  1.072186  0.018951   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10913  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10914  76405000.0  268158.932339         250.606478  265533.931942  93.578152   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10914    -2.161713     0.002319  0.0003  0.715803  0.018317  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10914          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10915  76412000.0  268180.560968         250.615716  265555.551282  93.634196   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10915    -2.184034     0.002929  0.0003  0.901727  0.018794  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10915          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10916  76419000.0  268202.260382         250.625583  265577.240777  95.065467   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10916    -2.185102     0.002909  0.0003  1.194894   0.01903  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10916          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10917  76426000.0  268224.904345         250.635421  265599.874839   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10917  115.934335    -2.167071     0.002624  0.0003  0.920888  0.018873   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10917  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10918  76433000.0  268249.046553         250.645346  265624.007069  77.350177   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10918    -2.160957     0.003274  0.0003  1.551771  0.019357  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10918          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10919  76440000.0  268273.10202         250.659022  265648.048808  89.488566   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10919    -2.181955     0.002645  0.0003  1.576382  0.017924  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10919          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10920  76447000.0  268297.223959         250.671748  265672.157964  80.589339   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10920    -2.171574     0.002073  0.0003  1.312562  0.018682  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10920          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "10921  76454000.0  268319.42808         250.682458  265694.351327  107.833758   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10921    -2.191047     0.002616  0.0003  1.503337  0.018288  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10921          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10922  76461000.0  268340.687703         250.692321  265715.600994  101.42044   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10922    -2.188094     0.003386  0.0003  1.459979  0.019494  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10922          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10923  76468000.0  268362.464209          250.70299  265737.366778  75.44364   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10923    -2.179649     0.002255  0.0003  1.083964  0.018372  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10923          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time    train_time      score  \\\n",
      "10924  76475000.0  268383.74093         250.712259  265758.63417  83.289318   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10924    -2.168707     0.003815  0.0003  1.383237  0.019221  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10924          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10925  76482000.0  268405.264462         250.721634  265780.14826  82.143523   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10925    -2.167853     0.002771  0.0003  1.150463  0.018873  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10925          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10926  76489000.0  268426.447537         250.732148  265801.32077  80.736548   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10926    -2.174127     0.002172  0.0003  1.354391   0.01816  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10926          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10927  76496000.0  268447.899765         250.742089  265822.763008   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10927  101.667181    -2.194106     0.002924  0.0003  1.484954  0.018732   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10927  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10928  76503000.0  268469.554681         250.751764  265844.408196  93.914203   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10928     -2.17284     0.002365  0.0003  1.296252  0.018462  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10928          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10929  76510000.0  268491.178373         250.763124  265866.020469  84.736727   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10929    -2.165987     0.002428  0.0003  1.400318  0.017565  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10929          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10930  76517000.0  268512.885573         250.773611  265887.717132  77.711649   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10930      -2.1803     0.002704  0.0003  1.468953   0.01813  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10930          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10931  76524000.0  268534.536255         250.782939  265909.358419  105.63051   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10931      -2.1687     0.003266  0.0003  1.524045  0.019516  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10931          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10932  76531000.0  268556.280619         250.792857  265931.092818  92.459351   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10932    -2.190209     0.003951  0.0003  1.084088  0.019464  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10932          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10933  76538000.0  268577.783361         250.802091  265952.586267  94.892226   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10933    -2.173852     0.002888  0.0003  1.102693  0.018605  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10933          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10934  76545000.0  268599.176428          250.81231  265973.969059  78.768022   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10934    -2.182825     0.003217  0.0003  1.338695  0.018974  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10934          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10935  76552000.0  268620.500583         250.823156  265995.282317  87.436914   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10935    -2.174101     0.002672  0.0003  1.379212   0.01847  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10935          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10936  76559000.0  268643.995369         250.832781  266018.767404  95.607199   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10936    -2.183957      0.00218  0.0003  0.675315  0.018497  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10936          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10937  76566000.0  268665.052035         250.843239  266039.813547   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10937  115.976989    -2.182035     0.003457  0.0003  0.859746  0.019576   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10937  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10938  76573000.0  268686.984427          250.85388  266061.735236   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10938  113.490231    -2.187918     0.003051  0.0003  1.390707  0.018516   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10938  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10939  76580000.0  268708.559916          250.86427  266083.300274  76.935984   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10939    -2.186212     0.004143  0.0003  1.023707  0.019822  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10939          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10940  76587000.0  268729.93395          250.87371  266104.664802  82.093261   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10940    -2.189447     0.003105  0.0003  1.493658  0.018652  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10940          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10941  76594000.0  268751.811256         250.883442  266126.532319  81.393369   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10941    -2.180153      0.00192  0.0003  0.674351  0.017969  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10941          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10942  76601000.0  268773.631037         250.894064  266148.341417  84.910333   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10942     -2.17513     0.002437  0.0003  1.454279  0.018588  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10942          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10943  76608000.0  268795.509786         250.903816  266170.210359  99.305138   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10943    -2.179337      0.00267  0.0003  1.800518  0.018591  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10943          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10944  76615000.0  268817.169774         250.913542  266191.860566  74.569494   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10944    -2.165622     0.002808  0.0003  1.648507  0.019459  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10944          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10945  76622000.0  268838.56506          250.92409  266213.245255  83.909608   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10945    -2.191201     0.002382  0.0003  1.604223  0.017972  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10945          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10946  76629000.0  268860.346186         250.933641  266235.016778  97.100854   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10946    -2.172141     0.002475  0.0003  1.138179  0.018686  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10946          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10947  76636000.0  268881.963338         250.942802  266256.624721  76.771058   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10947    -2.177984     0.002857  0.0003  1.868033  0.017766  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10947          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10948  76643000.0  268903.594331         250.951874  266278.246592  70.314452   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10948    -2.184817     0.003521  0.0003  1.209131  0.019645  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10948          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10949  76650000.0  268925.201919          250.96217  266299.843836  95.867999   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10949    -2.187309     0.002745  0.0003  0.993058  0.018874  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10949          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10950  76657000.0  268946.661237         250.972298  266321.29297  80.091044   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10950    -2.190247     0.003088  0.0003  1.364952  0.018828  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10950          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10951  76664000.0  268968.375934         250.982072  266342.997832  74.140929   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10951    -2.185678     0.002536  0.0003  0.902166  0.018927  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10951          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10952  76671000.0  268990.212968         250.991154  266364.825737  91.142982   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10952     -2.17774     0.002732  0.0003   1.2589  0.017869  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10952          1000.0      -8.31724  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "10953  76678000.0  269011.8163         251.000785  266386.419378  108.69614   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10953     -2.17138     0.002348  0.0003  1.377637  0.018129  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10953          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10954  76685000.0  269035.190191         251.011183  266409.782805  67.065693   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10954    -2.188589      0.00217  0.0003  0.426963  0.017371  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10954          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10955  76692000.0  269058.927142         251.021538  266433.509343  76.805048   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10955    -2.179603     0.003372  0.0003  0.806789  0.019513  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10955          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "10956  76699000.0  269083.31816         251.031537  266457.890308  88.20153   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10956    -2.178211     0.002899  0.0003  0.830907  0.018912  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10956          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "10957  76706000.0  269107.43968         251.041917  266482.001387  107.163794   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10957    -2.172468     0.003335  0.0003  1.07609  0.019553  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10957          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10958  76713000.0  269131.790988         251.052276  266506.342284  94.096333   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10958     -2.17181       0.0035  0.0003  1.406618  0.019342  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10958          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10959  76720000.0  269156.539778         251.065091  266531.078177  82.805559   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10959    -2.168974     0.002904  0.0003  1.715118  0.018399  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10959          1000.0      -8.31724  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "10960  76727000.0  269181.26565          251.07494  266555.794147  85.605346   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "10960    -2.194919     0.002854  0.0003  1.29414  0.019392  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10960          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10961  76734000.0  269205.334947         251.084757  266579.853574   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10961  109.455943    -2.175652     0.003277  0.0003  1.209015  0.019336   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "10961  20.429419          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10962  76741000.0  269229.642667          251.09449  266604.151505  100.11854   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10962    -2.167718     0.002815  0.0003  1.235023  0.019002  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10962          1000.0      -8.31724  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10963  76748000.0  269253.745066         251.104612  266628.24372  67.405254   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "10963    -2.181179     0.001925  0.0003  1.674332  0.017814  20.429419   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10963          1000.0      -8.31724  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10964  76755000.0  269293.655506         251.121859  266651.893766  94.020404   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10964    -2.187637     0.002888  0.0003  2.17496  0.018902  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10964          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10965  76762000.0  269318.323634         251.131862  266676.551831  87.689482   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10965    -2.179703     0.002712  0.0003  2.483315  0.018502  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10965          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10966  76769000.0  269341.909246         251.149847  266700.119402  85.325472   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10966    -2.166941     0.002616  0.0003  2.131793  0.018854  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10966          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10967  76776000.0  269366.144491          251.16064  266724.343755   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10967  107.523516    -2.186787     0.003915  0.0003  1.120512  0.019717   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10967  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10968  76783000.0  269389.365587         251.173254  266747.552134   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10968  100.096673    -2.170156      0.00343  0.0003  1.691022  0.019465   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10968  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10969  76790000.0  269413.517946          251.18503  266771.692665  95.377058   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10969    -2.179597     0.001766  0.0003  1.752562  0.017377  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10969          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10970  76797000.0  269436.902121         251.196966  266795.064787  79.822719   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10970    -2.178711     0.003658  0.0003  1.652059  0.019804  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10970          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10971  76804000.0  269460.064348         251.210757  266818.213167  91.682039   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10971    -2.193304     0.002481  0.0003  2.262246  0.018266  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10971          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10972  76811000.0  269481.823054          251.22385  266839.958717   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "10972  101.300918    -2.175333     0.002386  0.0003  1.83007   0.01855   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10972  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10973  76818000.0  269503.534314         251.234155  266861.659614  95.453948   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10973    -2.176186     0.002482  0.0003  1.659501  0.018763  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10973          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10974  76825000.0  269525.594456         251.243823  266883.710044  78.705743   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10974     -2.19369      0.00413  0.0003  1.785735  0.019749  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10974          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10975  76832000.0  269547.634082         251.253339  266905.740107   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10975  102.003857    -2.175894     0.002555  0.0003  1.504033  0.018813   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10975  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10976  76839000.0  269569.923558         251.263399  266928.01944  109.601147   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10976    -2.167047     0.002809  0.0003  1.32418   0.02012  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10976          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "10977  76846000.0  269592.147555         251.274206  266950.232581  82.02602   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10977    -2.172143     0.004038  0.0003  1.824731  0.019755  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10977          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10978  76853000.0  269614.366219         251.284172  266972.441226  73.086388   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10978    -2.185574     0.002134  0.0003  1.71325  0.018092  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10978          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10979  76860000.0  269636.324109          251.29563  266994.387608  102.55684   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10979    -2.192095      0.00328  0.0003  2.267448  0.019011  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10979          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10980  76867000.0  269658.601624         251.305562  267016.655139  83.417408   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10980    -2.185842       0.0027  0.0003  2.143446  0.018646  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10980          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10981  76874000.0  269680.714153         251.315877  267038.757305  87.197311   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10981    -2.163379     0.003708  0.0003  2.08492   0.01831  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10981          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10982  76881000.0  269705.219078         251.327105  267063.250921   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10982  107.041568    -2.166126     0.002755  0.0003  1.951766   0.01859   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10982  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10983  76888000.0  269726.541835          251.33733  267084.563405  80.997191   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10983    -2.174947     0.002872  0.0003  1.877791   0.01872  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10983          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10984  76895000.0  269747.788461          251.34712  267105.800189  93.438111   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10984    -2.197041     0.002437  0.0003  1.848037  0.018728  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10984          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10985  76902000.0  269769.355581          251.35756  267127.356818  93.269922   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10985    -2.177204     0.001703  0.0003  1.776357  0.017937  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10985          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10986  76909000.0  269791.329678         251.366966  267149.321449  93.171991   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10986    -2.185482     0.003168  0.0003  1.955849  0.019321  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10986          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "10987  76916000.0  269813.042176         251.376598  267171.02426  111.315742   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10987    -2.201103     0.001882  0.0003  1.833541  0.018015  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10987          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10988  76923000.0  269835.764254         251.385536  267193.737347  88.539243   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10988    -2.192145     0.003166  0.0003  2.098134  0.019284  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10988          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "10989  76930000.0  269857.377587         251.394886  267215.34127  93.912303   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10989    -2.179598     0.003032  0.0003  1.766068  0.019681  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10989          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10990  76937000.0  269879.285425         251.407027  267237.236914  92.451937   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10990    -2.191159     0.002495  0.0003  1.416788  0.019209  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10990          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10991  76944000.0  269901.271947         251.416406  267259.214004  75.434171   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "10991    -2.169937     0.004029  0.0003  1.33422  0.019979  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10991          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10992  76951000.0  269923.606926         251.425713  267281.539625  67.940898   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10992    -2.177582     0.002332  0.0003  1.539755  0.017798  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10992          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "10993  76958000.0  269945.651458          251.43722  267303.572597   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "10993  103.632204    -2.198832     0.002448  0.0003  2.078039  0.018567   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "10993  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10994  76965000.0  269967.767618         251.446956  267325.678971  91.914847   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10994    -2.191627     0.002453  0.0003  2.436707  0.017745  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10994          1000.0     -5.007243  \n",
      "         timestep         time  collect_wait_time     train_time       score  \\\n",
      "10995  76972000.0  269989.8329         251.456409  267347.734396  109.543659   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10995    -2.177929     0.002972  0.0003  1.707923  0.019387  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10995          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10996  76979000.0  270011.975904         251.467745  267369.866013  95.495626   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10996     -2.18334     0.003276  0.0003  2.026844  0.018713  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10996          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10997  76986000.0  270034.057233         251.476874  267391.938161  84.978461   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10997    -2.176556     0.003254  0.0003  1.505341  0.018976  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10997          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10998  76993000.0  270055.979894         251.488114  267413.849532  80.658272   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10998    -2.181883     0.002887  0.0003  1.869435  0.018843  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10998          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "10999  77000000.0  270078.453278         251.498857  267436.312117  68.745971   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "10999    -2.187401      0.00309  0.0003  1.569237     0.019  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "10999          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11000  77007000.0  270103.169443         251.508009  267461.019087  68.755753   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11000    -2.189778     0.002783  0.0003  1.616419  0.018666  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11000          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11001  77014000.0  270125.559787         251.518387  267483.398984  95.341973   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11001    -2.179174     0.001975  0.0003  1.65476  0.017625  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11001          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11002  77021000.0  270147.829789         251.528581  267505.658744  91.807358   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11002    -2.191678     0.002543  0.0003  1.574925  0.018032  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11002          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11003  77028000.0  270169.779779         251.537509  267527.599748  81.580639   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11003     -2.17702     0.003927  0.0003  1.739988  0.019577  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11003          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "11004  77035000.0  270191.537055         251.547346  267549.34713  96.53949   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11004    -2.177593     0.002063  0.0003  2.01566  0.017819  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11004          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11005  77042000.0  270213.256298         251.557865  267571.055793  81.317246   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11005    -2.196031     0.003441  0.0003  2.075289  0.019596  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11005          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11006  77049000.0  270235.246281         251.568736  267593.034855   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11006  108.088769    -2.190299     0.002544  0.0003  1.812506  0.018581   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11006  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11007  77056000.0  270257.223634         251.578235  267615.002661  78.599319   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11007    -2.192037     0.002272  0.0003  1.42475  0.018444  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11007          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11008  77063000.0  270279.222401         251.588211  267636.991395  80.844633   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11008    -2.192592     0.002583  0.0003  1.82224  0.018742  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11008          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11009  77070000.0  270300.862734         251.597639  267658.622241  97.056501   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11009    -2.177946     0.002517  0.0003  1.69411  0.018576  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11009          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11010  77077000.0  270321.984956         251.607354  267679.734699  65.67768   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11010    -2.182803     0.002038  0.0003  1.50179  0.018926  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11010          1000.0     -5.007243  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11011  77084000.0  270342.95681         251.616586  267700.697268  72.777621   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11011    -2.184442     0.002436  0.0003  1.800775   0.01815  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11011          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11012  77091000.0  270364.255314         251.627071  267721.985237  67.575626   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11012    -2.191458     0.002727  0.0003  1.681746   0.01926  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11012          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11013  77098000.0  270385.959454         251.636723  267743.679659   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11013  102.393605    -2.184379     0.003653  0.0003  1.361201  0.019522   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11013  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11014  77105000.0  270407.859732         251.646476  267765.570132  92.109025   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11014    -2.177517     0.002914  0.0003  1.445387  0.019581  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11014          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11015  77112000.0  270429.899236         251.655822  267787.600239  93.139842   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11015    -2.179752     0.001805  0.0003  1.823845  0.017873  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11015          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11016  77119000.0  270452.034056         251.665565  267809.725267  91.023646   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11016    -2.196182      0.00308  0.0003  1.660732  0.019184  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11016          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "11017  77126000.0  270473.891086         251.674727  267831.57308  77.77898   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11017    -2.179097     0.003432  0.0003  1.726129  0.019743  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11017          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11018  77133000.0  270496.756248         251.684212  267854.428701   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11018  106.254481    -2.183755     0.002419  0.0003  1.628081  0.018167   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11018  26.01626          1000.0     -5.007243  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11019  77140000.0  270518.70299         251.693588  267876.366014  78.413554   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11019    -2.187698     0.003117  0.0003  1.70166  0.019846  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11019          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11020  77147000.0  270540.477576         251.702966  267898.131164  85.410334   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11020    -2.201879     0.003424  0.0003  1.130587  0.019337  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11020          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11021  77154000.0  270562.688318         251.713567  267920.331254  83.403051   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11021    -2.210628     0.002716  0.0003  1.869851  0.018709  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11021          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11022  77161000.0  270584.947157         251.723614  267942.579991   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11022  107.564621    -2.185464     0.004331  0.0003  2.017812  0.020334   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11022  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11023  77168000.0  270606.614599         251.734952  267964.236027   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11023  103.401914    -2.195838     0.002812  0.0003  2.099613  0.018938   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11023  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11024  77175000.0  270628.567236          251.74409  267986.179431  94.328977   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11024    -2.198872     0.003633  0.0003  1.615422  0.019415  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11024          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11025  77182000.0  270650.244158         251.754788  268007.845565  85.379884   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11025    -2.180493     0.003029  0.0003  2.024133  0.018952  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11025          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11026  77189000.0  270671.835753          251.76586  268029.426035   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11026  112.755343     -2.17865     0.002096  0.0003  1.976724  0.018649   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11026  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11027  77196000.0  270693.628984         251.775962  268051.209104  80.559939   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11027    -2.196557       0.0033  0.0003  2.144864  0.019596  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11027          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11028  77203000.0  270715.347064          251.78605  268072.916986  92.448711   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11028    -2.195008     0.002603  0.0003  2.25258  0.019547  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11028          1000.0     -5.007243  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11029  77210000.0  270736.96768         251.798146  268094.525451  85.445873   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11029    -2.181387     0.002389  0.0003   1.6303  0.019081  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11029          1000.0     -5.007243  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11030  77217000.0  270758.63236         251.808158  268116.180068  76.643974   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11030    -2.183042     0.002352  0.0003  1.275284  0.019867  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11030          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11031  77224000.0  270780.492316         251.818391  268138.02974  92.094988   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11031     -2.19829     0.002283  0.0003  1.565366  0.018896  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11031          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11032  77231000.0  270802.754567         251.829022  268160.281284  98.008156   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11032    -2.187914      0.00265  0.0003  1.825149  0.017899  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11032          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11033  77238000.0  270824.616904         251.840558  268182.131998  89.381672   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11033    -2.202919     0.003087  0.0003  2.332497  0.018966  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11033          1000.0     -5.007243  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11034  77245000.0  270846.51628         251.852031  268204.019851  84.414504   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11034    -2.182875     0.002637  0.0003  2.327453  0.018758  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11034          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11035  77252000.0  270868.157131         251.862123  268225.650558  94.462663   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11035    -2.195307     0.002279  0.0003  1.66616  0.019029  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11035          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11036  77259000.0  270890.933356         251.871813  268248.417043  90.660408   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11036    -2.193124     0.003529  0.0003  1.725809  0.019828  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11036          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11037  77266000.0  270915.407552         251.881772  268272.881218  92.169262   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11037     -2.19054     0.002476  0.0003  1.847929  0.018764  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11037          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11038  77273000.0  270939.533612         251.891669  268296.997325  74.286707   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11038    -2.188322     0.002488  0.0003  1.889732  0.018761  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11038          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11039  77280000.0  270963.079871         251.902841  268320.532357  80.559105   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11039    -2.188084     0.003316  0.0003  1.874143  0.019366  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11039          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11040  77287000.0  270987.094869         251.915335  268344.534791   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11040  104.772203    -2.180309     0.003691  0.0003  2.672568  0.019616   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11040  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11041  77294000.0  271011.298043           251.9275  268368.725719   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11041  114.441268    -2.179578     0.002244  0.0003  1.962222  0.017885   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11041  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11042  77301000.0  271035.201776         251.938633  268392.618228  73.148519   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11042    -2.197931     0.002401  0.0003  1.695342  0.018515  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11042          1000.0     -5.007243  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11043  77308000.0  271059.84927         251.949044  268417.255259  77.774622   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11043    -2.186699     0.003127  0.0003  1.779334  0.019016  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11043          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11044  77315000.0  271084.595629         251.960994  268441.989615  89.337073   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11044    -2.181819     0.002985  0.0003  2.144515  0.018639  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11044          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "11045  77322000.0  271109.477854         251.970289  268466.862472  82.8982   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11045    -2.188904     0.001969  0.0003  1.760454  0.017636  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11045          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11046  77329000.0  271133.625908         251.980218  268491.000547  88.265754   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11046    -2.190447      0.00245  0.0003  1.469206  0.018731  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11046          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11047  77336000.0  271156.362758         251.990132  268513.727429  75.92465   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11047    -2.166943     0.002558  0.0003  2.090767  0.018765  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11047          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11048  77343000.0  271177.864279         252.001884  268535.217136  96.172214   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11048    -2.183012     0.003345  0.0003  1.791802  0.019382  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11048          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11049  77350000.0  271199.479093         252.012247  268556.821495  100.43741   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11049     -2.19886     0.002843  0.0003  1.638023  0.018793  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11049          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11050  77357000.0  271220.828834         252.025613  268578.157803  74.552581   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11050    -2.186922     0.002434  0.0003  1.539275  0.018912  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11050          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11051  77364000.0  271242.692232         252.036579  268600.010172  85.336566   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11051    -2.184222     0.002678  0.0003  1.061426  0.018999  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11051          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11052  77371000.0  271264.515405         252.047729  268621.822141  82.370058   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11052    -2.187786     0.002553  0.0003  2.097583  0.018778  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11052          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11053  77378000.0  271286.335132         252.058367  268643.631167  76.342851   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11053    -2.183633     0.003275  0.0003  1.721102  0.019274  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11053          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11054  77385000.0  271309.648646         252.070161  268666.932836  97.266853   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11054    -2.170915       0.0031  0.0003  1.038249  0.019659  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11054          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11055  77392000.0  271333.314104         252.080698  268690.587674   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11055  101.396151    -2.168057     0.003771  0.0003  1.539624  0.020204   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11055  26.01626          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11056  77399000.0  271355.398325         252.091824  268712.660684  80.014959   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11056    -2.176575      0.00189  0.0003  1.754694  0.017619  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11056          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11057  77406000.0  271376.536795         252.102307  268733.788621  84.597863   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11057    -2.181058     0.002148  0.0003  1.087451  0.018788  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11057          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11058  77413000.0  271398.274283         252.113018  268755.515348  84.875032   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11058    -2.184473     0.002204  0.0003  2.520345  0.017939  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11058          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11059  77420000.0  271420.105206         252.123248  268777.335991  88.274553   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11059    -2.186278     0.003081  0.0003  2.199032   0.01929  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11059          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11060  77427000.0  271442.188597         252.132623  268799.409957  96.272603   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11060    -2.181975     0.003791  0.0003  1.695689  0.020132  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11060          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11061  77434000.0  271463.865086         252.143018  268821.075994  85.571565   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11061    -2.182122     0.002176  0.0003  1.714387  0.018911  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11061          1000.0     -5.007243  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11062  77441000.0  271486.11299         252.152395  268843.314467  95.564961   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11062    -2.189042     0.003264  0.0003  1.682856  0.018798  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11062          1000.0     -5.007243  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11063  77448000.0  271507.836197          252.16206  268865.027936  72.650239   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11063    -2.189697     0.001993  0.0003  1.968224   0.01781  26.01626   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11063          1000.0     -5.007243  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11064  77455000.0  271546.363961         252.173189  268886.821002  71.324147   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11064    -2.184573     0.002465  0.0003  2.44112  0.018641  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11064          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11065  77462000.0  271570.228368         252.184249  268910.67429  89.648083   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11065    -2.176816     0.003445  0.0003  2.060567   0.01946  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11065          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11066  77469000.0  271594.601158         252.195733  268935.035537  92.251837   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11066     -2.17639     0.003298  0.0003  2.038813  0.019639  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11066          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11067  77476000.0  271616.152392         252.205455  268956.577002   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11067  110.468104     -2.17567     0.003091  0.0003  1.945457  0.019615   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11067  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11068  77483000.0  271637.043858         252.215252  268977.458611  56.858312   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11068    -2.191192     0.003087  0.0003  1.038117  0.019719  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11068          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11069  77490000.0  271658.569017         252.225018  268998.973934  56.174729   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11069    -2.191105     0.002825  0.0003  1.092702   0.01957  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11069          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11070  77497000.0  271679.976376         252.237432  269020.368819  88.654906   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11070    -2.180048     0.002256  0.0003  1.623105  0.018418  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11070          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11071  77504000.0  271702.011415         252.247236  269042.393953  97.354189   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11071    -2.176544     0.002924  0.0003  1.516556  0.019544  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11071          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11072  77511000.0  271723.972452         252.257329  269064.344849  79.782069   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11072    -2.170146     0.002419  0.0003  1.675699  0.018245  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11072          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11073  77518000.0  271746.058733         252.266687  269086.421721   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11073  116.489954    -2.188018     0.002224  0.0003  1.187073  0.018432   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11073  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11074  77525000.0  271767.716762         252.276231  269108.07013  100.687616   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11074    -2.172422      0.00283  0.0003  1.685136   0.01891  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11074          1000.0     -6.665394  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11075  77532000.0  271789.88947         252.286765  269130.232242  81.633173   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11075    -2.172704     0.002224  0.0003  1.855375  0.018273  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11075          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11076  77539000.0  271811.886664         252.296572  269152.219578  71.027219   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11076     -2.18247     0.002493  0.0003  1.547137  0.019085  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11076          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11077  77546000.0  271834.337845          252.30717  269174.660111  89.02242   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11077    -2.185777     0.002284  0.0003  1.351491  0.018611  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11077          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11078  77553000.0  271856.397022         252.317505  269196.708892  75.444072   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11078    -2.174601     0.002318  0.0003  1.706412  0.018225  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11078          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11079  77560000.0  271877.834388         252.328176  269218.135533  89.240521   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11079    -2.184056     0.002332  0.0003  1.70868   0.01812  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11079          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11080  77567000.0  271899.068936         252.336782  269239.361414  90.966001   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11080    -2.176742     0.002072  0.0003  1.706035  0.018482  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11080          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11081  77574000.0  271922.061595         252.347126  269262.343678  74.232481   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11081    -2.195035     0.002439  0.0003  1.060212  0.018558  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11081          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11082  77581000.0  271944.227449         252.356794  269284.499806  98.236606   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11082    -2.172452     0.002597  0.0003  1.287575  0.018696  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11082          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11083  77588000.0  271965.864071         252.366997  269306.126165  91.875717   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11083    -2.186141     0.003193  0.0003  1.407697  0.019608  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11083          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11084  77595000.0  271987.700126         252.377801  269327.951367  91.082014   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11084    -2.180377     0.002734  0.0003  1.694355  0.019092  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11084          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11085  77602000.0  272008.644919         252.389228  269348.884664  89.529157   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11085    -2.166677     0.002997  0.0003  1.744473  0.019457  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11085          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11086  77609000.0  272030.325805         252.401104  269370.553608  79.552837   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11086    -2.187255     0.002713  0.0003  1.492527  0.018757  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11086          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11087  77616000.0  272051.752492          252.41313  269391.968214  68.272874   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11087    -2.175316     0.002022  0.0003  1.305991  0.017903  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11087          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11088  77623000.0  272073.320763         252.422856  269413.526709  98.333731   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11088    -2.162289       0.0028  0.0003  1.075983  0.019057  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11088          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11089  77630000.0  272094.926714         252.432506  269435.122953   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11089  114.101395    -2.160763     0.002584  0.0003  1.778635  0.019191   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11089  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11090  77637000.0  272116.237801         252.441317  269456.425171  80.293864   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11090    -2.192334     0.003238  0.0003  1.761421  0.018775  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11090          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11091  77644000.0  272138.236046         252.451383  269478.413297  74.64724   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11091    -2.186905     0.002845  0.0003  1.308913   0.02052  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11091          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11092  77651000.0  272159.755289         252.460648  269499.923224   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11092  102.830373    -2.189359     0.002919  0.0003  1.279479  0.020191   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11092  27.059631          1000.0     -6.665394  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11093  77658000.0  272181.63507         252.470992  269521.792608  92.404872   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11093    -2.172885     0.002551  0.0003  0.938152  0.019222  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11093          1000.0     -6.665394  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "11094  77665000.0  272203.7663         252.480826  269543.913924  90.393484   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11094    -2.177967     0.002436  0.0003  0.774961  0.018809  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11094          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11095  77672000.0  272225.602962         252.492773  269565.738583  99.50956   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11095    -2.175012     0.002324  0.0003  1.332704  0.018644  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11095          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11096  77679000.0  272247.409323         252.502836  269587.53483  103.40444   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11096    -2.161868     0.002827  0.0003  1.242603  0.018826  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11096          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11097  77686000.0  272269.331166         252.512885  269609.446561  83.95943   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11097    -2.174846     0.002749  0.0003  0.975205  0.019552  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11097          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11098  77693000.0  272291.072979         252.523697  269631.177513  105.13588   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11098    -2.172944     0.002268  0.0003  1.377213  0.018042  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11098          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11099  77700000.0  272312.371523          252.53368  269652.466021  78.608632   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11099    -2.176699     0.002164  0.0003  1.250226   0.01826  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11099          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11100  77707000.0  272333.743576         252.543388  269673.828304  80.801688   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11100    -2.185381     0.001996  0.0003  1.280821  0.018274  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11100          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11101  77714000.0  272355.288585         252.552986  269695.363663  78.927383   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11101    -2.189008      0.00238  0.0003  1.216717  0.019339  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11101          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11102  77721000.0  272378.074943         252.562115  269718.140843  79.706959   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11102    -2.185377     0.004725  0.0003  1.170258  0.020246  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11102          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11103  77728000.0  272402.115133         252.571297  269742.171796  90.115406   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11103    -2.171665      0.00431  0.0003  1.407116  0.021574  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11103          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11104  77735000.0  272426.149106         252.580953  269766.196015  82.044609   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11104    -2.185211     0.002526  0.0003  1.456072  0.019333  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11104          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11105  77742000.0  272449.966817         252.590609  269790.003987  90.571254   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11105    -2.183642     0.003726  0.0003  0.760043  0.020115  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11105          1000.0     -6.665394  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11106  77749000.0  272474.04741         252.600764  269814.074375  88.943124   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11106    -2.163215     0.002314  0.0003  1.169606  0.019568  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11106          1000.0     -6.665394  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11107  77756000.0  272499.52478         252.610368  269839.542091  84.152445   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11107    -2.171002     0.002883  0.0003  0.892196  0.019067  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11107          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11108  77763000.0  272524.544924         252.620373  269864.552178  88.529699   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11108    -2.162426     0.002747  0.0003  1.677137  0.019322  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11108          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11109  77770000.0  272548.204522         252.630403  269888.201689   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11109  106.920555    -2.187863     0.002044  0.0003  1.583502  0.018186   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11109  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11110  77777000.0  272572.244137          252.63997  269912.231669  95.520549   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11110    -2.177583     0.002645  0.0003  1.939805  0.018038  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11110          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11111  77784000.0  272595.844308         252.650225  269935.821526   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11111  127.380788    -2.177039     0.002643  0.0003  1.624223  0.019123   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11111  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11112  77791000.0  272619.852293         252.662753  269959.816937  87.014444   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11112    -2.178259     0.002608  0.0003  1.755782  0.018269  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11112          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11113  77798000.0  272643.732049         252.672999  269983.686396  96.67328   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11113    -2.170606     0.002168  0.0003  1.764495  0.018031  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11113          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11114  77805000.0  272665.465008          252.68272  270005.409584  87.950222   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11114    -2.202569     0.002316  0.0003  1.795241  0.018657  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11114          1000.0     -6.665394  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "11115  77812000.0  272686.64924         252.692484  270026.583915  107.370742   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11115    -2.179326     0.003896  0.0003  1.486158  0.019773  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11115          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11116  77819000.0  272707.890018          252.70558  270047.811532   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11116  104.805097    -2.160955      0.00184  0.0003  1.635045  0.017474   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11116  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11117  77826000.0  272729.365619         252.715614  270069.277051   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "11117  107.395574     -2.17928     0.003112  0.0003   1.1989  0.018963   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11117  27.059631          1000.0     -6.665394  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11118  77833000.0  272750.07771         252.725329  270089.979369  95.595226   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11118    -2.175582      0.00218  0.0003  0.877085  0.018527  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11118          1000.0     -6.665394  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "11119  77840000.0  272771.2058         252.734054  270111.098685  86.992405   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11119    -2.197303     0.002221  0.0003  1.226763  0.018723  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11119          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11120  77847000.0  272794.204782         252.744121  270134.087525  90.037964   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11120    -2.183681     0.003314  0.0003  1.241647  0.019421  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11120          1000.0     -6.665394  \n",
      "         timestep          time  collect_wait_time    train_time      score  \\\n",
      "11121  77854000.0  272818.12729         252.755167  270157.99893  81.177257   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11121    -2.170217     0.003301  0.0003  1.796918  0.019758  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11121          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11122  77861000.0  272840.179293         252.765186  270180.040777  77.285476   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11122    -2.183246      0.00217  0.0003  2.007107  0.018121  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11122          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11123  77868000.0  272861.436914         252.775894  270201.287627  88.845335   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11123    -2.185808     0.001801  0.0003  1.846898  0.017779  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11123          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11124  77875000.0  272883.020591         252.786826  270222.860314  83.640657   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11124    -2.186214     0.002914  0.0003  1.302975  0.019441  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11124          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11125  77882000.0  272904.638305         252.797255  270244.467552  83.303936   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11125    -2.169371     0.003229  0.0003  1.762071  0.019287  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11125          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11126  77889000.0  272926.789907         252.806825  270266.609538  77.899814   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11126    -2.183045     0.002122  0.0003  1.503899  0.018958  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11126          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11127  77896000.0  272948.308822          252.81722  270288.117996  86.132489   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11127    -2.166391     0.003355  0.0003  1.589266  0.018574  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11127          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11128  77903000.0  272970.139554         252.827062  270309.938829  97.500175   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11128    -2.158811     0.002638  0.0003  1.162682  0.017946  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11128          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11129  77910000.0  272991.536047          252.84002  270331.322289   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11129  107.335694    -2.175772     0.004192  0.0003  1.275447  0.020242   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11129  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11130  77917000.0  273012.790773         252.850352  270352.566522  91.576703   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11130    -2.176392      0.00354  0.0003  1.659555  0.020008  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11130          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11131  77924000.0  273034.166806         252.864704  270373.928118  80.771104   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11131    -2.182694     0.002518  0.0003  1.38823  0.019629  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11131          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11132  77931000.0  273055.643296         252.881958  270395.387296  94.194196   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11132    -2.181562      0.00259  0.0003  1.218538  0.018742  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11132          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11133  77938000.0  273077.044225         252.895497  270416.774636  75.675413   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11133    -2.165429      0.00191  0.0003  1.179926  0.017998  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11133          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11134  77945000.0  273098.158165         252.909362  270437.874655  113.47758   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11134    -2.172994     0.002671  0.0003  1.329835  0.018951  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11134          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11135  77952000.0  273120.096185         252.922985  270459.798989  92.540786   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11135    -2.168684     0.001685  0.0003  1.24685  0.017354  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11135          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11136  77959000.0  273142.294579         252.933764  270481.98655  89.748301   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11136    -2.178559     0.002646  0.0003   1.2639  0.019398  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11136          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11137  77966000.0  273163.902931         252.943224  270503.585353  89.797355   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11137    -2.171185     0.003278  0.0003  1.002094  0.019378  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11137          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11138  77973000.0  273187.995452         252.955276  270527.66576  122.055672   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11138    -2.179823     0.004003  0.0003  0.409259  0.020067  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11138          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11139  77980000.0  273210.600201         252.966113  270550.259613   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11139  107.366778    -2.196667     0.002308  0.0003  0.706283  0.019074   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11139  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11140  77987000.0  273232.019962         252.975826  270571.669605   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11140  110.449128    -2.165433      0.00185  0.0003  1.578243  0.018303   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11140  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11141  77994000.0  273253.889224         252.986575  270593.528047  79.777732   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11141    -2.190502     0.001946  0.0003  0.969784  0.018638  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11141          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11142  78001000.0  273275.345381         252.997465  270614.973254   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11142  107.578114    -2.195835     0.003089  0.0003  1.174302   0.01958   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11142  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11143  78008000.0  273297.012858         253.008874  270636.629273  92.760026   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11143     -2.18077     0.002318  0.0003  1.284239  0.019169  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11143          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11144  78015000.0  273318.325203         253.018166  270657.932272  88.675447   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11144    -2.182798     0.002242  0.0003  1.167088  0.019318  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11144          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11145  78022000.0  273340.060141         253.029509  270679.655797  87.712035   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11145    -2.178828     0.002252  0.0003  1.748622  0.018592  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11145          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11146  78029000.0  273361.942152         253.040205  270701.527055  91.813669   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11146    -2.188187     0.003566  0.0003  1.15186  0.020436  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11146          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11147  78036000.0  273384.340775          253.05004  270723.915785  72.125338   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11147    -2.181635     0.002341  0.0003  1.59168  0.018618  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11147          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11148  78043000.0  273406.271036         253.059519  270745.836508   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11148  103.125148    -2.163736     0.002694  0.0003  1.862314  0.019237   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11148  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11149  78050000.0  273428.139851         253.069522  270767.69527  96.430749   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11149    -2.169474     0.002869  0.0003  1.258574  0.019413  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11149          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11150  78057000.0  273450.256499         253.079099  270789.802287  87.206125   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11150    -2.186201     0.002571  0.0003  1.067231  0.019601  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11150          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11151  78064000.0  273472.413689         253.091304  270811.947219   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11151  111.245945    -2.166314     0.002586  0.0003  1.080662  0.019554   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11151  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11152  78071000.0  273494.612497         253.103151  270834.134128  95.898278   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11152    -2.169417     0.002588  0.0003  0.759573  0.019037  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11152          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11153  78078000.0  273516.715506         253.113607  270856.22662  98.350428   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11153    -2.179652     0.002319  0.0003  0.635272  0.019006  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11153          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11154  78085000.0  273538.411856         253.124264  270877.912263  82.276836   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11154    -2.174172     0.003425  0.0003  1.132845  0.019475  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11154          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11155  78092000.0  273560.368362         253.134052  270899.858932  85.29152   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11155    -2.150743     0.003072  0.0003  0.827505  0.019316  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11155          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11156  78099000.0  273583.877107         253.144979  270923.356699  94.345056   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11156    -2.179261     0.002592  0.0003  1.021011   0.01902  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11156          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11157  78106000.0  273605.419454         253.154707  270944.889249  85.423574   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11157    -2.184097     0.002438  0.0003  1.189393  0.019636  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11157          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11158  78113000.0  273627.093022         253.166043  270966.551426   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11158  106.081784    -2.188525     0.002636  0.0003  1.130726  0.019643   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11158  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11159  78120000.0  273648.439183         253.175488  270987.888089  99.094056   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11159    -2.181126     0.002178  0.0003  1.352782  0.018895  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11159          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11160  78127000.0  273669.861908         253.185226  271009.301028  97.993165   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11160    -2.169333     0.002595  0.0003  1.108377  0.019605  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11160          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11161  78134000.0  273691.406623         253.195549  271030.835368   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11161  115.226492    -2.197299     0.003204  0.0003  0.780391  0.019181   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11161  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11162  78141000.0  273712.698209         253.205092  271052.117358   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "11162  103.099629     -2.20481     0.002301  0.0003  1.19154  0.019302   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11162  27.059631          1000.0     -6.665394  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11163  78148000.0  273734.288054         253.214999  271073.697247  87.412283   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11163    -2.179016     0.003381  0.0003  1.396546  0.020015  27.059631   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11163          1000.0     -6.665394  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11164  78155000.0  273772.640777         253.224983  271095.471171  81.336975   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11164    -2.187683     0.002937  0.0003  1.323189  0.019282  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11164          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11165  78162000.0  273797.132743         253.237057  271119.951003  80.505826   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11165    -2.178858     0.002672  0.0003  0.989275  0.019129  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11165          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11166  78169000.0  273820.928405         253.246714  271143.73696  72.908431   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11166    -2.186456     0.003139  0.0003  0.915825  0.019687  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11166          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11167  78176000.0  273845.068913         253.258151  271167.865975  86.510181   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11167    -2.179818     0.002258  0.0003 -0.032097  0.018499  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11167          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11168  78183000.0  273869.140253         253.268788  271191.926622  93.115084   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11168    -2.174228     0.002593  0.0003  1.423853  0.019866  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11168          1000.0     -3.168355  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11169  78190000.0  273892.63151         253.278423  271215.408178  99.854507   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11169    -2.169825     0.002403  0.0003  1.141049  0.019157  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11169          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11170  78197000.0  273917.805157         253.289521  271240.570671  86.523496   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11170    -2.177324      0.00305  0.0003  0.902934  0.020239  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11170          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11171  78204000.0  273941.598299         253.298381  271264.354897  79.716861   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11171    -2.186059     0.002102  0.0003  1.162065  0.018358  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11171          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11172  78211000.0  273965.819775         253.308668  271288.566034  90.88774   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11172    -2.196622     0.002365  0.0003  1.135862  0.018807  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11172          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11173  78218000.0  273989.950994          253.31816  271312.687711  93.250245   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11173    -2.168209     0.002897  0.0003  1.107181   0.01958  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11173          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11174  78225000.0  274013.521897         253.328737  271336.247973  86.303927   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11174     -2.17036     0.003954  0.0003  1.219628  0.019742  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11174          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11175  78232000.0  274037.534268         253.338657  271360.250363  87.966658   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11175    -2.184937     0.002051  0.0003  1.080632  0.018504  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11175          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11176  78239000.0  274061.628367         253.348725  271384.334338  87.836899   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11176    -2.188335     0.001823  0.0003  0.749163  0.017868  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11176          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11177  78246000.0  274086.603839         253.358556  271409.299921  92.374079   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11177    -2.165093     0.002928  0.0003 -0.979263  0.019878  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11177          1000.0     -3.168355  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "11178  78253000.0  274110.67475          253.36848  271433.360818  80.93746   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11178    -2.183961     0.003898  0.0003  0.656276  0.019974  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11178          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11179  78260000.0  274132.254171         253.379635  271454.929031   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11179  102.543399    -2.172346     0.002922  0.0003  1.144279  0.019956   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11179  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11180  78267000.0  274153.645756         253.389263  271476.310932  87.584846   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11180    -2.182466     0.004271  0.0003  1.065847  0.020085  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11180          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11181  78274000.0  274175.234509         253.401348  271497.887549   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11181  110.398501    -2.178869     0.002932  0.0003  0.793179  0.020013   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11181  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11182  78281000.0  274196.406575         253.410573  271519.050333  89.823864   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11182    -2.188918     0.002785  0.0003  0.480639  0.019339  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11182          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11183  78288000.0  274217.865123         253.421584  271540.497805  75.374022   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11183    -2.192535     0.002879  0.0003  0.301212  0.020632  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11183          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11184  78295000.0  274241.241349          253.43052  271563.865044  93.121216   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11184    -2.189818     0.002717  0.0003  0.381519  0.019433  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11184          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11185  78302000.0  274265.562901         253.442946  271588.174112   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11185  109.819144    -2.192052     0.002603  0.0003  1.319041    0.0193   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11185  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11186  78309000.0  274288.961236         253.453176  271611.562155  82.554452   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11186    -2.171189     0.002196  0.0003  1.191693  0.019586  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11186          1000.0     -3.168355  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "11187  78316000.0  274312.3001         253.464005  271634.890125  64.623609   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11187    -2.187521     0.002624  0.0003  1.008098  0.019464  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11187          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11188  78323000.0  274336.462054         253.474581  271659.041443  77.713557   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11188    -2.177809     0.002723  0.0003  1.095325  0.018416  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11188          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11189  78330000.0  274359.674491         253.484535  271682.243864  83.044723   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11189    -2.181453     0.003336  0.0003  1.315955  0.019745  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11189          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11190  78337000.0  274383.508844         253.495236  271706.067459  78.667231   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11190    -2.175263     0.004634  0.0003  0.695201   0.02061  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11190          1000.0     -3.168355  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "11191  78344000.0  274407.5819         253.505051  271730.130643  84.951177   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11191    -2.176338     0.003175  0.0003  0.781845  0.020116  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11191          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11192  78351000.0  274429.090495         253.515527  271751.628709  94.054714   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11192    -2.177189     0.002974  0.0003  1.008734  0.019022  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11192          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11193  78358000.0  274450.624994         253.525442  271773.153244  83.232865   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11193    -2.163114     0.001914  0.0003  1.093486   0.01869  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11193          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11194  78365000.0  274472.050327         253.535139  271794.568832  74.007519   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11194    -2.198992     0.003481  0.0003  0.796002  0.019702  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11194          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11195  78372000.0  274494.010938         253.545369  271816.519114   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "11195  101.589032    -2.176175     0.004154  0.0003  1.52838  0.019933   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11195  18.521315          1000.0     -3.168355  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "11196  78379000.0  274515.63439         253.557216  271838.130659  106.341064   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11196    -2.198339     0.002292  0.0003  0.875015  0.019187  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11196          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11197  78386000.0  274537.164175         253.567236  271859.650338  90.567365   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11197    -2.184184     0.002567  0.0003  1.297275  0.018621  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11197          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11198  78393000.0  274558.385744         253.585649  271880.853435  88.972166   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11198    -2.170911     0.002101  0.0003  1.373648  0.018482  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11198          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11199  78400000.0  274580.105199         253.596721  271902.561743  72.03003   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11199    -2.192629     0.003592  0.0003  1.131804  0.020356  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11199          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11200  78407000.0  274601.364888         253.610186  271923.807903  78.581385   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11200    -2.177108     0.002429  0.0003  1.237999  0.019244  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11200          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time   train_time      score  \\\n",
      "11201  78414000.0  274622.889663          253.62231  271945.3205  89.750245   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11201    -2.186623     0.003302  0.0003  1.57463  0.019746  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11201          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11202  78421000.0  274647.065316         253.634271  271969.484141  90.520561   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11202     -2.18855     0.003601  0.0003  1.247655  0.019467  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11202          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11203  78428000.0  274671.426507         253.645027  271993.834508   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11203  105.908737    -2.190453     0.004046  0.0003  1.433311  0.019631   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11203  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11204  78435000.0  274694.464609         253.655218  272016.862366  83.589964   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11204    -2.180938     0.003441  0.0003  1.351095  0.019564  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11204          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11205  78442000.0  274718.978976         253.665951  272041.365939  84.259577   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11205    -2.194258     0.002433  0.0003  1.354304  0.019274  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11205          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11206  78449000.0  274742.620221         253.675338  272064.997734   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11206  104.966231     -2.17926     0.002627  0.0003  1.092405  0.019189   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11206  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11207  78456000.0  274766.894259         253.685452  272089.261595   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11207  104.133703    -2.187925     0.002842  0.0003  1.319368  0.019317   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11207  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11208  78463000.0  274790.518245         253.695808  272112.875171  98.067507   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11208    -2.176612     0.002373  0.0003  0.940864  0.019508  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11208          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11209  78470000.0  274813.056248         253.706198  272135.402717  77.909537   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11209    -2.169831     0.003024  0.0003  0.600699  0.019768  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11209          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11210  78477000.0  274836.570453         253.718905  272158.90412  87.244362   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11210    -2.189795     0.002717  0.0003  1.16852  0.018799  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11210          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11211  78484000.0  274860.128069         253.729207  272182.451365  87.361082   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11211    -2.187642     0.003268  0.0003  1.184086  0.019408  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11211          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11212  78491000.0  274884.749456         253.740682  272207.061195  89.683615   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11212    -2.177665     0.002605  0.0003  1.555731  0.019094  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11212          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11213  78498000.0  274909.039712          253.75428  272231.337796  98.230697   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11213     -2.17087     0.003379  0.0003  1.406979  0.019764  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11213          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11214  78505000.0  274933.051448         253.772629  272255.331114   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11214  101.037823    -2.164978     0.002434  0.0003  1.049556  0.019264   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11214  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11215  78512000.0  274954.679309         253.784089  272276.947459  98.942608   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11215    -2.185231     0.002219  0.0003  1.527287  0.019296  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11215          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11216  78519000.0  274976.493704         253.794199  272298.751695  86.435683   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11216    -2.158411     0.002281  0.0003  1.275987  0.018472  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11216          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11217  78526000.0  274998.090142         253.805306  272320.336965  92.142932   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11217    -2.177493     0.003591  0.0003  0.967466  0.019843  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11217          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11218  78533000.0  275019.655446         253.814837  272341.892688  70.243567   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11218    -2.185898     0.003137  0.0003  1.069277  0.019957  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11218          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11219  78540000.0  275041.090502          253.82381  272363.318715  91.521516   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11219    -2.198036     0.002525  0.0003  1.101754  0.019041  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11219          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11220  78547000.0  275064.916708         253.834615  272387.134034  89.699149   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11220    -2.171562     0.002533  0.0003  1.46618  0.018697  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11220          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11221  78554000.0  275089.239723         253.846083  272411.445517  87.230504   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11221    -2.183391     0.002644  0.0003  1.344286  0.019412  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11221          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11222  78561000.0  275113.443335         253.855644  272435.639477  83.245113   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11222    -2.180806     0.002735  0.0003  0.69371  0.019522  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11222          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11223  78568000.0  275137.220433         253.865972  272459.406188   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11223  101.399526    -2.182437     0.003513  0.0003  0.622322  0.019402   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11223  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11224  78575000.0  275160.949035         253.879795  272483.120916  86.079203   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11224    -2.175713     0.002644  0.0003  1.144995  0.019611  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11224          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11225  78582000.0  275185.279221         253.894075  272507.436746   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11225  128.769621    -2.183324     0.002417  0.0003  1.562957  0.018715   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11225  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11226  78589000.0  275208.631876         253.902158  272530.78126  94.981864   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11226    -2.195918     0.002834  0.0003  0.613759  0.019581  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11226          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11227  78596000.0  275232.666111         253.912531  272554.805047  95.864721   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11227    -2.180124     0.003514  0.0003  0.95883  0.019748  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11227          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11228  78603000.0  275256.753013          253.92257  272578.881857  73.081344   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11228    -2.189151     0.002552  0.0003  1.358163  0.018633  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11228          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11229  78610000.0  275280.627862         253.932902  272602.74629  106.933594   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11229     -2.19048     0.002111  0.0003  1.056448  0.018147  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11229          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11230  78617000.0  275304.948244         253.942151  272627.057346  93.792461   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11230    -2.176966     0.003499  0.0003  1.018831  0.020034  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11230          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11231  78624000.0  275329.384761         253.952775  272651.483186   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11231  103.220724    -2.178134     0.002154  0.0003  1.079052  0.019305   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11231  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11232  78631000.0  275353.646815         253.963093  272675.734837  78.853376   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11232    -2.183433     0.002844  0.0003  0.762502  0.019414  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11232          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11233  78638000.0  275377.909077         253.972806  272699.987329  97.717077   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11233    -2.187066     0.001658  0.0003  1.41169  0.018306  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11233          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11234  78645000.0  275401.406068         253.981747  272723.475287   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "11234  105.631016    -2.168975     0.002296  0.0003  1.34862  0.018145   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11234  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11235  78652000.0  275425.956422         253.992057  272748.015244   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11235  112.100519    -2.177058      0.00378  0.0003  1.350283  0.019575   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11235  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11236  78659000.0  275451.168846         254.000466  272773.219199   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11236  113.935143    -2.179899     0.003492  0.0003  1.329462  0.018989   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11236  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11237  78666000.0  275474.848697         254.008559  272796.890905  96.033797   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11237    -2.178691     0.002877  0.0003  1.499132  0.019838  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11237          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11238  78673000.0  275499.262401         254.020417  272821.29269  109.608538   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11238     -2.19231     0.003389  0.0003  1.550302  0.019444  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11238          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11239  78680000.0  275523.500397         254.034549  272845.516431  70.09156   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11239    -2.169559     0.002361  0.0003  1.753728  0.018985  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11239          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11240  78687000.0  275546.642071         254.043145  272868.649459  82.913309   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11240    -2.186799     0.002726  0.0003  1.26544  0.019621  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11240          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11241  78694000.0  275568.148606         254.050243  272890.148836  79.895541   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11241    -2.171842     0.002934  0.0003  1.629895  0.019023  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11241          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11242  78701000.0  275589.858092         254.061092  272911.847415   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11242  110.367917    -2.179843     0.002105  0.0003  1.467849  0.018536   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11242  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11243  78708000.0  275611.676283         254.069041  272933.65761  75.352184   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11243    -2.172219     0.002184  0.0003  1.521725  0.018472  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11243          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11244  78715000.0  275632.830115         254.076053  272954.804385   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11244  109.927952    -2.184625     0.004301  0.0003  1.216829  0.020897   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11244  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11245  78722000.0  275654.786515         254.086258  272976.750527   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11245  104.376048      -2.1942     0.002488  0.0003  1.207055   0.01841   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11245  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11246  78729000.0  275676.538602         254.095564  272998.493253  92.191927   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11246    -2.178358     0.003263  0.0003  1.168581  0.019283  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11246          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11247  78736000.0  275698.262436         254.114271  273020.198323  90.643115   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11247    -2.180502     0.002544  0.0003  1.238356  0.019882  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11247          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11248  78743000.0  275720.129491         254.121391  273042.058202   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11248  111.875035    -2.182025       0.0022  0.0003  1.407278  0.019002   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11248  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11249  78750000.0  275742.213079         254.131554  273064.131553  68.624288   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11249    -2.178078     0.002197  0.0003  1.161221  0.018294  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11249          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11250  78757000.0  275764.309966         254.141732  273086.218204   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11250  100.545429    -2.186755     0.002703  0.0003  1.068318  0.019001   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11250  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11251  78764000.0  275786.025565          254.15094  273107.924537  78.379668   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11251    -2.189993     0.002892  0.0003  0.834802  0.019891  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11251          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11252  78771000.0  275807.781938         254.158819  273129.672974   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11252  104.734957    -2.166272     0.003382  0.0003  0.925783  0.019552   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11252  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11253  78778000.0  275829.514376         254.169748  273151.394426  96.014597   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11253    -2.182444     0.003381  0.0003  0.913972  0.019292  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11253          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11254  78785000.0  275851.570243         254.180037  273173.439893  95.079832   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11254    -2.176914     0.004088  0.0003  0.843295  0.020527  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11254          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11255  78792000.0  275873.761438         254.191369  273195.619697   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11255  112.137966    -2.170906      0.00248  0.0003  1.058413  0.018832   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11255  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11256  78799000.0  275895.767124         254.200603  273217.616099  98.272882   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11256    -2.172472     0.003796  0.0003  1.407932  0.020396  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11256          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11257  78806000.0  275917.433845         254.210715  273239.272658  98.386042   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11257    -2.169806     0.002246  0.0003  1.010115  0.018982  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11257          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11258  78813000.0  275939.357295         254.220659  273261.186112  89.658678   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11258    -2.188088     0.003248  0.0003  1.094399  0.018503  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11258          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11259  78820000.0  275961.190247          254.22839  273283.011284   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11259  112.830997    -2.177033     0.003316  0.0003  1.262959  0.019816   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11259  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11260  78827000.0  275982.821917         254.237703  273304.633584   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11260  102.032155    -2.173675     0.002657  0.0003  1.264241  0.018743   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11260  18.521315          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11261  78834000.0  276004.695482         254.248615  273326.49618  92.992187   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11261    -2.189553     0.003828  0.0003  0.971939  0.019881  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11261          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "11262  78841000.0  276028.517391         254.258754  273350.307898  99.7805   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11262    -2.200626     0.002789  0.0003  0.637291   0.01933  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11262          1000.0     -3.168355  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11263  78848000.0  276053.860691         254.267336  273375.642559  77.179685   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11263    -2.181799     0.002405  0.0003  1.46018  0.018593  18.521315   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11263          1000.0     -3.168355  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11264  78855000.0  276095.059821         254.277465  273399.954104  84.813543   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11264     -2.17626     0.002496  0.0003  1.692481  0.018139  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11264          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11265  78862000.0  276117.930086          254.28844  273422.813299  72.040409   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11265    -2.197727     0.003657  0.0003  1.140704  0.020009  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11265          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11266  78869000.0  276139.370929         254.297797  273444.244703  90.076782   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11266    -2.181076     0.001642  0.0003  0.868197  0.017582  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11266          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time   train_time       score  \\\n",
      "11267  78876000.0  276161.333981         254.308992  273466.1965  113.632299   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11267    -2.180011     0.002123  0.0003  1.156459  0.018347  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11267          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11268  78883000.0  276183.67627         254.320417  273488.527302  80.920175   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11268    -2.173059     0.002244  0.0003  1.113441  0.018341  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11268          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11269  78890000.0  276205.497712         254.329175  273510.339941   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11269  105.929129     -2.19903     0.002337  0.0003  1.160966  0.018469   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11269  28.71883          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "11270  78897000.0  276227.27905         254.337615  273532.112777  82.19368   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11270    -2.193317     0.004947  0.0003  1.464355  0.020664  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11270          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11271  78904000.0  276249.232806         254.348228  273554.055868   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11271  103.848495    -2.186247     0.003657  0.0003  1.245889  0.020046   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11271  28.71883          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "11272  78911000.0  276274.35836         254.357486  273579.172061  91.97522   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11272    -2.185037     0.002518  0.0003  1.026852  0.019082  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11272          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11273  78918000.0  276298.381842         254.370831  273603.182127  89.084543   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11273    -2.193912     0.002128  0.0003  1.254614  0.018993  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11273          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11274  78925000.0  276322.605277          254.37865  273627.397688  93.880281   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11274    -2.198873     0.002137  0.0003  1.274667  0.018385  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11274          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11275  78932000.0  276347.954197         254.388896  273652.736308  91.709919   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11275    -2.189387     0.003774  0.0003  1.56386  0.019616  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11275          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11276  78939000.0  276372.437795         254.398762  273677.209986  93.934141   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11276    -2.201329      0.00348  0.0003  1.431657  0.020015  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11276          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11277  78946000.0  276396.494645         254.408041  273701.257502  88.178046   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11277    -2.187786     0.002261  0.0003  1.003009  0.018727  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11277          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11278  78953000.0  276420.551009         254.416409  273725.305446  92.743963   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11278    -2.195124     0.002594  0.0003  1.511338  0.019006  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11278          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11279  78960000.0  276445.472335         254.426431  273750.216697  68.823926   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11279    -2.203855     0.001904  0.0003  0.950509  0.018298  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11279          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11280  78967000.0  276469.367162         254.440575  273774.097321  87.814443   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11280    -2.211616     0.002022  0.0003  1.25077  0.018585  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11280          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11281  78974000.0  276492.943186         254.450529  273797.663334  85.073911   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11281    -2.182994     0.003575  0.0003  1.74098  0.019552  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11281          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11282  78981000.0  276516.485989         254.460469  273821.196083  91.223709   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11282    -2.205721     0.003194  0.0003  1.633874  0.018646  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11282          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11283  78988000.0  276540.924511         254.472332  273845.622676  86.56463   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11283    -2.197986     0.002569  0.0003  1.795787   0.01934  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11283          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11284  78995000.0  276564.812161         254.483234  273869.499363  93.29091   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11284    -2.200016      0.00351  0.0003  1.625303  0.019644  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11284          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11285  79002000.0  276589.045405         254.494745  273893.721044  92.826121   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11285    -2.200635     0.001809  0.0003  1.065456  0.018073  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11285          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11286  79009000.0  276612.186263         254.505369  273916.851206  76.294616   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11286    -2.198243     0.002965  0.0003  1.278222   0.01918  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11286          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11287  79016000.0  276635.999869         254.518326  273940.651582   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11287  100.853756    -2.188393     0.003383  0.0003  1.172496   0.01941   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11287  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11288  79023000.0  276660.008731         254.533328  273964.645354  67.190051   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11288    -2.214906     0.002244  0.0003  1.035003  0.018742  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11288          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11289  79030000.0  276684.382151         254.542458  273989.009593  68.275213   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11289    -2.173426     0.003262  0.0003  0.92215  0.018981  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11289          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11290  79037000.0  276708.240907         254.552406  274012.858342  95.797203   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11290    -2.206301     0.002062  0.0003  1.11471  0.017872  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11290          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11291  79044000.0  276731.821686         254.565648  274036.425822   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11291  101.274662    -2.200335     0.003527  0.0003  0.787991  0.019685   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11291  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11292  79051000.0  276756.340326         254.578953  274060.931093   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11292  107.543804    -2.185852     0.002299  0.0003  1.055968  0.018215   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11292  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11293  79058000.0  276780.780433         254.590778  274085.359326  73.174208   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11293     -2.18364     0.003366  0.0003  0.158811  0.019796  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11293          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11294  79065000.0  276805.052091         254.600106  274109.621605  81.404671   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11294    -2.189761     0.002495  0.0003  0.935891   0.01889  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11294          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11295  79072000.0  276829.49062         254.610733  274134.049445  78.210324   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11295    -2.207275     0.002569  0.0003  1.825774  0.018185  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11295          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11296  79079000.0  276854.391186         254.617874  274158.94282  111.96889   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11296    -2.200601     0.003322  0.0003  1.542223  0.020101  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11296          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11297  79086000.0  276878.494773         254.628122  274183.036106  96.780017   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11297    -2.189751      0.00227  0.0003  1.142563  0.018479  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11297          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11298  79093000.0  276902.224167         254.641886  274206.751677   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11298  102.356263     -2.20522     0.001814  0.0003  1.073674  0.017775   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11298  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11299  79100000.0  276926.465087         254.650848  274230.983566  80.854625   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11299    -2.192501     0.001947  0.0003  1.050192  0.018488  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11299          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "11300  79107000.0  276950.967421         254.659004  274255.477594  80.5237   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11300    -2.183624     0.002662  0.0003  1.02772  0.018531  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11300          1000.0     -5.503853  \n",
      "         timestep         time  collect_wait_time     train_time       score  \\\n",
      "11301  79114000.0  276975.0044         254.671365  274279.502155  118.840128   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11301    -2.203401     0.002552  0.0003  0.524773  0.019641  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11301          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11302  79121000.0  276998.759388         254.680773  274303.247684  92.668459   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11302    -2.174827     0.002553  0.0003  1.028232  0.018704  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11302          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11303  79128000.0  277022.646637         254.688594  274327.127048   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11303  102.320969    -2.198024     0.002686  0.0003  0.748914  0.019061   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11303  28.71883          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11304  79135000.0  277046.47373         254.704822  274350.937855  79.530925   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11304    -2.200682      0.00297  0.0003  0.875783  0.018907  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11304          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11305  79142000.0  277070.682694          254.71634  274375.135229  75.396538   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11305    -2.172927     0.002932  0.0003  0.959493  0.019481  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11305          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11306  79149000.0  277095.819541         254.725014  274400.26335  89.404963   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11306    -2.193758     0.003219  0.0003  0.53001  0.019328  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11306          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11307  79156000.0  277120.552228         254.733078  274424.987921   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11307  110.949695    -2.192724     0.003295  0.0003  1.188721   0.01948   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11307  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11308  79163000.0  277144.984873         254.742745  274449.410843  89.449063   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11308    -2.194787     0.002291  0.0003  0.684774  0.018456  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11308          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11309  79170000.0  277169.810637         254.753587  274474.225708  100.05305   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11309    -2.192132     0.003117  0.0003  0.746403  0.019417  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11309          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11310  79177000.0  277193.914968         254.762231  274498.321327  81.344031   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11310    -2.213201     0.001976  0.0003  1.031582  0.018453  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11310          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11311  79184000.0  277218.615751         254.770934  274523.013349   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11311  113.512605    -2.200154     0.003962  0.0003  1.012244  0.020907   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11311  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11312  79191000.0  277242.864654         254.781047  274547.252085   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11312  106.042445    -2.209209     0.003031  0.0003  0.769454  0.019404   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11312  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11313  79198000.0  277268.033719         254.790388  274572.411758  83.120659   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11313    -2.198261     0.002162  0.0003  0.334961  0.018338  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11313          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11314  79205000.0  277291.947857         254.799166  274596.31706  91.014408   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11314    -2.184784     0.002787  0.0003  0.92389  0.019355  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11314          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11315  79212000.0  277315.682555         254.806625  274620.044251  94.270371   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11315    -2.194661     0.002341  0.0003  0.210112  0.018662  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11315          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11316  79219000.0  277339.881925         254.815973  274644.234185  84.769882   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11316    -2.208636     0.002422  0.0003  0.71961  0.018691  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11316          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time   train_time      score  \\\n",
      "11317  79226000.0  277364.154957         254.827737  274668.4954  81.112251   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11317    -2.209252      0.00188  0.0003  1.070027   0.01873  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11317          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11318  79233000.0  277388.899379         254.836223  274693.231287  95.365927   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11318    -2.192402      0.00288  0.0003  0.612394  0.018815  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11318          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11319  79240000.0  277413.347378         254.848504  274717.666953  94.524913   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error    return  \\\n",
      "11319    -2.201366      0.00224  0.0003  0.64285  0.019024  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11319          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11320  79247000.0  277437.751554         254.858291  274742.061285  77.561907   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11320    -2.185869     0.002085  0.0003  0.125684  0.019029  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11320          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11321  79254000.0  277462.637213         254.867384  274766.937803  77.321291   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11321     -2.19944     0.002636  0.0003  0.307136  0.019039  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11321          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11322  79261000.0  277486.948507         254.874895  274791.241536  90.066217   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11322    -2.205032     0.002047  0.0003  0.581645  0.018658  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11322          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11323  79268000.0  277511.780665         254.885288  274816.06324  77.041537   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11323    -2.177566     0.002327  0.0003  0.160188  0.019395  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11323          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11324  79275000.0  277535.534734         254.894943  274839.807606  80.716781   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11324    -2.204354     0.003124  0.0003  0.281181  0.019911  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11324          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11325  79282000.0  277557.565348         254.903398  274861.82971  68.300471   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11325    -2.207989     0.002023  0.0003 -0.168111  0.019353  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11325          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11326  79289000.0  277581.485718         254.910573  274885.742856  84.877267   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11326    -2.188598     0.003654  0.0003  0.212761  0.019931  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11326          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11327  79296000.0  277606.531506         254.924582  274910.774571   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11327  100.825116    -2.200867     0.002851  0.0003  0.467968  0.019443   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11327  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11328  79303000.0  277630.736082         254.933416  274934.970254  79.573043   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11328    -2.204606     0.002387  0.0003 -0.168163  0.019579  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11328          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11329  79310000.0  277653.208403         254.942417  274957.433526  72.058126   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11329    -2.182051     0.003016  0.0003 -0.602134  0.020674  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11329          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11330  79317000.0  277674.84796         254.953022  274979.062424  80.410354   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11330    -2.200103     0.003021  0.0003 -0.265505  0.020304  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11330          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11331  79324000.0  277697.056277         254.963256  275001.26046  102.846451   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11331    -2.202073     0.002822  0.0003  0.046198   0.02004  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11331          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11332  79331000.0  277719.413605         254.971701  275023.609273   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11332  104.702115    -2.208083     0.002548  0.0003  0.183925  0.020181   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11332  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11333  79338000.0  277741.334002         254.980127  275045.521192  83.627328   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11333    -2.215298     0.002949  0.0003  0.347598  0.020161  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11333          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11334  79345000.0  277763.501741         254.991003  275067.677996  100.67863   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11334    -2.211639     0.003554  0.0003 -0.058824  0.020781  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11334          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11335  79352000.0  277785.444984         255.002254  275089.60985  98.541513   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11335    -2.204478     0.003919  0.0003  0.315874  0.020521  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11335          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11336  79359000.0  277807.665362         255.013448  275111.818968  95.92551   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11336    -2.206951     0.002614  0.0003 -0.708559  0.019253  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11336          1000.0     -5.503853  \n",
      "         timestep        time  collect_wait_time     train_time      score  \\\n",
      "11337  79366000.0  277829.168          255.02105  275133.313952  89.387126   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11337    -2.212171     0.003361  0.0003 -0.235487  0.020363  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11337          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11338  79373000.0  277851.767374         255.032161  275155.902157   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11338  107.404381    -2.220672     0.003343  0.0003  0.338267  0.020203   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11338  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11339  79380000.0  277873.867232         255.040385  275177.993742  95.97598   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11339     -2.20848     0.002792  0.0003  0.451822  0.019787  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11339          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11340  79387000.0  277896.283561         255.048964  275200.401441  82.968451   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11340    -2.199584     0.004004  0.0003  0.265127  0.020381  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11340          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11341  79394000.0  277918.522278         255.058741  275222.63033  87.177254   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11341    -2.196867     0.003007  0.0003 -0.047357  0.020136  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11341          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11342  79401000.0  277940.946537         255.068349  275245.044913  98.039255   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11342    -2.203983     0.002927  0.0003  0.844922  0.019888  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11342          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11343  79408000.0  277965.183741         255.079811  275269.270591   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "11343  107.048444    -2.192363     0.002997  0.0003  1.26344  0.020422   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11343  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11344  79415000.0  277989.847486         255.087639  275293.926454  79.260044   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11344      -2.2045     0.002952  0.0003  1.129301  0.019853  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11344          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11345  79422000.0  278014.233627         255.097578  275318.302599  79.01983   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11345    -2.205605     0.001911  0.0003  1.409534  0.019102  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11345          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11346  79429000.0  278036.123664         255.108114  275340.182007  83.090767   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11346    -2.194912     0.002954  0.0003  1.196204   0.02008  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11346          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11347  79436000.0  278058.031047         255.117081  275362.080371   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11347  102.263533    -2.209638     0.002615  0.0003  0.893306  0.019097   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11347  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11348  79443000.0  278080.020271         255.125166  275384.061458  80.489382   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11348    -2.216464     0.002633  0.0003  0.713149  0.019038  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11348          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11349  79450000.0  278102.561014         255.135178  275406.592096  99.795327   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11349    -2.214838      0.00264  0.0003  0.901719  0.019627  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11349          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11350  79457000.0  278124.642835          255.14645  275428.662587  78.11816   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11350    -2.201151     0.003185  0.0003  0.468685  0.019503  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11350          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11351  79464000.0  278146.839165         255.154803  275450.850513  73.562212   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11351    -2.205706     0.001971  0.0003  0.825168  0.018637  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11351          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11352  79471000.0  278168.981242         255.163205  275472.984133  99.992074   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11352    -2.201748     0.002148  0.0003  1.043842  0.018855  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11352          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11353  79478000.0  278190.740897         255.172639  275494.734306  92.572771   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11353    -2.204238     0.003654  0.0003  1.039832  0.019684  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11353          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11354  79485000.0  278212.984622         255.181192  275516.969425  83.439343   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11354    -2.219152     0.003288  0.0003  0.403496  0.020289  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11354          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "11355  79492000.0  278235.59603         255.188871  275539.573105  104.048513   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11355    -2.209834     0.003371  0.0003  0.391322  0.019944  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11355          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11356  79499000.0  278257.592207         255.198651  275561.559428  99.183649   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11356    -2.187583     0.002966  0.0003  0.336168  0.020142  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11356          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11357  79506000.0  278279.936544          255.21043  275583.891889  80.005325   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11357     -2.20447     0.002075  0.0003  0.536328  0.018999  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11357          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "11358  79513000.0  278302.13627         255.220035  275606.081959  101.144768   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11358    -2.206831     0.003729  0.0003  0.924948  0.020253  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11358          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11359  79520000.0  278324.488728         255.227131  275628.427253  59.261907   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11359    -2.184505     0.002167  0.0003  0.588309  0.018817  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11359          1000.0     -5.503853  \n",
      "         timestep          time  collect_wait_time    train_time      score  \\\n",
      "11360  79527000.0  278346.46871         255.239684  275650.39463  81.243873   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11360    -2.198263     0.002908  0.0003  0.753155  0.020971  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11360          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11361  79534000.0  278369.325094         255.250761  275673.239847   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11361  100.372449     -2.20294     0.002268  0.0003  0.901249  0.019263   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11361  28.71883          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11362  79541000.0  278393.897151         255.261817  275697.800786  87.229192   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error    return  \\\n",
      "11362    -2.194899     0.002145  0.0003  0.864128  0.018217  28.71883   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11362          1000.0     -5.503853  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11363  79548000.0  278418.594734         255.279443  275722.480686   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11363  103.596106     -2.20839     0.004436  0.0003  1.322503  0.020379   \n",
      "\n",
      "         return  episode_length  eval_entropy  \n",
      "11363  28.71883          1000.0     -5.503853  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11364  79555000.0  278458.67047         255.289186  275745.871405  75.316258   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11364     -2.18099     0.004015  0.0003  0.900252  0.020834  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11364          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11365  79562000.0  278481.871277         255.299803  275769.061535   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11365  107.103519     -2.20118     0.002417  0.0003  0.948538  0.019041   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11365  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11366  79569000.0  278503.320611         255.306787  275790.503834   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11366  116.365932    -2.194816     0.001802  0.0003  0.806084  0.018553   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11366  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11367  79576000.0  278524.924644         255.317295  275812.097297  73.183551   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11367    -2.206542     0.002514  0.0003  1.522264  0.019303  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11367          1000.0     -6.894948  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "11368  79583000.0  278546.45947          255.32726  275833.622106  98.64711   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11368    -2.199474     0.002969  0.0003  1.035497  0.020495  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11368          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "11369  79590000.0  278567.941906          255.33527  275855.096467  75.1715   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11369     -2.19667      0.00246  0.0003  1.107392  0.019056  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11369          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11370  79597000.0  278590.166627         255.342748  275877.313648   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "11370  101.195529    -2.189054     0.003008  0.0003  1.47706  0.019327   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11370  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "11371  79604000.0  278613.662466         255.352322  275900.79983  93.90454   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11371    -2.194867     0.004138  0.0003  1.367434    0.0205  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11371          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11372  79611000.0  278635.369222         255.362036  275922.496809  84.212817   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11372    -2.188174     0.002499  0.0003  1.139373  0.019315  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11372          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11373  79618000.0  278657.013216         255.371891  275944.130901  85.294083   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11373     -2.19313     0.002754  0.0003  1.704331  0.019569  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11373          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11374  79625000.0  278678.818875         255.381915  275965.92648  103.717707   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11374    -2.197299     0.002946  0.0003  1.077838  0.019348  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11374          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11375  79632000.0  278700.551709          255.39322  275987.647954  95.716074   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11375    -2.184983     0.003216  0.0003  1.492169  0.019347  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11375          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11376  79639000.0  278722.260819         255.403557  276009.346676  86.818978   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11376    -2.207357     0.003174  0.0003  1.238718  0.019316  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11376          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11377  79646000.0  278743.154403         255.411138  276030.232633   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11377  103.311591    -2.186021     0.002376  0.0003  1.405158   0.01856   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11377  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11378  79653000.0  278765.029552         255.420717  276052.098144  80.454402   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11378    -2.190316       0.0022  0.0003  1.19005  0.018424  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11378          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11379  79660000.0  278786.792552         255.430173  276073.851635  97.131327   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11379    -2.206702     0.002684  0.0003  1.537143   0.01904  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11379          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11380  79667000.0  278808.200115         255.438673  276095.250649   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11380  101.862811    -2.192024     0.003134  0.0003  1.265471   0.01958   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11380  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time   train_time       score  \\\n",
      "11381  79674000.0  278829.924191         255.446641  276116.9667  100.447163   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11381    -2.209112     0.002561  0.0003  1.621015  0.018842  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11381          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11382  79681000.0  278851.236297         255.457417  276138.267971   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11382  112.081192    -2.201985     0.003527  0.0003  1.533541  0.020773   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11382  25.857162          1000.0     -6.894948  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11383  79688000.0  278872.76036         255.467422  276159.781928  99.531047   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11383    -2.205913      0.00218  0.0003  1.053913  0.019386  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11383          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11384  79695000.0  278894.224821          255.47753  276181.236228   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11384  103.063442    -2.199968     0.002512  0.0003  1.522228  0.018181   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11384  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11385  79702000.0  278915.572614         255.485256  276202.576246  97.050928   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11385    -2.204725     0.002476  0.0003  1.34904  0.018959  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11385          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11386  79709000.0  278937.605848          255.49475  276224.599939  92.398994   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11386    -2.210734     0.002402  0.0003  1.590991  0.018352  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11386          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11387  79716000.0  278959.714908         255.504412  276246.69928  92.037924   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11387    -2.198973     0.003341  0.0003  1.982916  0.019975  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11387          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11388  79723000.0  278981.980507         255.515042  276268.954199   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11388  100.879446    -2.202106     0.002356  0.0003  2.412646  0.019329   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11388  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11389  79730000.0  279005.289588         255.524347  276292.253926  114.89556   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11389    -2.194953     0.002575  0.0003  1.620863  0.018459  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11389          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11390  79737000.0  279030.342919         255.531653  276317.299898  82.328379   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11390     -2.19108     0.002377  0.0003  1.276408  0.019062  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11390          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11391  79744000.0  279053.557097         255.542288  276340.503385   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "11391  130.373627    -2.185566     0.003139  0.0003   1.5661  0.019047   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11391  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11392  79751000.0  279075.055623         255.552101  276361.992051  80.586948   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11392     -2.20626     0.003492  0.0003 -0.293832   0.01998  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11392          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11393  79758000.0  279097.181254         255.563898  276384.105827  87.060235   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11393    -2.202051      0.00285  0.0003  1.273147  0.019202  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11393          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11394  79765000.0  279119.179864         255.574213  276406.09406  103.219733   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11394    -2.203115     0.001726  0.0003  1.754793   0.01798  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11394          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11395  79772000.0  279141.062169         255.585034  276427.965488  86.774834   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11395    -2.199236     0.002339  0.0003  1.764516  0.018794  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11395          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11396  79779000.0  279163.237474         255.595412  276450.130359  89.066885   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11396     -2.20592     0.003142  0.0003  1.560344  0.019064  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11396          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11397  79786000.0  279185.092364          255.60465  276471.97596  78.770071   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11397    -2.201641     0.002796  0.0003  1.586327  0.019148  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11397          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11398  79793000.0  279206.727369         255.612916  276493.602654  81.26761   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11398    -2.213634     0.002723  0.0003  2.054696  0.018995  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11398          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11399  79800000.0  279228.341531         255.622694  276515.206985  69.134653   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11399    -2.191663     0.003261  0.0003  1.062301  0.019251  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11399          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11400  79807000.0  279250.079876         255.632949  276536.935019   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11400  101.336986    -2.201323     0.002866  0.0003  1.232445  0.019013   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11400  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11401  79814000.0  279272.007508         255.643215  276558.852317  75.927384   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11401    -2.218322     0.002593  0.0003  1.373059  0.018735  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11401          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11402  79821000.0  279293.739202         255.654243  276580.572922  82.30748   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11402    -2.210271     0.003289  0.0003  1.262899  0.019583  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11402          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "11403  79828000.0  279315.333338         255.664349  276602.156864  88.2487   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11403    -2.203321     0.002175  0.0003  1.870473  0.018246  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11403          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11404  79835000.0  279336.887414         255.674579  276623.70063  80.334954   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11404    -2.206795     0.002689  0.0003  1.499774  0.018368  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11404          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11405  79842000.0  279358.069047         255.682931  276644.87386  96.776597   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11405    -2.196749     0.002201  0.0003  1.202758  0.018486  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11405          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11406  79849000.0  279379.488392         255.689705  276666.286383  93.069077   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11406    -2.196265     0.002607  0.0003  1.469109  0.018731  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11406          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11407  79856000.0  279401.824925         255.700633  276688.611924   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11407  102.040902    -2.193136     0.002938  0.0003  2.127415  0.018903   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11407  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11408  79863000.0  279427.059756         255.711029  276713.836294  96.618459   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11408    -2.199503     0.002292  0.0003  1.733237   0.01897  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11408          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11409  79870000.0  279449.145957         255.721651  276735.911802   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11409  112.080239    -2.208202     0.003173  0.0003  1.616321  0.019399   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11409  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11410  79877000.0  279470.771733         255.731856  276757.527308  90.602115   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11410    -2.216104      0.00309  0.0003   1.7094  0.019606  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11410          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11411  79884000.0  279492.573542         255.743037  276779.317882  60.878038   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11411    -2.190407     0.003036  0.0003  1.785868  0.019813  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11411          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11412  79891000.0  279514.298979         255.753013  276801.033296  82.854509   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11412    -2.180691     0.001986  0.0003  1.977101  0.018192  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11412          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11413  79898000.0  279536.050987          255.76563  276822.772623  97.78031   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11413    -2.189728     0.003994  0.0003  1.837118  0.019898  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11413          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11414  79905000.0  279557.524443         255.775114  276844.236534  83.864167   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11414     -2.18886     0.003642  0.0003  1.438168  0.019525  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11414          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11415  79912000.0  279579.242708         255.784794  276865.945031  93.086548   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11415    -2.199793     0.002249  0.0003  1.407661  0.019537  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11415          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11416  79919000.0  279600.944684         255.795361  276887.636389  96.630231   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11416    -2.207771     0.002953  0.0003  0.872478  0.019456  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11416          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11417  79926000.0  279622.932213          255.80254  276909.616693  74.883871   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11417    -2.199713     0.003291  0.0003  1.401924  0.018298  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11417          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11418  79933000.0  279644.614729         255.812604  276931.289089  92.498235   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11418     -2.21115     0.003632  0.0003  1.474551  0.019447  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11418          1000.0     -6.894948  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "11419  79940000.0  279666.4332         255.822641  276953.097474  101.84741   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11419    -2.193716     0.001921  0.0003  0.873788  0.018046  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11419          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11420  79947000.0  279688.368333         255.830879  276975.024314  97.928241   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11420    -2.193549     0.002111  0.0003  1.584192  0.019243  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11420          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11421  79954000.0  279710.739126         255.837895  276997.388028  89.919195   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11421    -2.210978     0.003151  0.0003  1.40714  0.020473  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11421          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11422  79961000.0  279732.712795         255.847931  277019.351602  105.72356   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11422    -2.199397     0.002805  0.0003  1.668582  0.019133  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11422          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11423  79968000.0  279754.581887         255.857735  277041.210838  96.434437   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11423    -2.201581     0.002727  0.0003  1.664471  0.019269  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11423          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11424  79975000.0  279776.579879          255.86596  277063.200558  84.892735   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11424    -2.194857     0.002069  0.0003  2.104847  0.018727  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11424          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11425  79982000.0  279799.150774         255.873263  277085.764102  94.994627   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11425    -2.200597     0.003117  0.0003  2.153594  0.019224  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11425          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11426  79989000.0  279822.000838         255.883346  277108.604023  97.863868   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11426    -2.206816     0.003706  0.0003  1.473378  0.020016  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11426          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11427  79996000.0  279844.255796          255.89319  277130.849081   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11427  100.002438     -2.20841      0.00213  0.0003  1.476816  0.018125   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11427  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11428  80003000.0  279865.832431         255.901638  277152.417217  89.10124   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11428     -2.19946     0.002524  0.0003  1.397076  0.019358  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11428          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11429  80010000.0  279887.921512          255.91053  277174.49735  92.920668   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11429    -2.176706     0.002548  0.0003  1.904361  0.018777  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11429          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11430  80017000.0  279909.473573         255.922059  277196.03782  64.273395   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11430    -2.216786     0.001724  0.0003  1.875401  0.018193  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11430          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11431  80024000.0  279931.296003         255.933335  277217.848915   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11431  104.903698    -2.217543      0.00254  0.0003  2.191172  0.018532   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11431  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time   train_time      score  \\\n",
      "11432  80031000.0  279952.810188         255.941889  277239.3545  93.593113   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11432    -2.218478     0.002385  0.0003  2.364043  0.018267  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11432          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11433  80038000.0  279974.351536         255.950486  277260.887202  93.830481   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11433    -2.196055     0.004241  0.0003  1.594552  0.020998  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11433          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11434  80045000.0  279995.418385         255.960352  277281.944129  97.482666   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11434    -2.194354     0.002918  0.0003  1.402658  0.019023  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11434          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11435  80052000.0  280016.944847         255.969396  277303.461494  82.721315   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11435    -2.186946     0.003373  0.0003  1.655835  0.020326  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11435          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11436  80059000.0  280038.757791         255.977758  277325.265984  63.919477   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11436    -2.190245     0.002279  0.0003  1.862875   0.01826  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11436          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11437  80066000.0  280060.431498         255.989635  277346.927761  64.254999   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11437    -2.216531     0.003415  0.0003  1.558915  0.019369  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11437          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11438  80073000.0  280082.401102         255.999446  277368.887508  85.955165   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11438    -2.214333     0.002106  0.0003  1.340353  0.018908  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11438          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11439  80080000.0  280104.256767         256.007991  277390.73458  102.745637   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11439    -2.187407      0.00317  0.0003  1.682649  0.019267  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11439          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11440  80087000.0  280125.989137         256.015977  277412.458906   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11440  103.741109    -2.208879     0.002924  0.0003  1.379789  0.019513   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11440  25.857162          1000.0     -6.894948  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11441  80094000.0  280147.56278         256.027489  277434.020974  85.577175   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11441    -2.198711     0.003631  0.0003  1.76198  0.018869  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11441          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11442  80101000.0  280169.413882         256.038644  277455.86087  83.126061   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11442    -2.210988     0.003023  0.0003  1.382968  0.018977  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11442          1000.0     -6.894948  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11443  80108000.0  280191.64174         256.047336  277478.079977  78.273319   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11443    -2.190297     0.002305  0.0003  1.017831  0.018507  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11443          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11444  80115000.0  280213.628827         256.054913  277500.059427  74.069165   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11444    -2.197375     0.003274  0.0003  1.44898  0.019644  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11444          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11445  80122000.0  280235.388521          256.06561  277521.808364  62.864579   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11445    -2.212363     0.002599  0.0003  1.581864  0.018604  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11445          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11446  80129000.0  280257.301887         256.074687  277543.712601  72.975934   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11446    -2.194654     0.002542  0.0003  1.782778  0.018557  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11446          1000.0     -6.894948  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11447  80136000.0  280278.92863         256.081549  277565.332436  90.710551   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11447    -2.195014     0.002197  0.0003  1.642184  0.018319  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11447          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11448  80143000.0  280300.683414         256.090907  277587.07781  100.025982   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11448    -2.197826     0.003104  0.0003  1.594042   0.01923  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11448          1000.0     -6.894948  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11449  80150000.0  280323.81347         256.100272  277610.198018  90.331487   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11449    -2.208134      0.00225  0.0003  2.049216   0.01961  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11449          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11450  80157000.0  280349.059628         256.109234  277635.435097  69.057897   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11450    -2.181159     0.002467  0.0003  2.09748  0.019237  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11450          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11451  80164000.0  280370.472177         256.117454  277656.839368   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11451  104.455709    -2.187376     0.002559  0.0003  1.365523  0.019335   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11451  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11452  80171000.0  280392.130438         256.126959  277678.488066  62.145036   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11452    -2.205126     0.002036  0.0003  1.425917  0.018501  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11452          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11453  80178000.0  280414.115951         256.137037  277700.463442   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11453  107.213639    -2.207185     0.001965  0.0003  1.679552  0.018306   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11453  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11454  80185000.0  280436.240341         256.145394  277722.579417  98.699138   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11454    -2.188252      0.00279  0.0003  1.747806  0.018905  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11454          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11455  80192000.0  280457.776526         256.152158  277744.108793   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11455  115.652926    -2.186756     0.003472  0.0003  1.360427  0.019977   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11455  25.857162          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11456  80199000.0  280479.962824         256.162625  277766.284574  72.503418   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11456    -2.180723     0.002417  0.0003  2.269232  0.018252  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11456          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11457  80206000.0  280501.376018         256.170214  277787.690127  91.892682   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11457    -2.184082     0.002111  0.0003  1.847115  0.017704  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11457          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11458  80213000.0  280523.872505         256.177335  277810.179432  98.264933   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11458    -2.189149     0.002777  0.0003  1.963706  0.018378  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11458          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11459  80220000.0  280545.401177         256.187671  277831.697705  90.817913   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11459    -2.188815     0.003461  0.0003  1.910496  0.020223  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11459          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11460  80227000.0  280567.560429         256.199286  277853.845284  93.375452   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11460    -2.196112     0.003765  0.0003  1.300884  0.020484  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11460          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11461  80234000.0  280589.153477          256.20929  277875.428278  92.527568   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11461    -2.194565      0.00424  0.0003  1.698808  0.019941  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11461          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11462  80241000.0  280610.961168          256.21701  277897.228198  94.839662   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11462    -2.189555     0.002698  0.0003  1.96322  0.018374  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11462          1000.0     -6.894948  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11463  80248000.0  280632.666926         256.227767  277918.923141  88.854589   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11463    -2.183688     0.002547  0.0003  2.000403  0.018054  25.857162   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11463          1000.0     -6.894948  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11464  80255000.0  280670.969086         256.239143  277940.816287  68.327732   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11464     -2.18032     0.003746  0.0003  1.653959  0.020438  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11464          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11465  80262000.0  280695.003254         256.249595  277964.839904  90.955168   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11465    -2.194126     0.003339  0.0003  1.809499  0.018692  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11465          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11466  80269000.0  280719.348242         256.258307  277989.176126  92.616051   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11466    -2.191603     0.001994  0.0003  1.973671  0.018256  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11466          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11467  80276000.0  280743.635823         256.268454  278013.453491  96.85526   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11467    -2.184332      0.00228  0.0003  1.849943  0.018513  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11467          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11468  80283000.0  280768.131406         256.278679  278037.938788  78.489006   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11468    -2.209342     0.002845  0.0003  1.555892   0.01958  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11468          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11469  80290000.0  280793.495437         256.289452  278063.291989  85.331358   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11469    -2.198806     0.002492  0.0003  1.632037  0.018769  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11469          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11470  80297000.0  280815.712203         256.301828  278085.49632  103.697918   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11470    -2.204784     0.004797  0.0003  1.250885  0.021296  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11470          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11471  80304000.0  280837.707485         256.310935  278107.482435  93.376935   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11471     -2.18235     0.003269  0.0003  1.42982  0.019867  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11471          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11472  80311000.0  280859.194655         256.320439  278128.960047   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11472  106.593199    -2.196071     0.003402  0.0003  1.535694   0.01996   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11472  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11473  80318000.0  280881.142994         256.329746  278150.899027  71.396492   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11473    -2.198542     0.002209  0.0003  0.904527  0.019607  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11473          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11474  80325000.0  280902.860261         256.339396  278172.606591   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11474  104.012243    -2.189589     0.003315  0.0003  1.233772  0.020148   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11474  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11475  80332000.0  280924.922938         256.348364  278194.660251  73.919624   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11475    -2.185203      0.00355  0.0003  1.393553  0.019465  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11475          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11476  80339000.0  280946.811707         256.355806  278216.541531  86.407281   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11476    -2.209972       0.0032  0.0003  1.153864   0.01896  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11476          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11477  80346000.0  280969.115256         256.365486  278238.835346   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11477  117.883734    -2.191911     0.002437  0.0003  1.151969  0.018509   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11477  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11478  80353000.0  280990.941778         256.376266  278260.65104  99.697895   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11478    -2.189642     0.002753  0.0003  1.31989   0.01857  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11478          1000.0     -2.046506  \n",
      "         timestep          time  collect_wait_time     train_time    score  \\\n",
      "11479  80360000.0  281013.19822         256.385496  278282.898196  76.4575   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11479    -2.205723     0.003182  0.0003  1.030541  0.019042  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11479          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11480  80367000.0  281035.445264         256.393023  278305.13766  105.932474   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11480     -2.19779     0.001988  0.0003  0.838826  0.019177  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11480          1000.0     -2.046506  \n",
      "         timestep          time  collect_wait_time    train_time     score  \\\n",
      "11481  80374000.0  281057.39027         256.402228  278327.07341  81.86511   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11481    -2.200324     0.002629  0.0003  1.37513  0.019124  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11481          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11482  80381000.0  281079.574986         256.411824  278349.248473  93.364723   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11482     -2.19533     0.003057  0.0003  1.224658  0.019444  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11482          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11483  80388000.0  281101.972341         256.421878  278371.635709   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11483  101.184063    -2.178928     0.002305  0.0003  1.605731  0.018845   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11483  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11484  80395000.0  281123.897297         256.430001  278393.552491   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11484  101.402007    -2.218487     0.001925  0.0003  1.480176  0.017444   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11484  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11485  80402000.0  281148.277002         256.441289  278417.920765  102.85639   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11485    -2.185706     0.002442  0.0003  0.853131  0.019015  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11485          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11486  80409000.0  281173.134704         256.453829  278442.765868  90.712689   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11486     -2.20028     0.003388  0.0003  1.288041  0.018687  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11486          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11487  80416000.0  281197.392313         256.462117  278467.015129  94.506606   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11487    -2.211454      0.00221  0.0003  1.005978  0.019112  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11487          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11488  80423000.0  281219.875476         256.469677  278489.490681   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11488  113.188058    -2.199247     0.003133  0.0003  0.915361  0.019403   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11488  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11489  80430000.0  281241.673126         256.479541  278511.278401  78.763261   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11489    -2.197658     0.002471  0.0003  1.404039  0.019201  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11489          1000.0     -2.046506  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11490  80437000.0  281263.26851         256.487739  278532.865489  79.480618   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11490     -2.20291     0.003325  0.0003  1.602172  0.019184  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11490          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11491  80444000.0  281284.657181         256.496203  278554.24565  104.847632   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11491    -2.207879     0.003236  0.0003  1.556266  0.019028  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11491          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11492  80451000.0  281305.832707         256.505481  278575.411847   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11492  108.082324     -2.20319     0.003585  0.0003  1.363659  0.019205   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11492  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11493  80458000.0  281327.414799          256.51535  278596.984019  88.442713   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11493    -2.198551     0.002821  0.0003  1.288717  0.018979  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11493          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11494  80465000.0  281349.157667         256.525511  278618.716676  95.767796   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11494    -2.217029      0.00276  0.0003  1.547676  0.018395  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11494          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11495  80472000.0  281370.695591          256.53561  278640.244452  85.556023   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11495    -2.209947     0.002754  0.0003  1.264855  0.018712  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11495          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11496  80479000.0  281392.440726         256.544023  278661.981123  66.348121   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11496     -2.21009      0.00352  0.0003  1.327819  0.019754  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11496          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11497  80486000.0  281414.010431         256.550895  278683.543902  83.323367   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11497    -2.196717     0.002417  0.0003  1.522047  0.018297  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11497          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11498  80493000.0  281435.629912         256.561161  278705.153065  78.152448   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11498     -2.20152     0.003031  0.0003  1.093369  0.019083  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11498          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11499  80500000.0  281456.854285         256.570887  278726.367648  80.340743   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11499    -2.212693     0.002251  0.0003  0.97692  0.018037  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11499          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11500  80507000.0  281478.552078          256.58058  278748.055692  80.099054   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11500     -2.19831     0.002444  0.0003  0.110711  0.018812  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11500          1000.0     -2.046506  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "11501  80514000.0  281500.11466         256.588555  278769.610245  100.545518   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11501    -2.195281     0.003006  0.0003  0.628136  0.019658  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11501          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11502  80521000.0  281521.869269         256.598512  278791.354847  64.419314   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11502    -2.209327      0.00248  0.0003  0.723729  0.018059  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11502          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11503  80528000.0  281543.362822         256.608783  278812.838082  89.122184   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11503    -2.200224     0.004483  0.0003  1.602629  0.020604  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11503          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11504  80535000.0  281565.461936         256.616972  278834.928908   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11504  107.677758    -2.197891     0.002515  0.0003  1.477839  0.018498   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11504  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11505  80542000.0  281587.399766         256.626524  278856.857129   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11505  100.088686    -2.213978     0.002407  0.0003  1.612161  0.018927   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11505  18.903261          1000.0     -2.046506  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "11506  80549000.0  281609.25138         256.636536  278878.698658  105.150014   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11506    -2.208697     0.002722  0.0003  1.574022  0.018765  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11506          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11507  80556000.0  281631.009954         256.647264  278900.446445   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11507  108.650737    -2.192223     0.003224  0.0003  0.579823  0.019029   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11507  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11508  80563000.0  281654.703895         256.656662  278924.130933  92.889205   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11508    -2.193996     0.001914  0.0003  0.557389  0.018152  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11508          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11509  80570000.0  281678.883132         256.664235  278948.302533  91.443099   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11509    -2.221908     0.001958  0.0003  1.487261  0.018103  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11509          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11510  80577000.0  281702.688183         256.674475  278972.097284  89.867301   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11510    -2.214868     0.002058  0.0003  1.247932  0.017735  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11510          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11511  80584000.0  281726.354767         256.683206  278995.755083  76.20097   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11511    -2.210084     0.003729  0.0003  0.979215   0.02064  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11511          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11512  80591000.0  281750.156201         256.691524  279019.548144  84.778391   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11512    -2.197121     0.002822  0.0003  0.721459  0.019438  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11512          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11513  80598000.0  281774.723815         256.701174  279044.106055  98.119549   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11513    -2.216284     0.002855  0.0003  0.798923  0.018629  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11513          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11514  80605000.0  281798.881933         256.711241  279068.254045  96.497059   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11514    -2.210681     0.003802  0.0003  1.291241  0.019809  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11514          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11515  80612000.0  281823.844715         256.720366  279093.207644  99.403962   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11515    -2.196722     0.002557  0.0003  0.933645  0.018773  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11515          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11516  80619000.0  281847.767008         256.728244  279117.121993  99.003099   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11516    -2.203469     0.003268  0.0003  1.08841  0.018814  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11516          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11517  80626000.0  281871.685758         256.740207  279141.028701  82.603268   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11517     -2.21409     0.002344  0.0003  1.096443   0.01871  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11517          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11518  80633000.0  281896.485726         256.749617  279165.819206  92.818685   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11518     -2.19592     0.003205  0.0003  1.496104  0.019575  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11518          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11519  80640000.0  281919.952693         256.757729  279189.278013  95.265903   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11519    -2.207683     0.001828  0.0003  1.200462  0.017537  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11519          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11520  80647000.0  281943.841006         256.767675  279213.156293   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11520  105.415158    -2.213017     0.003165  0.0003 -0.469361  0.019754   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11520  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11521  80654000.0  281968.149938         256.780041  279237.452772  93.339339   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11521    -2.207118     0.003297  0.0003  0.800319  0.019306  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11521          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11522  80661000.0  281990.668035         256.788932  279259.961897  68.15742   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11522    -2.223525     0.003316  0.0003  1.360059   0.01986  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11522          1000.0     -2.046506  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "11523  80668000.0  282011.34977         256.797266  279280.635229  83.57992   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11523    -2.221634     0.002052  0.0003  0.843516  0.018369  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11523          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11524  80675000.0  282032.995663         256.808018  279302.270311  96.44594   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11524    -2.215979     0.002881  0.0003  1.10954  0.019292  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11524          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11525  80682000.0  282054.814381         256.818114  279324.078832  93.819447   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11525    -2.203981     0.002808  0.0003  0.979988    0.0191  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11525          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11526  80689000.0  282076.345408          256.83632  279345.591592  110.8317   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11526    -2.202629     0.002736  0.0003  1.147854  0.019197  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11526          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11527  80696000.0  282098.336926         256.848936  279367.570444  83.495066   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11527    -2.215947     0.002512  0.0003  0.60836   0.01913  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11527          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11528  80703000.0  282119.713063          256.85963  279388.935829  85.106343   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11528    -2.216728     0.004148  0.0003  0.722698  0.020224  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11528          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11529  80710000.0  282141.129805         256.871176  279410.340966  74.269541   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11529    -2.204967     0.002585  0.0003  0.722347  0.019064  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11529          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11530  80717000.0  282162.533008         256.879187  279431.736109  91.781601   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11530    -2.215354     0.003668  0.0003  0.516186  0.019904  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11530          1000.0     -2.046506  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11531  80724000.0  282184.50995         256.889183  279453.703007  93.117386   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11531     -2.19874     0.003161  0.0003  1.312842  0.019266  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11531          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11532  80731000.0  282209.152153         256.899484  279478.334848  95.237568   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11532    -2.226499     0.002429  0.0003  1.031076  0.019108  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11532          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11533  80738000.0  282233.628004         256.909752  279502.800367  85.292876   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11533    -2.214113     0.002819  0.0003  1.197536  0.018883  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11533          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11534  80745000.0  282257.381705         256.919003  279526.544754  102.70682   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11534    -2.221994     0.002889  0.0003  0.662011  0.019609  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11534          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11535  80752000.0  282281.616946         256.926907  279550.772038  90.15549   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11535     -2.21504     0.002771  0.0003  1.009266  0.018832  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11535          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11536  80759000.0  282305.348782         256.936994  279574.493717  95.451884   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11536    -2.221214     0.002606  0.0003  1.772206  0.018796  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11536          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11537  80766000.0  282329.525617         256.947503  279598.659989  83.399691   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11537     -2.19695     0.003444  0.0003  1.542097  0.019542  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11537          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11538  80773000.0  282354.074187         256.958197  279623.19776  87.284558   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11538    -2.210119     0.002135  0.0003  1.648898   0.01858  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11538          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11539  80780000.0  282378.368274         256.967929  279647.482032  90.15429   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11539    -2.218912     0.002952  0.0003  1.19055  0.019962  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11539          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11540  80787000.0  282402.010381         256.977452  279671.114507  77.116199   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11540    -2.203501     0.003714  0.0003  1.892633  0.019604  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11540          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11541  80794000.0  282424.730268         256.991023  279693.820764  87.090716   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11541    -2.224866     0.003329  0.0003  1.215245  0.019841  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11541          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11542  80801000.0  282448.732223         257.001167  279717.812513   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11542  103.286199    -2.198666      0.00274  0.0003  0.655634  0.019141   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11542  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11543  80808000.0  282472.677005         257.010862  279741.747549   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11543  105.349787    -2.207594      0.00281  0.0003  1.420356  0.018732   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11543  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11544  80815000.0  282496.830286         257.019463  279765.892174  83.618201   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11544    -2.198923     0.003653  0.0003  1.282963  0.020211  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11544          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11545  80822000.0  282520.730801         257.029735  279789.78236  92.376761   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11545     -2.22126     0.002623  0.0003  1.548888   0.01915  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11545          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11546  80829000.0  282544.893459         257.041456  279813.933238  90.943582   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11546    -2.212704     0.003005  0.0003  1.149122  0.019074  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11546          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11547  80836000.0  282569.196233         257.055027  279838.222377  96.550763   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11547    -2.207983     0.002714  0.0003  1.102434  0.019305  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11547          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11548  80843000.0  282593.328295         257.063442  279862.345965  89.254119   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11548    -2.211833     0.002699  0.0003  1.642816   0.01855  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11548          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11549  80850000.0  282617.509302         257.073642  279886.516717  93.735814   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11549    -2.198265     0.003535  0.0003  1.852015  0.019492  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11549          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11550  80857000.0  282641.845225         257.084296  279910.841931  66.960104   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11550    -2.214072     0.003403  0.0003  1.60256  0.019994  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11550          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11551  80864000.0  282665.648542         257.093497  279934.635995   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11551  101.019569    -2.202255     0.003706  0.0003  1.383794  0.020011   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11551  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11552  80871000.0  282689.198172         257.101026  279958.178005  84.775791   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11552    -2.198855     0.002713  0.0003  0.758804  0.019252  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11552          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11553  80878000.0  282710.490685          257.11206  279979.45943  66.474877   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11553     -2.19923     0.003223  0.0003  1.463948  0.018338  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11553          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11554  80885000.0  282731.559189         257.120406  280000.519538  79.619728   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11554    -2.215395     0.003526  0.0003  1.451561  0.019868  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11554          1000.0     -2.046506  \n",
      "         timestep         time  collect_wait_time     train_time      score  \\\n",
      "11555  80892000.0  282752.7343          257.12786  280021.687142  71.925163   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11555    -2.209085     0.001864  0.0003  1.203312  0.018426  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11555          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11556  80899000.0  282774.257051         257.137564  280043.20013  89.878939   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11556    -2.234702     0.002918  0.0003  1.559249    0.0191  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11556          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11557  80906000.0  282795.831054         257.147728  280064.763921  72.708674   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11557     -2.20894     0.003663  0.0003  1.69962   0.01928  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11557          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time   train_time     score  \\\n",
      "11558  80913000.0  282817.573357         257.157199  280086.4967  71.02602   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11558    -2.214612     0.003556  0.0003  1.142564  0.019964  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11558          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11559  80920000.0  282839.385744         257.164215  280108.302024   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11559  100.746527     -2.20578     0.002672  0.0003  1.789564   0.01868   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11559  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11560  80927000.0  282860.879189         257.174454  280129.785166   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11560  100.597776    -2.208575     0.002844  0.0003  1.671983  0.019216   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11560  18.903261          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11561  80934000.0  282882.482684         257.183045  280151.38001  69.552153   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11561    -2.208016     0.002609  0.0003  1.985606  0.018231  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11561          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11562  80941000.0  282904.724943         257.191183  280173.614073  80.704421   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11562     -2.21997     0.001864  0.0003  1.801747   0.01845  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11562          1000.0     -2.046506  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11563  80948000.0  282926.549617         257.201294  280195.428591  73.208235   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11563    -2.220534     0.003338  0.0003  1.596979  0.020027  18.903261   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11563          1000.0     -2.046506  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11564  80955000.0  282965.298115         257.211083  280217.538878  92.88881   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11564    -2.199539     0.002727  0.0003  1.743903  0.018916  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11564          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11565  80962000.0  282989.934886          257.22054  280242.166131  104.19035   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11565    -2.193535     0.003351  0.0003  1.750873  0.019052  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11565          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11566  80969000.0  283015.460812         257.228381  280267.684164  90.248242   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11566    -2.192803     0.003278  0.0003  1.044346   0.01893  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11566          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11567  80976000.0  283039.179963          257.23846  280291.393174  72.669864   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11567    -2.206432     0.002844  0.0003 -0.568409  0.019351  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11567          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11568  80983000.0  283060.755995         257.248323  280312.959284  86.536959   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11568    -2.188358     0.002487  0.0003  1.975379  0.018836  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11568          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11569  80990000.0  283081.951798         257.257954  280334.145401  78.742504   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11569     -2.19934     0.002991  0.0003  1.283456  0.019343  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11569          1000.0     -2.065011  \n",
      "         timestep          time  collect_wait_time     train_time       score  \\\n",
      "11570  80997000.0  283103.22715         257.266498  280355.412156  103.219301   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11570     -2.20268      0.00327  0.0003  1.429184  0.019446  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11570          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11571  81004000.0  283124.479516         257.276866  280376.654104  75.278588   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11571    -2.183463     0.003076  0.0003  1.349803  0.019587  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11571          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11572  81011000.0  283145.906063         257.288224  280398.06924  81.986025   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11572    -2.184675     0.002159  0.0003  1.384719   0.01831  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11572          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11573  81018000.0  283167.430538         257.296653  280419.585212  83.786801   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11573    -2.202936     0.002287  0.0003  0.900347  0.018273  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11573          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11574  81025000.0  283189.024187          257.30539  280441.170065  93.433544   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11574    -2.197307     0.001731  0.0003  0.817135  0.017889  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11574          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11575  81032000.0  283210.266416         257.316336  280462.401297   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11575  106.196009    -2.172533     0.002837  0.0003  1.214716   0.01881   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11575  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11576  81039000.0  283231.539013          257.32477  280483.665398  78.416698   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11576    -2.201406     0.002189  0.0003  1.72776  0.019157  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11576          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11577  81046000.0  283252.798872         257.332108  280504.91786  96.337661   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11577    -2.204583     0.002378  0.0003  1.614743  0.019092  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11577          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11578  81053000.0  283274.106138         257.342433  280526.214744   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha  entropy  td_error  \\\n",
      "11578  101.401769    -2.195181      0.00233  0.0003  1.07576  0.018966   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11578  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time     score  \\\n",
      "11579  81060000.0  283295.414923         257.353098  280547.51281  81.48893   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11579     -2.20924     0.002339  0.0003  1.28312  0.019125  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11579          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11580  81067000.0  283317.975348         257.362879  280570.063401  83.128922   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11580    -2.182644     0.003816  0.0003  1.097981  0.020986  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11580          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11581  81074000.0  283342.954516         257.370834  280595.034533  104.12237   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11581    -2.181901     0.002925  0.0003  1.086241  0.018984  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11581          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11582  81081000.0  283367.485461          257.38222  280619.554061   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11582  119.912952    -2.190801     0.002387  0.0003  1.200258  0.018681   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11582  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11583  81088000.0  283392.371157         257.395229  280644.42669  96.929915   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11583    -2.198753     0.002027  0.0003  1.65154  0.018298  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11583          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11584  81095000.0  283416.448301         257.404006  280668.495004  103.52549   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11584    -2.193092     0.003283  0.0003  1.853895  0.019937  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11584          1000.0     -2.065011  \n",
      "         timestep          time  collect_wait_time     train_time     score  \\\n",
      "11585  81102000.0  283440.40365          257.41175  280692.442559  79.75556   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11585    -2.197291     0.003621  0.0003  1.518016  0.020121  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11585          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11586  81109000.0  283464.294229         257.421755  280716.323072  77.941425   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11586    -2.195094     0.003028  0.0003  1.520439  0.019334  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11586          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11587  81116000.0  283488.072299         257.431371  280740.091471  91.548465   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11587    -2.200944     0.002177  0.0003  1.315236  0.017899  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11587          1000.0     -2.065011  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11588  81123000.0  283512.51019         257.439923  280764.520737  79.045586   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11588    -2.182907     0.003553  0.0003  1.239355  0.019246  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11588          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11589  81130000.0  283533.772072         257.449215  280785.77327  77.445954   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11589    -2.194872     0.002789  0.0003  1.456315  0.019002  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11589          1000.0     -2.065011  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11590  81137000.0  283554.68143         257.460359  280806.671433  87.896518   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11590    -2.192344     0.002446  0.0003  1.550392  0.018778  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11590          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11591  81144000.0  283575.668796         257.468786  280827.650317  92.945926   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11591    -2.184341     0.002645  0.0003  1.367779   0.01875  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11591          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11592  81151000.0  283597.190024         257.475821  280849.164456  81.45465   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11592    -2.198097     0.002396  0.0003  2.122806  0.018091  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11592          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11593  81158000.0  283618.920954         257.484386  280870.88677  57.086382   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11593    -2.192882     0.003563  0.0003  1.815334  0.019328  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11593          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11594  81165000.0  283640.575576         257.493453  280892.532207  84.988579   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11594    -2.190472     0.003737  0.0003  1.172941  0.020177  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11594          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11595  81172000.0  283662.604466         257.508386  280914.546112   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11595  102.667026    -2.202847     0.002442  0.0003  1.661474   0.01862   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11595  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11596  81179000.0  283684.324918          257.51936  280936.255539  83.947271   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11596    -2.177161      0.00186  0.0003  2.287345  0.017629  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11596          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11597  81186000.0  283706.061192         257.530782  280957.980334  87.182119   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11597    -2.197344     0.002549  0.0003  2.047771  0.018401  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11597          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11598  81193000.0  283727.583391         257.541605  280979.491655   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11598  107.010774    -2.203522     0.002398  0.0003  1.971899  0.018721   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11598  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11599  81200000.0  283749.383128         257.550493  281001.282452  88.309579   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11599    -2.182411     0.002345  0.0003  1.669685  0.018302  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11599          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11600  81207000.0  283770.422409         257.557724  281022.314449   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11600  103.141524    -2.182895     0.003144  0.0003  2.336036  0.019273   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11600  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11601  81214000.0  283791.886502          257.56858  281043.767633   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11601  105.945453    -2.198742     0.003566  0.0003  2.038551  0.020388   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11601  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time       score  \\\n",
      "11602  81221000.0  283812.986173         257.579338  281064.85645  100.476347   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11602    -2.199614      0.00275  0.0003  1.966117  0.018569  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11602          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11603  81228000.0  283834.793006          257.58914  281086.653422   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11603  103.234075    -2.207956     0.001954  0.0003  1.417405  0.018798   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11603  21.948324          1000.0     -2.065011  \n",
      "         timestep          time  collect_wait_time     train_time    score  \\\n",
      "11604  81235000.0  283858.65293         257.603687  281110.498742  93.6177   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11604    -2.181972     0.002644  0.0003  1.77378  0.019898  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11604          1000.0     -2.065011  \n",
      "         timestep          time  collect_wait_time     train_time      score  \\\n",
      "11605  81242000.0  283882.45335         257.616049  281134.286752  64.598568   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11605    -2.207609     0.003093  0.0003  1.56073  0.018908  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11605          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11606  81249000.0  283905.830816         257.629736  281157.650448  74.808098   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11606    -2.202879     0.002824  0.0003  1.54193  0.020001  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11606          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11607  81256000.0  283930.249878          257.64524  281182.053938   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11607  102.935128    -2.207688     0.002643  0.0003  1.701626   0.01897   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11607  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11608  81263000.0  283954.126148         257.655678  281205.919707  78.787521   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11608    -2.196597     0.002488  0.0003  1.220517  0.018553  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11608          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11609  81270000.0  283978.244557         257.664861  281230.028858  86.459935   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11609    -2.196012     0.002877  0.0003  1.491078  0.019302  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11609          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11610  81277000.0  284002.922138         257.676768  281254.69448  91.341324   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11610    -2.194525     0.003054  0.0003  1.364639  0.019861  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11610          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11611  81284000.0  284027.686087         257.686689  281279.448414  77.959187   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11611    -2.191762     0.002794  0.0003  1.620413  0.019403  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11611          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11612  81291000.0  284052.102115         257.701357  281303.849679  62.925845   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11612    -2.198519     0.004009  0.0003  1.781945  0.019559  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11612          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11613  81298000.0  284073.957781         257.710413  281325.696237  95.494993   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11613    -2.199457     0.001911  0.0003  1.480977  0.018446  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11613          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11614  81305000.0  284095.862002         257.722651  281347.58816  69.246911   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11614    -2.198825     0.002253  0.0003  1.489509  0.018489  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11614          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11615  81312000.0  284117.480282          257.73583  281369.193176  62.872663   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11615    -2.209158     0.002833  0.0003  1.758276  0.019024  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11615          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time    score  \\\n",
      "11616  81319000.0  284139.065571         257.745867  281390.768369  74.9254   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11616    -2.190796     0.003281  0.0003  1.408348  0.020154  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11616          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11617  81326000.0  284160.948544         257.755744  281412.641405  82.961135   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha  entropy  td_error     return  \\\n",
      "11617    -2.173212     0.002261  0.0003  1.57837  0.019331  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11617          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time    train_time      score  \\\n",
      "11618  81333000.0  284182.618674         257.766816  281434.30038  67.684293   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11618     -2.18924     0.002544  0.0003  1.916134  0.019144  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11618          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11619  81340000.0  284204.445226          257.77862  281456.115077  93.130708   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11619    -2.190009     0.003322  0.0003  1.667521  0.019886  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11619          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11620  81347000.0  284226.189771          257.78904  281477.849147  76.225571   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11620    -2.193484     0.003345  0.0003  0.898398  0.019897  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11620          1000.0     -2.065011  \n",
      "         timestep          time  collect_wait_time    train_time      score  \\\n",
      "11621  81354000.0  284248.26252         257.797626  281499.91326  89.409061   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11621     -2.17053     0.002892  0.0003  1.158004  0.019976  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11621          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11622  81361000.0  284271.912762         257.805228  281523.555841  67.414738   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11622     -2.17027     0.003439  0.0003  1.294664   0.02008  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11622          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time  \\\n",
      "11623  81368000.0  284295.067372         257.815891  281546.699737   \n",
      "\n",
      "            score  policy_loss  qvalue_loss   alpha   entropy  td_error  \\\n",
      "11623  109.405413    -2.199946     0.001602  0.0003  1.576806  0.017436   \n",
      "\n",
      "          return  episode_length  eval_entropy  \n",
      "11623  21.948324          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time     score  \\\n",
      "11624  81375000.0  284316.425084         257.824585  281568.048702  81.02873   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11624    -2.167656     0.002286  0.0003  1.672922  0.019458  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11624          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11625  81382000.0  284337.932591         257.832554  281589.548186  98.037563   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11625    -2.186904     0.002683  0.0003  1.527348  0.019102  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11625          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11626  81389000.0  284358.745696         257.841912  281610.351884  90.760395   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11626    -2.183146     0.001947  0.0003  1.929421  0.018334  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11626          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11627  81396000.0  284380.087448         257.853094  281631.682403  99.416047   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11627    -2.181148     0.003236  0.0003  1.484429  0.019619  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11627          1000.0     -2.065011  \n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11628  81403000.0  284402.080174         257.861943  281653.666202  87.533332   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11628    -2.206431     0.003704  0.0003  1.491507  0.019448  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11628          1000.0     -2.065011  \n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11629  81410000.0  284423.756204         257.870769  281675.333347  72.952025   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11629    -2.194317      0.00293  0.0003  1.685507  0.018652  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11629          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11630  81417000.0  284445.245316         257.882026  281696.811146  79.499386   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11630    -2.196594     0.001938  0.0003  1.812465  0.018162  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11630          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11631  81424000.0  284466.386223         257.890989  281717.943007  79.732969   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11631     -2.17905     0.002735  0.0003  1.501778  0.019082  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11631          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11632  81431000.0  284488.131311         257.899957  281739.679063  81.911258   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11632    -2.180998     0.003397  0.0003  1.488327   0.01996  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11632          1000.0     -2.065011  \n",
      "         timestep           time  collect_wait_time     train_time      score  \\\n",
      "11633  81438000.0  284509.304294         257.907494  281760.844456  82.344577   \n",
      "\n",
      "       policy_loss  qvalue_loss   alpha   entropy  td_error     return  \\\n",
      "11633    -2.185447     0.003282  0.0003  2.004758  0.019756  21.948324   \n",
      "\n",
      "       episode_length  eval_entropy  \n",
      "11633          1000.0     -2.065011  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Converting a tensordict to boolean value is not permitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/utils.py:532\u001b[0m, in \u001b[0;36m_vmap_func.<locals>.decorated_module\u001b[0;34m(*module_args_params)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     r \u001b[39m=\u001b[39m module(\u001b[39m*\u001b[39;49mmodule_args)\n\u001b[1;32m    533\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1772\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py:328\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m _self \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mreturn\u001b[39;00m func(tensordict, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/utils.py:372\u001b[0m, in \u001b[0;36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     result \u001b[39m=\u001b[39m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    373\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py:1191\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(tensors_out, (\u001b[39mdict\u001b[39;49m, TensorDictBase)) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m   1192\u001b[0m     key \u001b[39min\u001b[39;00m tensors_out \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_keys\n\u001b[1;32m   1193\u001b[0m ):\n\u001b[1;32m   1194\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors_out, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/abc.py:117\u001b[0m, in \u001b[0;36mABCMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m _abc_register(\u001b[39mcls\u001b[39m, subclass)\n\u001b[0;32m--> 117\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__instancecheck__\u001b[39m(\u001b[39mcls\u001b[39m, instance):\n\u001b[1;32m    118\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l_/gtffst3j133f47568b_fd9zr0000gn/T/ipykernel_31067/1907665846.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# 1. SAC Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss_actor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss_qvalue\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss_alpha\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# 2. Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1769\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1905\u001b[0m                         warnings.warn(\"module forward hook with ``always_call=True`` raised an exception \"\n\u001b[1;32m   1906\u001b[0m                                       f\"that was silenced as another error was raised in forward: {str(e)}\")\n\u001b[1;32m   1907\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m             \u001b[0;31m# raise exception raised in try block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1823\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfull_backward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbackward_pre_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m                 \u001b[0mbw_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBackwardHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_pre_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1827\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1828\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1829\u001b[0m                 for hook_id, hook in (\n\u001b[1;32m   1830\u001b[0m                     \u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/common.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_self\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensordict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/sac.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0mloss_qvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qvalue_v2_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m         \u001b[0mloss_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_actor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         \u001b[0mloss_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_actor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_prob\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mtensordict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpriority\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"td_error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         if (loss_actor.shape != loss_qvalue.shape) or (\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/sac.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_reparm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mtd_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensordict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqvalue_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mtd_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_reparm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         td_q = self._vmap_qnetworkN0(\n\u001b[0m\u001b[1;32m    677\u001b[0m             \u001b[0mtd_q\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_detached_qvalue_params\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# should we clone?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         )\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/_functorch/apis.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         return vmap_impl(\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         )\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# If chunk_size is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     return _flat_vmap(\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mflat_in_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/_functorch/vmap.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mvmap_increment_nesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomness\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         batched_inputs = _create_batched_inputs(\n\u001b[1;32m    482\u001b[0m             \u001b[0mflat_in_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         )\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mbatched_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unwrap_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmap_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*module_args_params)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorated_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodule_args_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_args_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mmodule_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_args_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodule_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Converting a tensordict to boolean value is not permitted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Converting a tensordict to boolean value is not permitted"
     ]
    }
   ],
   "source": [
    "try: eval_env.close()\n",
    "except: pass\n",
    "eval_env = create_env(graphics=False, time_scale=TIME_SCALE)\n",
    "short_watch.start(); long_watch.start()\n",
    "for i, data in enumerate(collector):\n",
    "    # 1. Extend\n",
    "    replay_buffer.extend(data)\n",
    "    logger.add({\"collect_wait_time\": short_watch.end(), \"timestep\": ENV_STEPS})\n",
    "    avg_reward = data[\"next\", \"reward\"].mean().cpu().item()\n",
    "    score = (avg_reward / BENCH_AVG_REWARD) * 100\n",
    "    logger.acc({\"score\": score})\n",
    "\n",
    "    # 2. Train Loop\n",
    "    short_watch.start()\n",
    "    for j in range(TRAIN_STEPS):\n",
    "        batch = replay_buffer.sample().to(device)\n",
    "\n",
    "        # 1. SAC Loss\n",
    "        loss_data = loss_module(batch)\n",
    "        loss = loss_data[\"loss_actor\"] + loss_data[\"loss_qvalue\"] + loss_data[\"loss_alpha\"]\n",
    "        \n",
    "        # 2. Update\n",
    "        optimizer.zero_grad(); loss.backward()\n",
    "        optimizer.step(); target_updater.step()\n",
    "\n",
    "        # 3. Log Metrics\n",
    "        logger.acc({\n",
    "            \"policy_loss\":  loss_data[\"loss_actor\"].detach().cpu().item(),\n",
    "            \"qvalue_loss\":  loss_data[\"loss_qvalue\"].detach().cpu().item(),\n",
    "            \"entropy\":      loss_data[\"entropy\"].detach().cpu().item(),\n",
    "            \"td_error\":     batch[\"td_error\"].detach().mean().cpu().item(),\n",
    "            \"alpha\":        loss_data[\"alpha\"].detach().mean().cpu().item(),\n",
    "        }, mode='ema')\n",
    "    logger.add({\"train_time\": short_watch.end()})\n",
    "\n",
    "\n",
    "    # 3. Sync Policy\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # 4. Checkpoint and eval\n",
    "    if (i+1) % CKPT_EVAL_INTERVAL == 0:\n",
    "        eval_env.reset()\n",
    "        with torch.no_grad():\n",
    "            data = eval_env.rollout(EVAL_STEPS, policy=policy, break_when_any_done=False, auto_cast_to_device=True)\n",
    "        metrics = metric_module(data)\n",
    "        metrics[\"eval_entropy\"] = metrics[\"entropy\"]\n",
    "        del metrics[\"entropy\"]\n",
    "        logger.acc(metrics)\n",
    "        checkpointer.save_progress(state_obj={\n",
    "            \"timestep\": logger.last()[\"timestep\"],\n",
    "            \"loss_module\": loss_module.state_dict(),\n",
    "        })\n",
    "    else:\n",
    "        logger.add({key: 0 for key in [\"return\", \"episode_length\", \"eval_entropy\"]})\n",
    "\n",
    "    # 5. Log\n",
    "    logger.add({\"time\": long_watch.end()})\n",
    "    short_watch.start(); long_watch.start()\n",
    "    logger.next(print_row=True)\n",
    "collector.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "def plot(name, alpha=0):\n",
    "    # Get dataframe\n",
    "    logger = Logger(keys = LOG_KEYS, log_path=LOG_PATH, name=name)\n",
    "    logger.revert()\n",
    "    df = logger.dataframe()\n",
    "    df = df.set_index(LOG_INDEX)\n",
    "    if alpha>0:\n",
    "        df = df.ewm(alpha=alpha).mean()\n",
    "\n",
    "    # Plot it\n",
    "    rows = (len(df.columns) + 3) // 4\n",
    "    df.plot(subplots=True, layout=(rows,4), figsize=(15, int(rows* 7/3)))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, f\"{name}.png\"))\n",
    "plot(\"run0\", alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 243]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = create_env(graphics=True, time_scale=1)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    data = env.rollout(1000, policy=policy, break_when_any_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'return': 25.069377899169922,\n",
       " 'episode_length': 1000.0,\n",
       " 'entropy': -1.6405929327011108}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n",
      "Default audio device was changed, but the audio system failed to initialize it. Attempting to reset sound system.\n"
     ]
    }
   ],
   "source": [
    "SimpleMetricModule(mode=\"approx\")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
