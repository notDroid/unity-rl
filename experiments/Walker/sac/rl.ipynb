{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Walker SAC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), torch.float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rlkit.util import MultiVersionCheckpointer, Checkpointer, Logger, SimpleMetricModule, Stopwatch\n",
    "from config import *\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from env import create_env\n",
    "\n",
    "# Import Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchinfo import summary\n",
    "\n",
    "# Models and Loss\n",
    "from rlkit.models import MLP, CatWrapper\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "# Data\n",
    "from tensordict import TensorDict\n",
    "from torchrl.collectors import SyncDataCollector, MultiaSyncDataCollector, aSyncDataCollector\n",
    "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "# Util\n",
    "from torchrl.objectives import SACLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if str(device).startswith(\"cuda\") else \"cpu\"\n",
    "amp_dtype   = torch.float16 if device_type == \"cuda\" else torch.float32\n",
    "device, amp_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(model_config):\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"out_features\"] *= 2\n",
    "    model = MLP(**model_config)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        model,\n",
    "        NormalParamExtractor()\n",
    "    )\n",
    "    model = TensorDictModule(model, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "    \n",
    "    policy = ProbabilisticActor(\n",
    "        module=model,  \n",
    "        distribution_class=TanhNormal,\n",
    "\n",
    "        in_keys=[\"loc\", \"scale\"],\n",
    "        out_keys=[\"action\"],\n",
    "\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=\"log_prob\",\n",
    "        cache_dist=True,\n",
    "    )\n",
    "\n",
    "    return policy\n",
    "\n",
    "def create_qvalue(model_config):\n",
    "    # Remove out_features from config\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"in_features\"] = model_config[\"in_features\"] + model_config[\"out_features\"]\n",
    "    model_config[\"out_features\"] = 1\n",
    "\n",
    "    model = MLP(**model_config)\n",
    "    model = CatWrapper(model)\n",
    "    qvalue = TensorDictModule(model, in_keys=[\"observation\", \"action\"], out_keys=[\"state_action_value\"])\n",
    "    return qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = create_policy(MODEL_CONFIG)\n",
    "qvalue = create_qvalue(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TensorDictModule                              [1, 1]                    --\n",
       "├─CatWrapper: 1-1                             [1, 1]                    --\n",
       "│    └─MLP: 2-1                               [1, 1]                    --\n",
       "│    │    └─Linear: 3-1                       [1, 128]                  36,224\n",
       "│    │    └─ModuleList: 3-2                   --                        393,600\n",
       "│    │    └─Sequential: 3-3                   [1, 1]                    257\n",
       "===============================================================================================\n",
       "Total params: 430,081\n",
       "Trainable params: 430,081\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.43\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.02\n",
       "Params size (MB): 1.72\n",
       "Estimated Total Size (MB): 1.75\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = (torch.randn(1, MODEL_CONFIG[\"in_features\"]), torch.randn(1, MODEL_CONFIG[\"out_features\"]))\n",
    "summary(qvalue, input_data=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TensorDictModule                              [1, 39]                   --\n",
       "├─Sequential: 1-1                             [1, 39]                   --\n",
       "│    └─MLP: 2-1                               [1, 78]                   --\n",
       "│    │    └─Linear: 3-1                       [1, 128]                  31,232\n",
       "│    │    └─ModuleList: 3-2                   --                        393,600\n",
       "│    │    └─Sequential: 3-3                   [1, 78]                   10,190\n",
       "│    └─NormalParamExtractor: 2-2              [1, 39]                   --\n",
       "│    │    └─biased_softplus: 3-4              [1, 39]                   --\n",
       "===============================================================================================\n",
       "Total params: 435,022\n",
       "Trainable params: 435,022\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.44\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.03\n",
       "Params size (MB): 1.74\n",
       "Estimated Total Size (MB): 1.77\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = torch.randn(1 ,MODEL_CONFIG[\"in_features\"])\n",
    "summary(policy.module[0], input_data=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_module = SACLoss(\n",
    "#     actor_network=policy, qvalue_network=qvalue, value_network=None,\n",
    "#     num_qvalue_nets=2,\n",
    "#     alpha_init=0.1, fixed_alpha=True, \n",
    "#     delay_actor=False, delay_qvalue=True\n",
    "# )\n",
    "# target_updater = SoftUpdate(loss_module, tau=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: env.close()\n",
    "# except: pass\n",
    "# env = create_env(graphics=False, time_scale=10)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     data = env.rollout(100, policy=policy, break_when_any_done=False)\n",
    "# data, loss_module(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SCALE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS = os.cpu_count() // 2\n",
    "TIMESTEPS = 30_000_000\n",
    "BUFFER_SIZE = 3_000_000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# ENTROPY_TARGET = 1e-3\n",
    "ENTROPY_COEF = 1e-3\n",
    "# print(\"Entropy Target:\", ENTROPY_TARGET)\n",
    "GAMMA = 0.95\n",
    "TAU = 0.005\n",
    "\n",
    "LR = 1e-5\n",
    "\n",
    "TRAIN_STEPS = 64\n",
    "ENV_STEPS = 1000 * WORKERS\n",
    "# Steps = N*K, train_iters = K. K helps with buffering async collected data with less stalls\n",
    "\n",
    "CKPT_EVAL_INTERVAL = 100\n",
    "EVAL_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'run0'\n",
    "CONTINUE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = create_policy(MODEL_CONFIG).to(device)\n",
    "qvalue = create_qvalue(MODEL_CONFIG).to(device)\n",
    "\n",
    "loss_module = SACLoss(\n",
    "    actor_network=policy, qvalue_network=qvalue, value_network=None,\n",
    "    num_qvalue_nets=2,\n",
    "    alpha_init=ENTROPY_COEF, fixed_alpha=True,\n",
    "    # target_entropy=ENTROPY_TARGET, fixed_alpha=False,\n",
    "    delay_actor=False, delay_qvalue=True\n",
    ")\n",
    "loss_module.make_value_estimator(gamma=GAMMA)\n",
    "target_updater = SoftUpdate(loss_module, tau=TAU)\n",
    "\n",
    "optimizer = optim.Adam(loss_module.parameters(), lr=LR)\n",
    "\n",
    "replay_buffer = TensorDictReplayBuffer(\n",
    "    storage = LazyMemmapStorage(max_size=BUFFER_SIZE, ndim=2),\n",
    "    batch_size = BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(keys = LOG_KEYS, log_path=LOG_PATH, name=NAME)\n",
    "checkpointer = MultiVersionCheckpointer(ckpt_path=CKPT_PATH, name=NAME, metric_key=\"score\", levels=10, base_interval=10, interval_scale=1.75)\n",
    "\n",
    "# Continue/Reset\n",
    "start_timestep = 0\n",
    "if not CONTINUE:\n",
    "    logger.reset()\n",
    "    checkpointer.reset()\n",
    "else:\n",
    "    checkpoint = checkpointer.load_progress()\n",
    "    if checkpoint:\n",
    "        start_timestep = checkpoint['timestep']\n",
    "        print(f\"Found checkpoint at timestep: {start_timestep}\")\n",
    "        logger.revert(\"timestep\", start_timestep)\n",
    "        loss_module.load_state_dict(checkpoint[\"loss_module\"], strict=False)\n",
    "    else:\n",
    "        print(\"Checkpoint not found, restarting\")\n",
    "        logger.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/collectors/collectors.py:2137: UserWarning: total_frames (30000000) is not exactly divisible by frames_per_batch (7000). This means 2000 additional frames will be collected. To silence this message, set the environment variable RL_WARNINGS to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "New input system (experimental) initialized\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "Forcing GfxDevice: Null\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "NullGfxDevice:\n",
      "    Vendor:   Unity Technologies\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "Begin MonoManager ReloadAssembly\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Loaded All Assemblies, in  0.095 seconds\n",
      "- Loaded All Assemblies, in  0.096 seconds\n",
      "- Loaded All Assemblies, in  0.095 seconds\n",
      "- Loaded All Assemblies, in  0.095 seconds\n",
      "- Loaded All Assemblies, in  0.096 seconds\n",
      "- Loaded All Assemblies, in  0.096 seconds\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Loaded All Assemblies, in  0.098 seconds\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "- Finished resetting the current domain, in  0.002 seconds\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.413625 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.400959 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.369833 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.434625 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.471666 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.322834 ms\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.337250 ms\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "Registered Communicator in Agent.\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "Thread 0x17122b000 may have been prematurely finalized\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Thread 0x16fc8b000 may have been prematurely finalized\n",
      "Thread 0x16fd17000 may have been prematurely finalized\n",
      "Thread 0x17005f000 may have been prematurely finalized\n",
      "Thread 0x16fe2f000 may have been prematurely finalized\n",
      "Thread 0x16ff47000 may have been prematurely finalized\n",
      "Thread 0x16ffd3000 may have been prematurely finalized\n",
      "Thread 0x170177000 may have been prematurely finalized\n",
      "Thread 0x1700eb000 may have been prematurely finalized\n",
      "Thread 0x16fda3000 may have been prematurely finalized\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 69 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 464 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 70 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 1 frames, [4.0 MB-8.0 MB]: 69 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 5.4 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 69 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 70 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 70 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 69 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 70 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 70 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "Thread 0x1713bb000 may have been prematurely finalized\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Thread 0x16fd8f000 may have been prematurely finalized\n",
      "Thread 0x16fe1b000 may have been prematurely finalized\n",
      "Thread 0x16fea7000 may have been prematurely finalized\n",
      "Thread 0x16ff33000 may have been prematurely finalized\n",
      "Thread 0x16ffbf000 may have been prematurely finalized\n",
      "Thread 0x17004b000 may have been prematurely finalized\n",
      "Thread 0x170163000 may have been prematurely finalized\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 126 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 127 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 2 frames, [4.0 MB-8.0 MB]: 125 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 4.8 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 126 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 127 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 127 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 126 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 127 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 127 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "Thread 0x17101f000 may have been prematurely finalized\n",
      "Thread 0x16d48f000 may have been prematurely finalized\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Thread 0x16beef000 may have been prematurely finalized\n",
      "Thread 0x16bf7b000 may have been prematurely finalized\n",
      "Thread 0x16c11f000 may have been prematurely finalized\n",
      "Thread 0x16c093000 may have been prematurely finalized\n",
      "Thread 0x16c007000 may have been prematurely finalized\n",
      "Thread 0x16c237000 may have been prematurely finalized\n",
      "Thread 0x16fa7f000 may have been prematurely finalized\n",
      "Thread 0x16fb0b000 may have been prematurely finalized\n",
      "Thread 0x16fb97000 may have been prematurely finalized\n",
      "Thread 0x16fedf000 may have been prematurely finalized\n",
      "Thread 0x16ff6b000 may have been prematurely finalized\n",
      "Thread 0x16fff7000 may have been prematurely finalized\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 119 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 80 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 120 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 6 frames, [4.0 MB-8.0 MB]: 114 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 4.6 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 119 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 120 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.6 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 120 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 119 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 120 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 120 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 120 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 121 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 5 frames, [4.0 MB-8.0 MB]: 116 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 4.6 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 120 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 121 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 121 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 120 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 121 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 121 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "Thread 0x171323000 may have been prematurely finalized\n",
      "Thread 0x17129f000 may have been prematurely finalized\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Thread 0x16fc73000 may have been prematurely finalized\n",
      "Thread 0x16fe17000 may have been prematurely finalized\n",
      "Thread 0x16fcff000 may have been prematurely finalized\n",
      "Thread 0x16fd8b000 may have been prematurely finalized\n",
      "Thread 0x16fea3000 may have been prematurely finalized\n",
      "Thread 0x16ffbb000 may have been prematurely finalized\n",
      "Thread 0x170047000 may have been prematurely finalized\n",
      "Thread 0x16fd83000 may have been prematurely finalized\n",
      "Thread 0x16fe0f000 may have been prematurely finalized\n",
      "Thread 0x16fe9b000 may have been prematurely finalized\n",
      "Thread 0x17003f000 may have been prematurely finalized\n",
      "Thread 0x1701e3000 may have been prematurely finalized\n",
      "Thread 0x17026f000 may have been prematurely finalized\n",
      "Thread 0x1702fb000 may have been prematurely finalized\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 120 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 121 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 2 frames, [4.0 MB-8.0 MB]: 119 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 4.8 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 120 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 121 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.6 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 121 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 120 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 121 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 121 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 120 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 121 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 2 frames, [4.0 MB-8.0 MB]: 119 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 120 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 121 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 121 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 120 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 121 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 121 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "Thread 0x16f4af000 may have been prematurely finalized\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Thread 0x16df9b000 may have been prematurely finalized\n",
      "Thread 0x16e027000 may have been prematurely finalized\n",
      "Thread 0x16e1cb000 may have been prematurely finalized\n",
      "Thread 0x16e13f000 may have been prematurely finalized\n",
      "Thread 0x16e257000 may have been prematurely finalized\n",
      "Thread 0x16e0b3000 may have been prematurely finalized\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [1.0 KB-2.0 KB]: 126 frames, [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 272 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 127 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 3 frames, [4.0 MB-8.0 MB]: 124 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 4.6 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [32.0 KB-64.0 KB]: 126 frames, [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 127 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 127 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 126 frames, [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 127 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 127 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n"
     ]
    }
   ],
   "source": [
    "collector = MultiaSyncDataCollector(\n",
    "    [lambda: create_env(graphics=False, time_scale=TIME_SCALE)]*WORKERS, policy, \n",
    "    frames_per_batch = ENV_STEPS, \n",
    "    total_frames = TIMESTEPS - start_timestep, \n",
    "    env_device=\"cpu\", device=device, storing_device=\"cpu\",\n",
    "    reset_at_each_iter=False,\n",
    ")\n",
    "short_watch, long_watch = Stopwatch(), Stopwatch()\n",
    "metric_module = SimpleMetricModule(mode=\"approx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Walker.app/Contents/MonoBleedingEdge/etc'\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "New input system (experimental) initialized\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Walker.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Loaded All Assemblies, in  0.052 seconds\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.305709 ms\n",
      "Registered Communicator in Agent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/transforms/transforms.py:587: FutureWarning: The key 'continuous_action' is unaccounted for by the transform (expected keys ['VectorSensor_size243', 'done', 'terminated', 'truncated', 'group_reward', 'reward']). Every new entry in the tensordict resulting from a call to a transform must be registered in the specs for torchrl rollouts to be consistently built. Make sure transform_output_spec/transform_observation_spec/... is coded correctly. This warning will trigger a KeyError in v0.9, make sure to adapt your code accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "Thread 0x16d0a7000 may have been prematurely finalized\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Thread 0x16bb93000 may have been prematurely finalized\n",
      "Thread 0x16bc1f000 may have been prematurely finalized\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 92.6 KB\n",
      "      Overflow Count 5\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.5 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0.7 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 158 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 2.2 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 69.7 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.7 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.9 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.3 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 1 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 1 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "   timestep       time  collect_wait_time  train_time    score  policy_loss  \\\n",
      "0      7000  10.749954           8.120273    2.629605 -1.89343     0.521823   \n",
      "\n",
      "   qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "0     0.228255  0.001  14.720404  0.511746       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "0             0  \n",
      "   timestep      time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "1     14000  13.74312           8.131315    5.611675 -1.709854     0.408005   \n",
      "\n",
      "   qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "1     0.156196  0.001  14.473047  0.411432       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "1             0  \n",
      "   timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "2     21000  19.334724          10.927051    8.407488 -1.744988     0.323107   \n",
      "\n",
      "   qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "2     0.115243  0.001  13.871239  0.352043       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "2             0  \n",
      "   timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "3     28000  22.272102          10.937466   11.334402 -1.766347     0.224463   \n",
      "\n",
      "   qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "3     0.090759  0.001  12.409922  0.312395       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "3             0  \n",
      "   timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "4     35000  25.395583          10.946778   14.448512 -1.821429      0.13189   \n",
      "\n",
      "   qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "4     0.068392  0.001  9.915728  0.270166       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "4             0  \n",
      "   timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "5     42000  28.260876          10.957442   17.303087 -1.628045     0.059973   \n",
      "\n",
      "   qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "5      0.05227  0.001  6.647981  0.237597       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "5             0  \n",
      "   timestep    time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "6     49000  31.087          10.968087   20.118503 -2.007725     0.001387   \n",
      "\n",
      "   qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "6     0.041109  0.001  2.649119   0.21168       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "6             0  \n",
      "   timestep       time  collect_wait_time  train_time    score  policy_loss  \\\n",
      "7     56000  33.818383          10.979663   22.838256 -1.80525    -0.030156   \n",
      "\n",
      "   qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "7     0.033028  0.001 -0.386116  0.190436       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "7             0  \n",
      "   timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "8     63000  36.665276          10.989352   25.675409 -2.059363    -0.038433   \n",
      "\n",
      "   qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "8     0.027367  0.001 -2.658467  0.172774       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "8             0  \n",
      "   timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "9     70000  39.347734          11.000145   28.347025 -2.329901      -0.0384   \n",
      "\n",
      "   qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "9     0.022414  0.001 -3.599847  0.157023       0               0   \n",
      "\n",
      "   eval_entropy  \n",
      "9             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "10     77000  42.204225          11.009059   31.194551 -2.558246    -0.039103   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "10     0.019042  0.001 -3.532955  0.143088       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "10             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "11     84000  45.056822          11.018699   34.037455 -2.627587    -0.029006   \n",
      "\n",
      "    qvalue_loss  alpha  entropy  td_error  return  episode_length  \\\n",
      "11     0.016241  0.001 -2.78982  0.132848       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "11             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "12     91000  47.806516          11.027916   36.777874 -2.641775    -0.021348   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "12     0.014412  0.001 -1.503281  0.124175       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "12             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "13     98000  50.659807          11.038942   39.620087 -2.915496    -0.021796   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "13     0.012428  0.001 -0.101528  0.115851       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "13             0  \n",
      "    timestep      time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "14    105000  53.64271          11.049243   42.592624 -2.892202    -0.021599   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "14     0.011004  0.001  1.165959  0.108228       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "14             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "15    112000  56.349211          11.059132   45.289176 -2.876858    -0.025367   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "15     0.010044  0.001  2.128664  0.103085       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "15             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "16    119000  59.169761          11.068031   48.100767 -2.910432    -0.023696   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "16     0.009143  0.001  3.111048  0.098017       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "16             0  \n",
      "    timestep       time  collect_wait_time  train_time    score  policy_loss  \\\n",
      "17    126000  62.017066          11.077007   50.939044 -2.75845    -0.021245   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "17     0.008205  0.001  4.086648  0.093122       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "17             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "18    133000  64.872282          11.087179   53.784039 -2.523306     -0.02619   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "18     0.007453  0.001  4.960173  0.088505       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "18             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "19    140000  67.684212           11.10264   56.580442 -2.635343     -0.02549   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "19     0.006948  0.001  6.041299  0.085237       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "19             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "20    147000  70.540392          11.114303   59.424897 -2.459902    -0.025264   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "20     0.006427  0.001  6.678664   0.08172       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "20             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "21    154000  73.247917          11.126628    62.12004 -2.437478    -0.027249   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "21     0.005961  0.001  7.331818  0.079455       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "21             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "22    161000  76.158752          11.137871   65.019577 -2.279345     -0.02957   \n",
      "\n",
      "    qvalue_loss  alpha  entropy  td_error  return  episode_length  \\\n",
      "22     0.005297  0.001  7.72384   0.07548       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "22             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "23    168000  79.221822          11.153377   68.067079 -2.176102     -0.03147   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "23      0.00516  0.001  8.355187  0.073608       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "23             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "24    175000  82.174318          11.162639   71.010264 -1.971736    -0.031436   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "24     0.004782  0.001  8.608886  0.070857       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "24             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "25    182000  85.003114          11.174412   73.827232 -1.931106    -0.036584   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "25     0.004538  0.001  9.015714  0.069641       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "25             0  \n",
      "    timestep      time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "26    189000  88.05995          11.184222   76.874202 -1.992175    -0.038551   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "26     0.004272  0.001  9.129513   0.06733       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "26             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "27    196000  91.144527           11.19374   79.949205 -1.814692    -0.042343   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "27     0.004094  0.001  9.482454   0.06592       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "27             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "28    203000  94.060952          11.207148   82.852157 -1.717707     -0.04316   \n",
      "\n",
      "    qvalue_loss  alpha   entropy  td_error  return  episode_length  \\\n",
      "28     0.003831  0.001  9.792243  0.063831       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "28             0  \n",
      "    timestep      time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "29    210000  96.95819           11.21713   85.739362 -1.738449    -0.045609   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "29     0.003627  0.001  10.098433  0.062036       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "29             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "30    217000  99.829257          11.225827   88.601682 -1.821958     -0.04777   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "30     0.003529  0.001  10.355797  0.061112       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "30             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "31    224000  102.573822          11.234731   91.337285 -1.752913   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "31    -0.048042     0.003292  0.001  10.657273  0.059136       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "31               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "32    231000  105.487163          11.244352   94.240947 -1.794505   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "32     -0.04992     0.003158  0.001  11.020856  0.057916       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "32               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "33    238000  108.394123          11.254217   97.137971 -1.832405   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "33    -0.051232     0.003014  0.001  11.236378  0.056428       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "33               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "34    245000  111.236118          11.263019    99.97111 -1.954709   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "34    -0.051968     0.002868  0.001  11.55386  0.055223       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "34               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "35    252000  114.184753          11.272243  102.910475 -1.804932   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "35    -0.054602     0.002795  0.001  12.006891  0.054589       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "35               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "36    259000  117.542557          11.281253  106.259209 -1.665651   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "36    -0.055289     0.002656  0.001  12.095608  0.052747       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "36               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "37    266000  120.529239          11.295214  109.231876 -1.835278   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "37     -0.05843     0.002539  0.001  12.414861  0.051588       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "37               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "38    273000  123.855599          11.305615  112.547773 -1.710623   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "38    -0.058199     0.002384  0.001  12.619751   0.05019       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "38               0             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "39    280000  126.94894          11.315221  115.631452 -1.609502    -0.060396   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "39     0.002349  0.001  12.813831   0.04982       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "39             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "40    287000  129.793805            11.3238  118.467673 -1.542541   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "40    -0.062189     0.002248  0.001  13.192252  0.048617       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "40               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "41    294000  132.648907          11.334397   121.31213 -1.502891   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "41    -0.062307     0.002113  0.001  13.370366  0.047233       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "41               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "42    301000  135.517456          11.343445  124.171577 -1.646802   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "42    -0.063754     0.002066  0.001  13.627848  0.046752       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "42               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "43    308000  138.518196          11.354029   127.16167 -1.483654   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "43    -0.064934     0.001991  0.001  13.90078  0.045642       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "43               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "44    315000  141.261722            11.3651  129.894074 -1.477397   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "44    -0.064569     0.001963  0.001  14.017102  0.045226       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "44               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "45    322000  144.162245          11.374216  132.785429 -1.725254   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "45    -0.066434     0.001919  0.001  14.281651  0.044789       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "45               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "46    329000  147.044972          11.383897  135.658421 -1.465084   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "46    -0.066436     0.001782  0.001  14.457742  0.043379       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "46               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "47    336000  149.688584          11.393632   138.29223 -1.278651   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "47    -0.067554     0.001751  0.001  14.639427  0.042872       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "47               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "48    343000  152.466767          11.402943  141.061049 -1.465375   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "48    -0.067554     0.001673  0.001  14.769607  0.041994       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "48               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "49    350000  155.327043          11.412195  143.912016 -1.553206   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "49    -0.068035     0.001625  0.001  14.909288  0.041318       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "49               0             0  \n",
      "    timestep        time  collect_wait_time  train_time    score  policy_loss  \\\n",
      "50    357000  158.047161          11.422334  146.621943 -1.23139    -0.068346   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "50     0.001558  0.001  15.066759  0.040265       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "50             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "51    364000  160.723294          11.430973  149.289386 -1.255698   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "51    -0.069817     0.001537  0.001  15.16861  0.039913       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "51               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "52    371000  163.659475          11.440902  152.215582 -1.483162   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "52    -0.069437      0.00144  0.001  15.19679  0.038973       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "52               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "53    378000  166.505918          11.449974  155.052895 -1.481427   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "53    -0.070968     0.001423  0.001  15.585977  0.038507       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "53               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "54    385000  169.242654          11.459249  157.780306 -1.692675   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "54    -0.071213      0.00137  0.001  15.768453  0.037824       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "54               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "55    392000  172.002106          11.468503  160.530449 -1.587392   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "55    -0.070716     0.001311  0.001  15.959039  0.036933       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "55               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "56    399000  174.799804          11.479325  163.317273 -1.321902   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "56    -0.071723     0.001282  0.001  16.296166  0.036519       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "56               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "57    406000  177.438823          11.488982  165.946581 -1.558352   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "57    -0.072333     0.001238  0.001  16.389717  0.036125       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "57               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "58    413000  180.260186          11.499801  168.757067 -1.507913   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "58    -0.074003     0.001185  0.001  16.635374  0.035358       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "58               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "59    420000  183.211695          11.511007  171.697313 -1.386863   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "59    -0.075284     0.001166  0.001  16.938101  0.034736       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "59               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "60    427000  186.115371          11.519696  174.592248 -1.487389   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "60    -0.075147     0.001127  0.001  17.186986  0.034249       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "60               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "61    434000  188.929281          11.529202  177.396592 -1.616808   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "61    -0.075735     0.001135  0.001  17.368273  0.033984       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "61               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "62    441000  191.786314           11.53847  180.244306 -1.712581   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "62    -0.076916     0.001059  0.001  17.529687  0.032947       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "62               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "63    448000  194.624408          11.547179  183.073636 -1.579471   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "63    -0.077351     0.001032  0.001  17.602939  0.032513       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "63               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "64    455000  197.321099          11.556835  185.760606 -1.499837   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "64    -0.077505     0.001004  0.001  17.718152  0.032301       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "64               0             0  \n",
      "    timestep       time  collect_wait_time  train_time   score  policy_loss  \\\n",
      "65    462000  200.39106          11.568024  188.819308 -1.4147    -0.078877   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "65     0.000941  0.001  18.106653  0.031251       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "65             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "66    469000  203.408225          11.577978  191.826461 -1.528505   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "66    -0.079922     0.000969  0.001  18.217139  0.031467       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "66               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "67    476000  206.022309            11.5874  194.431074 -1.600532   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "67    -0.081601     0.000909  0.001  18.28552  0.030393       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "67               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "68    483000  208.869742          11.596708  197.269145 -1.704294   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "68    -0.082304      0.00087  0.001  18.602655   0.02971       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "68               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "69    490000  211.566663          11.605507  199.957209 -1.429365   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "69    -0.082414     0.000879  0.001  18.538778  0.029837       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "69               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "70    497000  214.364025          11.616204   202.74381 -1.486583   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "70    -0.084464     0.000864  0.001  18.836481  0.029603       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "70               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "71    504000  217.180176          11.626343  205.549776 -1.654861   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "71    -0.085112     0.000848  0.001  19.037693  0.029151       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "71               0             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "72    511000  220.04372          11.636131   208.40348 -1.515874    -0.085811   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "72     0.000813  0.001  19.351108  0.028795       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "72             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "73    518000  222.756599          11.648826  211.103601 -1.637655   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "73    -0.087177     0.000779  0.001  19.354735   0.02817       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "73               0             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "74    525000  225.43085          11.659159  213.767459 -1.742178    -0.087307   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "74     0.000786  0.001  19.377824  0.028077       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "74             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "75    532000  228.284828          11.668014  216.612526 -1.562512   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "75    -0.089034      0.00076  0.001  19.549297  0.027669       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "75               0             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "76    539000  231.20794          11.677722  219.525869 -1.870076     -0.08945   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "76      0.00075  0.001  19.639225  0.027298       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "76             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "77    546000  233.923717          11.687151  222.232166 -1.702011   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "77    -0.091041      0.00071  0.001  19.769992  0.026834       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "77               0             0  \n",
      "    timestep        time  collect_wait_time  train_time    score  policy_loss  \\\n",
      "78    553000  236.655348          11.695503  224.955398 -1.77058    -0.091787   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "78     0.000697  0.001  19.888328  0.026428       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "78             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "79    560000  239.590569          11.704091  227.881973 -1.683219   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "79    -0.094282     0.000688  0.001  20.151751  0.025998       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "79               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "80    567000  242.370199          11.712994   230.65265 -1.441485   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "80    -0.094352     0.000687  0.001  20.430482  0.026007       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "80               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "81    574000  245.089405          11.722136  233.362658 -1.777341   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "81    -0.095326     0.000693  0.001  20.492728  0.025849       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "81               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "82    581000  247.935442          11.731941  236.198826 -1.634887   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "82    -0.096771     0.000683  0.001  20.673369   0.02584       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "82               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "83    588000  250.809604          11.740177  239.064698 -1.711149   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "83    -0.097747     0.000643  0.001  20.761272   0.02488       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "83               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "84    595000  253.505564           11.74895  241.751834 -1.715527   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "84    -0.099127     0.000667  0.001  20.787449  0.025239       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "84               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "85    602000  256.299771          11.758057  244.536875 -1.645883   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "85    -0.100234     0.000633  0.001  20.972528  0.024668       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "85               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "86    609000  259.021744          11.766834  247.250016 -1.764392   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "86     -0.10025     0.000621  0.001  20.93446  0.024387       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "86               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "87    616000  261.815575          11.776222  250.034407 -1.675992   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "87    -0.101644     0.000641  0.001  21.124255  0.024717       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "87               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "88    623000  264.500877           11.78596  252.709911 -1.604454   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "88    -0.102661     0.000611  0.001  21.238484  0.024001       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "88               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "89    630000  267.328444          11.795013   255.52837 -1.394317   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "89    -0.104887     0.000596  0.001  21.363325  0.023637       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "89               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "90    637000  270.249434          11.803953  258.440365 -1.621597   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "90    -0.105073     0.000622  0.001  21.419172  0.024126       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "90               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "91    644000  272.965453           11.81321  261.147074 -1.766895   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "91    -0.106579     0.000589  0.001  21.505752  0.023527       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "91               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "92    651000  275.747174          11.822196  263.919736 -1.808639   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "92    -0.107181     0.000589  0.001  21.463149  0.023375       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "92               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "93    658000  278.652064          11.833208  266.813562 -1.687062   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "93    -0.109356     0.000594  0.001  21.659918  0.023434       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "93               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "94    665000  281.526223          11.842772  269.678101 -1.634681   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "94    -0.110298     0.000574  0.001  21.622013  0.022869       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "94               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "95    672000  284.295741           11.85125  272.439076 -1.635425   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "95     -0.11097     0.000593  0.001  21.812336   0.02317       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "95               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "96    679000  287.110788          11.860138  275.245186 -1.520088   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "96    -0.112103     0.000575  0.001  21.90208   0.02268       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "96               0             0  \n",
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "97    686000  289.963166          11.869554  278.088092 -1.745098   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "97    -0.112557     0.000583  0.001  21.906567  0.022948       0   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "97               0             0  \n",
      "    timestep       time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "98    693000  292.72043          11.878866  280.835988 -1.579022    -0.113424   \n",
      "\n",
      "    qvalue_loss  alpha    entropy  td_error  return  episode_length  \\\n",
      "98     0.000584  0.001  22.036184  0.022797       0               0   \n",
      "\n",
      "    eval_entropy  \n",
      "98             0  \n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    timestep        time  collect_wait_time  train_time     score  \\\n",
      "99    700000  307.652347           11.89005  283.836644 -1.439175   \n",
      "\n",
      "    policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "99    -0.114682     0.000554  0.001  22.107883  0.022432 -0.009265   \n",
      "\n",
      "    episode_length  eval_entropy  \n",
      "99       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "100    707000  310.500867          11.900007  286.675122 -1.769534   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "100    -0.117102     0.000533  0.001  22.154224  0.021901 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "100       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "101    714000  314.121662          11.910159  290.285707 -1.832091   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "101    -0.118425     0.000538  0.001  22.195711  0.021859 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "101       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "102    721000  317.444221          11.920613  293.597696 -1.728933   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha  entropy  td_error    return  \\\n",
      "102    -0.119273     0.000534  0.001  22.2963  0.021784 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "102       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time   score  policy_loss  \\\n",
      "103    728000  320.473264          11.930748  296.616544 -1.6986    -0.120243   \n",
      "\n",
      "     qvalue_loss  alpha    entropy  td_error    return  episode_length  \\\n",
      "103     0.000559  0.001  22.419273  0.022024 -0.009265       22.075056   \n",
      "\n",
      "     eval_entropy  \n",
      "103     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "104    735000  323.490946          11.940301  299.624572 -1.696634   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "104    -0.121526     0.000539  0.001  22.338569  0.021762 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "104       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "105    742000  326.544643          11.951806  302.666716 -1.500884   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "105    -0.122673     0.000557  0.001  22.53272  0.022056 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "105       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "106    749000  329.583654          11.961264  305.696216 -1.591778   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "106    -0.122762     0.000552  0.001  22.592342  0.021903 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "106       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "107    756000  332.548412          11.970174  308.652009 -1.569514   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "107    -0.124586     0.000549  0.001  22.518023  0.021788 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "107       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "108    763000  335.494168          11.978609  311.589278 -1.554693   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "108     -0.12611      0.00053  0.001  22.623873   0.02147 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "108       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "109    770000  338.323772          11.987774  314.409665 -1.535312   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "109    -0.126625     0.000533  0.001  22.66918  0.021558 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "109       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "110    777000  341.088979          11.997648  317.164937 -1.647963   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "110    -0.128691     0.000519  0.001  22.836283  0.020989 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "110       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "111    784000  343.912627          12.006456  319.979715 -1.648139   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "111    -0.129226     0.000515  0.001  22.547326  0.021056 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "111       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "112    791000  346.685273          12.015843  322.742923 -1.643458   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "112    -0.129479     0.000537  0.001  22.686932  0.021478 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "112       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "113    798000  349.363711          12.025413  325.411732 -1.810034   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "113    -0.131064     0.000539  0.001  22.758227  0.021356 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "113       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "114    805000  352.178067          12.034375  328.217074 -1.732052   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "114    -0.131936     0.000512  0.001  22.63409  0.021047 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "114       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "115    812000  355.074538           12.04353  331.104329 -1.287738   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "115    -0.132341     0.000534  0.001  22.721835  0.021371 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "115       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "116    819000  357.923211          12.055609  333.940868 -1.51961   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "116    -0.135127     0.000519  0.001  22.93463   0.02089 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "116       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "117    826000  360.641601          12.064175  336.650644 -1.584982   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "117    -0.133791     0.000529  0.001  22.963331  0.021253 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "117       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "118    833000  363.507729          12.074688  339.506208 -1.458567   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "118    -0.136683      0.00051  0.001  23.040684  0.020883 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "118       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "119    840000  366.323514          12.084243  342.312389 -1.655899   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "119      -0.1373     0.000496  0.001  22.996483  0.020652 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "119       22.075056     22.243607  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "120    847000  369.01455          12.094303  344.993316 -1.613737   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "120    -0.138337     0.000532  0.001  22.792066  0.021109 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "120       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "121    854000  371.847157          12.103386  347.816778 -1.483099   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "121     -0.13903     0.000515  0.001  22.865023  0.020876 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "121       22.075056     22.243607  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "122    861000  374.71011          12.114847  350.668212 -1.792215   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "122    -0.139887     0.000504  0.001  22.956219  0.020583 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "122       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "123    868000  377.344798           12.12398  353.293718 -1.719442   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "123    -0.142013     0.000524  0.001  23.029981  0.020856 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "123       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "124    875000  380.151964          12.133726   356.09108 -1.574698   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "124    -0.141899     0.000499  0.001  23.061622  0.020599 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "124       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "125    882000  383.066553          12.143255  358.996083 -1.534146   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "125    -0.143074     0.000505  0.001  23.064803  0.020699 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "125       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "126    889000  386.082057          12.153237  362.001554 -1.745178   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "126    -0.143555     0.000499  0.001  23.045045  0.020462 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "126       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "127    896000  389.073294          12.163456  364.982501 -1.580185   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "127    -0.144028     0.000501  0.001  23.146493  0.020629 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "127       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "128    903000  392.646759          12.174742  368.544621 -1.666418   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "128    -0.146177      0.00052  0.001  23.305984   0.02094 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "128       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "129    910000  395.737132          12.184326  371.625358 -1.555343   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "129    -0.146673     0.000497  0.001  23.329337  0.020681 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "129       22.075056     22.243607  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "130    917000  398.53468          12.193585  374.413585 -1.500816   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "130    -0.146286      0.00052  0.001  23.30931  0.020875 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "130       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "131    924000  401.404217           12.20369  377.272965 -1.528455   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "131    -0.148116     0.000502  0.001  23.203659  0.020539 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "131       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "132    931000  404.714463          12.213487  380.573358 -1.791041   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "132    -0.149128     0.000476  0.001  23.190998  0.020218 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "132       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "133    938000  407.990993          12.224477  383.838846 -1.552967   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "133    -0.148766     0.000479  0.001  23.310581  0.020452 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "133       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "134    945000  410.808128          12.233739  386.646665 -1.773912   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "134    -0.148667     0.000504  0.001  23.32425  0.020705 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "134       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "135    952000  413.956287           12.24294  389.785566 -1.739649   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "135    -0.151269      0.00047  0.001  23.321762  0.020213 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "135       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "136    959000  416.982394          12.252688  392.801852 -1.652337   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "136    -0.152656     0.000485  0.001  23.196013  0.020316 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "136       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "137    966000  419.797618          12.263601  395.606107 -1.544092   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "137    -0.152733     0.000496  0.001  23.217936  0.020543 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "137       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "138    973000  422.841471          12.274891  398.638589 -1.621655   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "138    -0.153617     0.000479  0.001  23.417904  0.020434 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "138       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "139    980000  425.666946          12.285318  401.453585 -1.46555   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "139    -0.152426     0.000493  0.001  23.433798  0.020626 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "139       22.075056     22.243607  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "140    987000  428.73225          12.295183  404.508964 -1.473977   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "140    -0.155205     0.000488  0.001  23.327457  0.020532 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "140       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "141    994000  431.657922          12.304894  407.424865 -1.672728   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "141    -0.155508     0.000472  0.001  23.362811   0.02031 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "141       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "142   1001000  434.619226          12.315269   410.37574 -1.523488   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "142    -0.156974     0.000462  0.001  23.368207  0.020193 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "142       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "143   1008000  437.702125          12.324512  413.449344 -1.361476   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "143    -0.157593     0.000475  0.001  23.298087  0.020163 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "143       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "144   1015000  440.455971          12.335226  416.192421 -1.591269   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "144    -0.157667     0.000478  0.001  23.203202  0.020263 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "144       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "145   1022000  443.290686          12.344498  419.017813 -1.454779   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "145    -0.157908     0.000498  0.001  23.281066  0.020729 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "145       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "146   1029000  446.301099          12.354072  422.018591 -1.453382   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "146    -0.159285     0.000465  0.001  23.162052  0.020206 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "146       22.075056     22.243607  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "147   1036000  449.23102          12.363069  424.939454 -1.500092   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "147    -0.160172     0.000467  0.001  23.281049   0.02029 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "147       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "148   1043000  452.262579          12.372944  427.961085 -1.306786   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "148    -0.160039     0.000483  0.001  23.290088  0.020559 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "148       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "149   1050000  455.408158          12.384194  431.095362 -1.335138   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "149    -0.160142     0.000482  0.001  23.300538  0.020471 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "149       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "150   1057000  458.333947          12.393544   434.01174 -1.097077   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "150    -0.161409     0.000475  0.001  23.248224  0.020535 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "150       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "151   1064000  461.234188          12.403421  436.902045 -1.466143   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "151     -0.16217      0.00046  0.001  23.150906  0.020363 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "151       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "152   1071000  464.137842          12.415319  439.793746 -1.674113   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "152    -0.161471     0.000456  0.001  23.208874  0.020341 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "152       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "153   1078000  467.271072          12.425088  442.917158 -1.416161   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "153    -0.163594     0.000453  0.001  23.379485  0.020065 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "153       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "154   1085000  470.177369           12.43535   445.81314 -1.321648   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "154    -0.162879     0.000469  0.001  23.415369  0.020474 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "154       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "155   1092000  473.199468          12.446714  448.823819 -1.677005   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "155    -0.164213     0.000446  0.001  23.273898  0.020223 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "155       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "156   1099000  476.180621          12.457434  451.794198 -1.739144   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "156    -0.164118     0.000459  0.001  23.241886  0.020465 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "156       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "157   1106000  479.096555          12.467535  454.699977 -1.705277   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "157     -0.16449     0.000465  0.001  23.353557  0.020524 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "157       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "158   1113000  481.942941          12.476502  457.537341 -1.425003   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "158    -0.165442     0.000461  0.001  23.016714  0.020363 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "158       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "159   1120000  484.862948          12.485696  460.448104 -1.407651   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "159    -0.164197     0.000454  0.001  23.114006  0.020216 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "159       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "160   1127000  487.769997          12.494662  463.346131 -1.305353   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "160    -0.166364     0.000465  0.001  23.371785  0.020613 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "160       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "161   1134000  490.610653          12.503966  466.177426 -1.468058   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "161    -0.165979     0.000441  0.001  23.379963  0.020213 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "161       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "162   1141000  493.625725          12.513227  469.183185 -1.360142   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "162    -0.167665     0.000468  0.001  23.453472  0.020477 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "162       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "163   1148000  496.758835          12.522865  472.306602 -1.52274   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "163    -0.166578     0.000452  0.001  23.29562  0.020492 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "163       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "164   1155000  499.639242          12.532139  475.177683 -1.381299   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "164    -0.169023     0.000437  0.001  23.254112  0.020155 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "164       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "165   1162000  502.379848          12.541615   477.90876 -1.470227   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "165    -0.167964     0.000465  0.001  23.454865  0.020515 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "165       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "166   1169000  505.428705          12.551695  480.947479 -1.445487   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "166    -0.168316     0.000458  0.001  23.348113  0.020386 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "166       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "167   1176000  508.451115          12.560722  483.960808 -1.421833   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "167    -0.168949     0.000451  0.001  23.189525  0.020408 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "167       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "168   1183000  511.296481          12.571801  486.795034 -1.433566   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "168    -0.168848      0.00045  0.001  23.289522  0.020374 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "168       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "169   1190000  514.348703          12.581743  489.837257 -1.343313   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "169    -0.169006     0.000427  0.001  23.464635  0.020142 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "169       22.075056     22.243607  \n",
      "     timestep      time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "170   1197000  517.3916          12.591578  492.870259 -1.370668    -0.169371   \n",
      "\n",
      "     qvalue_loss  alpha    entropy  td_error    return  episode_length  \\\n",
      "170     0.000446  0.001  23.407207  0.020383 -0.009265       22.075056   \n",
      "\n",
      "     eval_entropy  \n",
      "170     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "171   1204000  520.378556          12.600968  495.847769 -1.642085   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "171     -0.16914     0.000429  0.001  23.508376  0.020182 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "171       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "172   1211000  523.387914          12.611073  498.846967 -1.293456   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "172    -0.169912     0.000431  0.001  23.278215  0.020122 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "172       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "173   1218000  526.611049            12.6215   502.05962 -1.348952   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "173    -0.169066     0.000458  0.001  23.393593  0.020581 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "173       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "174   1225000  529.598643          12.631232  505.037434 -1.526223   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "174    -0.170702     0.000442  0.001  23.320352  0.020154 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "174       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "175   1232000  532.535776          12.640427  507.965313 -1.552602   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "175      -0.1698     0.000452  0.001  23.42412  0.020225 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "175       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "176   1239000  535.661069          12.649054  511.081928 -1.523839   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "176    -0.170495     0.000418  0.001  23.325929  0.019925 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "176       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "177   1246000  538.597376          12.659214  514.008019 -1.429523   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "177    -0.172209     0.000429  0.001  23.351402  0.020109 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "177       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "178   1253000  541.237698          12.668461   516.63904 -1.342063   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "178     -0.17034      0.00045  0.001  23.51908  0.020467 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "178       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "179   1260000  543.969168          12.678645  519.360271 -1.417711   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "179    -0.172063     0.000447  0.001  23.337467  0.020351 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "179       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "180   1267000  546.937074           12.68727  522.319499 -1.459226   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "180    -0.172341     0.000448  0.001  23.392296  0.020313 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "180       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "181   1274000  550.056174          12.698915  525.426898 -1.306773   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "181      -0.1718     0.000431  0.001  23.501692  0.020166 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "181       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "182   1281000  552.917838          12.707056  528.280363 -1.337311   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "182    -0.172138     0.000435  0.001  23.331079  0.020139 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "182       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "183   1288000  555.845447          12.716407  531.198564 -1.54328   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "183    -0.173366     0.000448  0.001  23.189087  0.020319 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "183       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "184   1295000  558.882961          12.727339  534.225087 -1.549918   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "184    -0.173268     0.000418  0.001  22.975791  0.020015 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "184       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "185   1302000  561.891856          12.735977  537.225289 -1.473022   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "185    -0.173436     0.000434  0.001  23.069269  0.020079 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "185       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "186   1309000  564.879829          12.746839  540.202346 -1.161295   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "186    -0.173443     0.000437  0.001  23.381013  0.020073 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "186       22.075056     22.243607  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "187   1316000  567.86681          12.756573  543.179522 -1.186071   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "187    -0.172889     0.000445  0.001  23.479042  0.020108 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "187       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "188   1323000  570.812346          12.766298  546.115281 -1.19328   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "188    -0.172932     0.000429  0.001  23.339426  0.020241 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "188       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "189   1330000  573.789503          12.776592  549.082089 -1.180176   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "189    -0.173598     0.000434  0.001  23.285142  0.020042 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "189       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "190   1337000  576.716525          12.786394   551.99926 -1.296285   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "190    -0.174552     0.000427  0.001  23.532743  0.019981 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "190       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "191   1344000  579.722138          12.796275  554.994942 -1.371235   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "191    -0.174668     0.000439  0.001  23.415551  0.020359 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "191       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "192   1351000  582.598279          12.805398  557.861906 -1.359662   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "192    -0.173793     0.000438  0.001  23.506067  0.020341 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "192       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "193   1358000  585.633335          12.818647  560.883658 -1.267487   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "193    -0.175652     0.000426  0.001  23.370744  0.020112 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "193       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "194   1365000  588.640727          12.828078  563.881568 -1.315534   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "194    -0.176079     0.000419  0.001  23.426384   0.01989 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "194       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "195   1372000  591.574705          12.837987  566.805581 -1.127016   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "195    -0.173828     0.000414  0.001  23.455863  0.019917 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "195       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "196   1379000  594.554818          12.847684  569.775944 -1.222254   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "196    -0.174764     0.000432  0.001  23.476457  0.019975 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "196       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "197   1386000  597.595234          12.858831  572.805161 -1.423153   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "197    -0.176011     0.000421  0.001  23.281906  0.019909 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "197       22.075056     22.243607  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "198   1393000  600.638967          12.867287  575.840388 -1.19636   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "198    -0.174834     0.000425  0.001  23.326623  0.020172 -0.009265   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "198       22.075056     22.243607  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "199   1400000  616.766218          12.877784  579.050015 -1.516323   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "199    -0.175129     0.000412  0.001  23.240807  0.019943  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "199       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "200   1407000  619.795484           12.88823  582.068778 -1.33681   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "200    -0.175965      0.00043  0.001  23.294484  0.020012  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "200       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "201   1414000  622.726648            12.8971  584.991018 -0.91392   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "201     -0.17652      0.00041  0.001  23.530222  0.019802  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "201       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "202   1421000  625.792279          12.906169  588.047528 -0.882795   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "202    -0.176643     0.000421  0.001  23.477312  0.019894  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "202       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "203   1428000  628.586403          12.916153  590.831613 -1.192134   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "203    -0.177533     0.000422  0.001  23.518552  0.019837  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "203       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "204   1435000  631.577227           12.92553  593.813004 -1.214666   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "204    -0.176059     0.000414  0.001  23.609113  0.019951  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "204       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "205   1442000  634.44413          12.936476    596.6689 -1.336827   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "205    -0.175628     0.000415  0.001  23.646162  0.019851  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "205       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "206   1449000  637.342872          12.946904  599.557156 -1.252249   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "206    -0.177666     0.000438  0.001  23.420535   0.02021  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "206       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "207   1456000  640.150877           12.95689  602.355115 -1.310312   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "207     -0.17663     0.000418  0.001  23.349385  0.020001  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "207       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "208   1463000  643.022387          12.966968  605.216488 -1.461538   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "208    -0.178708     0.000423  0.001  23.358734  0.020009  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "208       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "209   1470000  645.83508          12.976396  608.019702 -1.281476   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "209    -0.177264     0.000403  0.001  23.483027  0.019688  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "209       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "210   1477000  648.506573          12.989691  610.677817 -1.025816   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "210    -0.178433      0.00041  0.001  23.513547   0.01973  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "210       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "211   1484000  651.240987          13.008511  613.393306 -1.17103   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "211    -0.176958     0.000424  0.001  23.449824  0.020083  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "211       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "212   1491000  654.301839          13.023439  616.439156 -1.136387   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "212    -0.177395     0.000431  0.001  23.553815  0.019968  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "212       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "213   1498000  657.091141          13.034303  619.217543 -1.121735   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "213    -0.178474     0.000398  0.001  23.568092  0.019613  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "213       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "214   1505000  659.697198          13.043457  621.814399 -1.159503   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "214    -0.178324     0.000427  0.001  23.572746  0.020055  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "214       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "215   1512000  662.480104          13.053348  624.587364 -1.124244   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "215     -0.17673     0.000432  0.001  23.484426  0.020146  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "215       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "216   1519000  665.310454          13.062311  627.408686 -1.363177   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "216    -0.177282     0.000428  0.001  23.452869  0.019932  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "216       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "217   1526000  668.221508          13.072872  630.309119 -1.324946   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "217    -0.177514     0.000431  0.001  23.368475  0.020215  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "217       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "218   1533000  670.880166          13.082847   632.95775 -1.315333   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "218    -0.176905     0.000411  0.001  23.543992  0.019858  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "218       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "219   1540000  673.620337          13.092141   635.68858 -1.199275   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "219    -0.177062     0.000418  0.001  23.406486   0.01977  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "219       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "220   1547000  676.42522          13.101817  638.483731 -1.536898   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "220    -0.177319     0.000423  0.001  23.488948  0.019921  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "220       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "221   1554000  679.284219          13.112226   641.33226 -1.277726   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "221    -0.178655     0.000412  0.001  23.475052    0.0197  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "221       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "222   1561000  681.968083           13.12544  644.002849 -1.245057   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "222    -0.176377     0.000424  0.001  23.520955  0.019999  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "222       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "223   1568000  684.724759          13.136171  646.748738 -1.409798   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "223    -0.178208     0.000424  0.001  23.535211  0.020087  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "223       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "224   1575000  687.594986          13.146037  649.609046 -1.282687   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "224    -0.176419     0.000418  0.001  23.446771  0.019856  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "224       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "225   1582000  690.39323          13.155488  652.397791 -1.158124   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "225    -0.178536     0.000419  0.001  23.461743   0.01998  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "225       26.385223     22.819235  \n",
      "     timestep      time  collect_wait_time  train_time     score  policy_loss  \\\n",
      "226   1589000  693.1492          13.165293  655.143905 -1.097491      -0.1766   \n",
      "\n",
      "     qvalue_loss  alpha   entropy  td_error    return  episode_length  \\\n",
      "226     0.000413  0.001  23.46814  0.019966  0.008544       26.385223   \n",
      "\n",
      "     eval_entropy  \n",
      "226     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "227   1596000  695.844407          13.175242  657.829106 -1.31448   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "227    -0.179222     0.000407  0.001  23.449251  0.019726  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "227       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "228   1603000  698.800235          13.187387  660.772733 -1.124062   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "228    -0.178742     0.000419  0.001  23.543681  0.019845  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "228       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "229   1610000  701.557617          13.198892  663.518553 -1.159159   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "229    -0.177655     0.000421  0.001  23.660268  0.019986  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "229       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "230   1617000  704.416013          13.208172  666.367617 -1.323735   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "230    -0.177524     0.000417  0.001  23.670874  0.019733  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "230       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "231   1624000  707.376899          13.218188  669.318435 -1.011865   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "231    -0.178425     0.000416  0.001  23.666887  0.019838  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "231       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "232   1631000  710.382803          13.228384  672.314086 -1.237576   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "232    -0.178648     0.000411  0.001  23.80894  0.019592  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "232       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "233   1638000  713.280426          13.237246  675.202786 -1.306309   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "233     -0.17949     0.000401  0.001  23.699327  0.019563  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "233       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "234   1645000  715.98654          13.259236  677.886853 -1.083994   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "234    -0.178162     0.000414  0.001  23.492612  0.019866  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "234       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "235   1652000  718.827102          13.269159  680.717435 -1.220036   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "235    -0.178351     0.000409  0.001  23.401226  0.019798  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "235       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "236   1659000  721.683167          13.279026  683.563561 -1.108199   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "236    -0.177284     0.000413  0.001  23.54918  0.019846  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "236       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "237   1666000  724.351803          13.288018  686.223158 -1.256432   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "237    -0.178744     0.000408  0.001  23.678907  0.019789  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "237       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "238   1673000  727.128727          13.297576  688.990467 -1.122061   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "238    -0.178247     0.000419  0.001  23.707106  0.019737  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "238       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "239   1680000  730.037325          13.306892  691.889691 -1.166058   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "239    -0.178728     0.000399  0.001  23.691864  0.019451  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "239       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "240   1687000  732.930772          13.317701  694.772272 -1.297342   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "240    -0.176422     0.000409  0.001  23.582544  0.019805  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "240       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "241   1694000  735.650683          13.327095  697.482733 -1.203414   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "241    -0.178466     0.000413  0.001  23.615993  0.019739  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "241       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "242   1701000  738.364814           13.33615   700.18775 -1.398901   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "242    -0.179181     0.000423  0.001  23.630178  0.019964  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "242       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "243   1708000  741.249026          13.346543  703.061511 -1.22282   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "243    -0.176997     0.000399  0.001  23.531368   0.01958  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "243       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "244   1715000  744.178293          13.356182  705.981081 -1.440524   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "244    -0.178826     0.000407  0.001  23.494991  0.019583  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "244       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "245   1722000  746.945895          13.365793  708.739007 -1.212499   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "245    -0.178666     0.000396  0.001  23.642983  0.019529  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "245       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "246   1729000  749.615514          13.377306  711.397062 -1.16662   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "246    -0.178665     0.000401  0.001  23.697428  0.019523  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "246       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "247   1736000  752.534527          13.387014  714.306313 -0.84183   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "247    -0.180231     0.000397  0.001  23.768557  0.019442  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "247       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "248   1743000  755.742458          13.398217  717.502985 -1.031164   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "248     -0.17935      0.00042  0.001  23.570707    0.0198  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "248       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "249   1750000  758.746821          13.417044  720.488461 -1.116636   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "249    -0.177565     0.000394  0.001  23.579673  0.019456  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "249       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "250   1757000  761.527007          13.427574   723.25806 -1.034914   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "250    -0.179584     0.000399  0.001  23.565305   0.01943  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "250       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "251   1764000  764.407476          13.438071  726.127981 -1.299306   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "251    -0.178534       0.0004  0.001  23.595585  0.019432  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "251       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "252   1771000  767.227244          13.448653   728.93711 -1.087205   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "252     -0.17701     0.000399  0.001  23.615926  0.019498  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "252       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "253   1778000  770.060937          13.458351  731.761054 -0.984979   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "253    -0.178614     0.000394  0.001  23.656362    0.0195  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "253       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "254   1785000  772.882389          13.468111  734.572693 -1.282763   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "254    -0.179762     0.000388  0.001  23.672722  0.019342  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "254       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "255   1792000  775.706144          13.479807  737.384703 -1.108074   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "255    -0.178531     0.000423  0.001  23.701372  0.019825  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "255       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "256   1799000  778.395752          13.491019  740.063045 -1.26815   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "256    -0.180125     0.000404  0.001  23.678966  0.019636  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "256       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time   score  policy_loss  \\\n",
      "257   1806000  781.175277          13.501456  742.832064 -1.1172    -0.180227   \n",
      "\n",
      "     qvalue_loss  alpha    entropy  td_error    return  episode_length  \\\n",
      "257     0.000396  0.001  23.499724  0.019379  0.008544       26.385223   \n",
      "\n",
      "     eval_entropy  \n",
      "257     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "258   1813000  784.216108          13.516992    745.8573 -1.208165   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "258    -0.178583     0.000399  0.001  23.615428  0.019564  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "258       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "259   1820000  787.075014          13.529011  748.704135 -0.971709   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "259    -0.179856      0.00039  0.001  23.632487  0.019189  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "259       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "260   1827000  789.794585          13.539035  751.413633 -1.219393   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "260    -0.178961     0.000392  0.001  23.724756   0.01936  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "260       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "261   1834000  792.59915          13.548403  754.208778 -1.113432   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "261    -0.177329     0.000403  0.001  23.725964  0.019593  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "261       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "262   1841000  795.463303           13.55743  757.063852 -1.177183   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "262    -0.180403     0.000386  0.001  23.549624   0.01931  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "262       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "263   1848000  798.161177          13.567278  759.751821 -1.341833   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "263    -0.180045     0.000397  0.001  23.694096  0.019326  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "263       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "264   1855000  800.833326          13.577485  762.413708 -1.151113   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "264    -0.179545     0.000404  0.001  23.635282  0.019525  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "264       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "265   1862000  803.631999          13.586399  765.203413 -1.268676   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "265    -0.179803      0.00038  0.001  23.736959  0.019059  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "265       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "266   1869000  806.888755          13.596597  768.449915 -1.150079   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "266    -0.179517     0.000393  0.001  23.954332  0.019419  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "266       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "267   1876000  809.62816          13.606771  771.179092 -1.311065   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "267    -0.180704       0.0004  0.001  23.727383  0.019336  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "267       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "268   1883000  812.375866          13.616635   773.91688 -1.166892   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "268    -0.178733     0.000402  0.001  23.779771  0.019492  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "268       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "269   1890000  815.195583          13.626514  776.726642 -1.168148   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "269     -0.17838     0.000397  0.001  23.590669  0.019412  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "269       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "270   1897000  818.054785          13.636718  779.575585 -1.239636   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "270    -0.179448     0.000391  0.001  23.686023  0.019222  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "270       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "271   1904000  820.75839          13.646304  782.269554 -1.060795   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "271    -0.179541     0.000387  0.001  23.746136  0.019205  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "271       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "272   1911000  823.461667          13.655404  784.963681 -1.07589   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "272    -0.179188     0.000397  0.001  23.72599  0.019366  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "272       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "273   1918000  826.19168          13.664874  787.684169 -1.147842   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "273     -0.17885     0.000391  0.001  23.610563  0.019296  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "273       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "274   1925000  829.005051          13.674327  790.488036 -1.090663   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "274    -0.179389     0.000377  0.001  23.60867  0.019083  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "274       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "275   1932000  831.776291          13.683127  793.250431 -0.704175   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "275    -0.180662     0.000392  0.001  23.691347   0.01927  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "275       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "276   1939000  834.72276          13.694602  796.185372 -1.117414   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "276    -0.180159     0.000388  0.001  23.699241  0.019272  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "276       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "277   1946000  837.48102          13.704592  798.933585 -1.103747   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "277    -0.180179      0.00039  0.001  23.823876  0.019284  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "277       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "278   1953000  840.368838            13.7147  801.811244 -0.823507   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "278    -0.181064     0.000368  0.001  23.763469  0.018899  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "278       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "279   1960000  843.221014          13.724336   804.65371 -0.966391   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "279    -0.180007     0.000382  0.001  23.605015  0.019122  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "279       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "280   1967000  846.338163          13.733371  807.761775 -1.188092   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "280    -0.180648     0.000371  0.001  23.715989  0.018823  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "280       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "281   1974000  849.171967          13.743656  810.585239 -1.269372   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "281    -0.180303     0.000378  0.001  23.750056  0.018927  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "281       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "282   1981000  852.006267           13.75404  813.409104 -1.091885   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "282    -0.179606     0.000379  0.001  23.750866   0.01899  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "282       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "283   1988000  854.920591          13.763228  816.314187 -1.240352   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "283     -0.17949     0.000379  0.001  23.796836  0.018996  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "283       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "284   1995000  857.787446          13.772079  819.172131 -0.89175   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "284    -0.180187     0.000387  0.001  23.458036   0.01904  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "284       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "285   2002000  860.483266          13.781583  821.858387 -0.924135   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "285    -0.178293     0.000395  0.001  23.817362  0.019348  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "285       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "286   2009000  863.230871          13.790071  824.597448 -1.146459   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "286    -0.178628     0.000378  0.001  23.949404  0.018975  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "286       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "287   2016000  866.023715          13.799088  827.381222 -0.601529   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "287    -0.180359     0.000375  0.001  23.986714  0.018851  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "287       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "288   2023000  868.893161          13.808302  830.241404 -0.868384   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "288    -0.180733     0.000386  0.001  23.826378   0.01899  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "288       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "289   2030000  871.575794          13.817463  832.914823 -1.328735   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "289    -0.179678     0.000368  0.001  23.643568  0.018803  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "289       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "290   2037000  874.432991          13.829892  835.759483 -0.951575   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "290    -0.179553     0.000375  0.001  23.722005  0.018829  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "290       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "291   2044000  877.396664          13.842067  838.710933 -1.188038   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "291    -0.179073     0.000374  0.001  23.821115  0.018885  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "291       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "292   2051000  880.315486          13.852028  841.619738 -1.155553   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "292    -0.180781     0.000376  0.001  23.846465  0.018971  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "292       26.385223     22.819235  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "293   2058000  883.28068          13.861802  844.575105 -1.026854   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "293    -0.180819     0.000378  0.001  23.756239  0.018864  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "293       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "294   2065000  886.017779          13.885912  847.288043 -0.999617   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "294    -0.181776     0.000382  0.001  23.529138  0.018976  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "294       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "295   2072000  889.137796          13.896491  850.397419 -0.984625   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "295    -0.180119     0.000375  0.001  23.622653  0.018888  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "295       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "296   2079000  892.082027          13.909873  853.328209 -0.864669   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "296    -0.181748     0.000378  0.001  23.850893  0.018842  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "296       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "297   2086000  894.812339          13.921211  856.047127 -0.74218   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "297    -0.180937     0.000385  0.001  23.776749   0.01906  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "297       26.385223     22.819235  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "298   2093000  897.588314          13.930096  858.814165 -1.02782   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "298    -0.180561      0.00037  0.001  23.718364  0.018748  0.008544   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "298       26.385223     22.819235  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "299   2100000  913.650318          13.941284   861.81834 -1.034093   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "299    -0.179566     0.000374  0.001  23.860295  0.018905  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "299       27.100271     23.972837  \n",
      "     timestep       time  collect_wait_time  train_time     score  \\\n",
      "300   2107000  916.93648          13.953078  865.092645 -0.935737   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "300    -0.180885     0.000371  0.001  23.713293  0.018768  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "300       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "301   2114000  920.064396          13.963978  868.209598 -0.696524   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "301    -0.181527     0.000357  0.001  23.676685  0.018555  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "301       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "302   2121000  923.264701          14.211808  871.162014 -0.849282   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "302    -0.181856      0.00037  0.001  23.741528  0.018612  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "302       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "303   2128000  926.203815          14.221337  874.091526 -0.870232   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "303     -0.18235     0.000364  0.001  23.723025  0.018612  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "303       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "304   2135000  929.493534          14.232728    877.3698 -0.812108   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "304     -0.18115     0.000382  0.001  23.84212   0.01882  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "304       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "305   2142000  932.462404          14.242123  880.329224 -0.93832   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "305    -0.182193      0.00038  0.001  23.698668  0.018867  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "305       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "306   2149000  935.398956           14.25278  883.255068 -0.801764   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "306    -0.181426     0.000376  0.001  23.70056  0.018861  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "306       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "307   2156000  938.443897          14.262418  886.290316 -1.005673   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "307    -0.182603      0.00037  0.001  23.750128  0.018736  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "307       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "308   2163000  941.686769          14.274124  889.521431 -1.112737   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "308     -0.18178     0.000384  0.001  23.871015  0.018858  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "308       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "309   2170000  944.703246          14.283582  892.528399 -1.141268   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "309    -0.181185     0.000372  0.001  23.824371  0.018818  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "309       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "310   2177000  947.564579          14.294694  895.378558 -1.168432   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "310    -0.181856     0.000363  0.001  23.898458  0.018684  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "310       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "311   2184000  950.591678          14.305424  898.394872 -0.877929   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "311      -0.1811     0.000364  0.001  23.700245  0.018552  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "311       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "312   2191000  953.667342          14.315351  901.460551 -0.919482   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "312    -0.181523     0.000376  0.001  23.743958  0.018693  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "312       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "313   2198000  956.698623          14.326655  904.480474 -0.904814   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "313    -0.183042     0.000374  0.001  23.865683  0.018634  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "313       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "314   2205000  959.579561          14.335539   907.35248 -1.054335   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "314    -0.183522     0.000359  0.001  23.876479  0.018476  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "314       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "315   2212000  962.336854           14.34493   910.10032 -1.109404   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "315    -0.181323     0.000371  0.001  23.916344  0.018763  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "315       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "316   2219000  965.095063          14.353944  912.849463 -1.000488   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "316    -0.183352     0.000361  0.001  23.705298  0.018569  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "316       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "317   2226000  967.861228           14.36402   915.60549 -0.971077   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "317    -0.181837     0.000369  0.001  23.624068  0.018608  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "317       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time  score  policy_loss  \\\n",
      "318   2233000  970.766731          14.374649  918.500315 -1.223    -0.183256   \n",
      "\n",
      "     qvalue_loss  alpha    entropy  td_error    return  episode_length  \\\n",
      "318      0.00037  0.001  23.529431  0.018648  0.018342       27.100271   \n",
      "\n",
      "     eval_entropy  \n",
      "318     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "319   2240000  973.635594          14.383547  921.360227 -0.906338   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "319     -0.18266     0.000358  0.001  23.769179  0.018534  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "319       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time   score  policy_loss  \\\n",
      "320   2247000  976.404633          14.393709  924.119051 -0.8064     -0.18296   \n",
      "\n",
      "     qvalue_loss  alpha    entropy  td_error    return  episode_length  \\\n",
      "320     0.000362  0.001  23.726902  0.018543  0.018342       27.100271   \n",
      "\n",
      "     eval_entropy  \n",
      "320     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "321   2254000  979.134501           14.40332  926.839253 -0.859421   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "321     -0.18282     0.000367  0.001  23.726551  0.018696  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "321       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "322   2261000  982.022819          14.414238  929.716593 -0.97948   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "322    -0.182762     0.000363  0.001  23.81381  0.018587  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "322       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "323   2268000  984.980324          14.423311  932.664977 -1.151672   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "323    -0.182532     0.000369  0.001  23.77036  0.018638  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "323       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "324   2275000  987.705074          14.435672  935.377305 -1.07622   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "324    -0.183761     0.000358  0.001  23.836631  0.018546  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "324       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "325   2282000  990.553715           14.44619   938.21538 -0.966281   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "325    -0.182125     0.000365  0.001  23.984146  0.018515  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "325       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time   score  policy_loss  \\\n",
      "326   2289000  993.426706          14.455153  941.079358 -0.8322    -0.183915   \n",
      "\n",
      "     qvalue_loss  alpha    entropy  td_error    return  episode_length  \\\n",
      "326     0.000364  0.001  23.757613  0.018561  0.018342       27.100271   \n",
      "\n",
      "     eval_entropy  \n",
      "326     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "327   2296000  996.246556          14.466212   943.88809 -0.813346   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "327    -0.182499     0.000355  0.001  23.936596  0.018376  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "327       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "328   2303000  998.887262          14.474845  946.520108 -1.049528   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "328    -0.182826      0.00036  0.001  23.845122  0.018534  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "328       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "329   2310000  1001.630329          14.484021  949.253942 -0.970629   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "329    -0.183498     0.000361  0.001  23.574678   0.01856  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "329       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "330   2317000  1004.409351          14.494364  952.022569 -1.114981   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "330    -0.183333     0.000365  0.001  23.889173    0.0185  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "330       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "331   2324000  1007.086082          14.503802  954.689812 -0.917394   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "331    -0.184798     0.000361  0.001  23.847642  0.018436  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "331       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "332   2331000  1009.824303          14.513635  957.418149 -1.182139   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "332    -0.185438     0.000352  0.001  23.715479  0.018293  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "332       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "333   2338000  1012.626636          14.522937  960.211128 -1.107785   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "333    -0.182838     0.000358  0.001  23.738913  0.018467  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "333       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "334   2345000  1015.536878           14.54077  963.103429 -1.085433   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "334     -0.18397     0.000364  0.001  23.698256   0.01854  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "334       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "335   2352000  1018.377232          14.550753  965.933748 -0.745488   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "335    -0.184499     0.000357  0.001  23.771414  0.018232  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "335       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "336   2359000  1021.241617          14.560275  968.788557 -1.033686   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "336    -0.184103     0.000349  0.001  23.782057  0.018225  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "336       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "337   2366000  1023.989308          14.569984  971.526487 -0.982966   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "337    -0.184457     0.000359  0.001  23.889056  0.018479  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "337       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "338   2373000  1026.761556          14.580715  974.287951 -0.630312   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "338    -0.182552     0.000362  0.001  23.885776  0.018482  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "338       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "339   2380000  1029.539019          14.592179  977.053894 -1.188589   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "339    -0.185618     0.000354  0.001  23.840836  0.018289  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "339       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "340   2387000  1032.254429          14.601305  979.760128 -0.726694   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "340    -0.187065     0.000366  0.001  23.859491  0.018363  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "340       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "341   2394000  1035.113796          14.610856   982.60989 -1.021625   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "341    -0.185175     0.000357  0.001  23.829536  0.018184  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "341       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "342   2401000  1037.930333          14.620847  985.416381 -1.154407   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "342    -0.185251     0.000361  0.001  23.781372  0.018389  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "342       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "343   2408000  1040.646016          14.630656  988.122203 -0.908957   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "343    -0.183752     0.000353  0.001  23.862704  0.018305  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "343       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "344   2415000  1043.426483          14.641079  990.892186 -1.264885   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "344    -0.184551     0.000364  0.001  23.845245  0.018448  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "344       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time  train_time    score  \\\n",
      "345   2422000  1046.13346          14.650869  993.589322 -0.67642   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "345    -0.184396     0.000352  0.001  23.724169  0.018283  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "345       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "346   2429000  1049.079242          14.660434  996.525482 -0.823521   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "346    -0.184631     0.000355  0.001  23.571512  0.018233  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "346       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "347   2436000  1051.800746          14.671551  999.235812 -1.125329   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "347    -0.185565     0.000352  0.001  23.633537  0.018218  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "347       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "348   2443000  1054.720065          14.682568  1002.144053 -0.946053   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "348    -0.186981     0.000346  0.001  23.884489  0.018067  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "348       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "349   2450000  1057.613069          14.691413  1005.028158 -0.846286   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "349    -0.184364     0.000354  0.001  23.641754  0.018221  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "349       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "350   2457000  1060.513762           14.70056  1007.91964 -0.976466   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "350    -0.183493     0.000344  0.001  23.743277  0.018063  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "350       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "351   2464000  1063.492905          14.712231  1010.887056 -0.889421   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "351    -0.184893     0.000345  0.001  23.853574  0.018121  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "351       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "352   2471000  1066.332325          14.723786  1013.714867 -0.946745   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "352    -0.185993     0.000345  0.001  23.885371  0.017991  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "352       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "353   2478000  1069.076905          14.733378   1016.4498 -1.250718   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "353    -0.187859     0.000357  0.001  23.821149  0.018209  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "353       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "354   2485000  1071.876301           14.74248  1019.240043 -0.78335   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "354    -0.184225     0.000351  0.001  23.655263  0.018296  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "354       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "355   2492000  1074.700852          14.751305  1022.055716 -0.906783   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "355    -0.186999     0.000352  0.001  23.829218  0.018204  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "355       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "356   2499000  1077.529666           14.76311  1024.872671 -0.374211   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "356    -0.184542     0.000347  0.001  23.703021  0.018103  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "356       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "357   2506000  1080.45627          14.773764  1027.788556 -1.087023   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "357    -0.184809     0.000354  0.001  23.770407  0.018133  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "357       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "358   2513000  1083.412421          14.783512  1030.734908 -1.186524   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "358    -0.186001     0.000356  0.001  23.746608  0.018234  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "358       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "359   2520000  1086.371296          14.795203  1033.682038 -1.094158   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "359    -0.186874     0.000347  0.001  23.687529  0.018064  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "359       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "360   2527000  1089.120722          14.807596  1036.419017 -1.259923   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "360    -0.185436     0.000349  0.001  23.403035  0.018102  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "360       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "361   2534000  1091.840748          14.817725  1039.128854 -0.743336   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "361    -0.186544     0.000353  0.001  23.794818  0.018141  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "361       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "362   2541000  1094.71387          14.827113  1041.992532 -0.712446   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "362    -0.186977     0.000344  0.001  23.994855  0.017925  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "362       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "363   2548000  1097.663032          14.837739  1044.931009 -0.943145   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "363    -0.188195     0.000339  0.001  23.813613  0.017838  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "363       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "364   2555000  1100.499892          14.848603  1047.756949 -0.768188   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "364    -0.187201     0.000351  0.001  23.821591  0.018134  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "364       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "365   2562000  1103.286079          14.859571  1050.532111 -0.916566   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "365    -0.186699     0.000352  0.001  23.692486  0.017958  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "365       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "366   2569000  1106.045086          14.869955  1053.280681 -0.902274   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "366    -0.186417     0.000344  0.001  23.651007  0.018022  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "366       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "367   2576000  1109.03695          14.878414  1056.264035 -0.923143   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "367    -0.186244     0.000345  0.001  23.77223  0.017912  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "367       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "368   2583000  1111.915761          14.887428  1059.133779 -0.93188   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "368    -0.184596     0.000337  0.001  23.882008  0.017919  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "368       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "369   2590000  1114.756668          14.898901  1061.963158 -0.666445   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "369    -0.187627     0.000354  0.001  23.874204   0.01819  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "369       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "370   2597000  1117.557785          14.908542  1064.754574 -1.178556   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "370    -0.185989     0.000341  0.001  23.779099  0.017768  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "370       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "371   2604000  1120.458742          14.917436  1067.646587 -0.907879   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "371    -0.186701     0.000348  0.001  23.738903  0.018134  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "371       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "372   2611000  1123.305633          14.927468  1070.48339 -1.019196   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "372    -0.187025     0.000347  0.001  23.699153  0.018113  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "372       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "373   2618000  1126.147666          14.939016  1073.313805 -0.819259   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "373    -0.188113     0.000345  0.001  23.701662   0.01797  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "373       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "374   2625000  1129.149661          14.949756  1076.305008 -1.042377   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "374     -0.18707     0.000347  0.001  23.755046  0.018042  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "374       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "375   2632000  1132.063904           14.95892  1079.210032 -0.638527   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "375    -0.188057     0.000346  0.001  23.825223  0.017959  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "375       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "376   2639000  1134.760147          14.969126  1081.896016 -0.547202   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "376    -0.187683     0.000352  0.001  23.889099  0.017993  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "376       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "377   2646000  1137.477252          14.978291   1084.6039 -0.790575   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "377      -0.1876     0.000353  0.001  23.891285  0.018147  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "377       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "378   2653000  1140.352286          14.989375  1087.467797 -1.048832   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha  entropy  td_error    return  \\\n",
      "378    -0.186461     0.000349  0.001  23.9017  0.018035  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "378       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "379   2660000  1143.047995          14.999613  1090.153213 -0.730426   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "379    -0.189504     0.000344  0.001  23.843207   0.01787  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "379       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "380   2667000  1146.001615          15.008836  1093.097558 -1.000251   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "380    -0.185898     0.000348  0.001  23.887204  0.018086  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "380       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "381   2674000  1148.916848          15.018262  1096.003311 -0.9487   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "381    -0.187407     0.000347  0.001  23.72019  0.018016  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "381       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "382   2681000  1151.790293          15.028418  1098.866545 -0.775966   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "382    -0.189169     0.000342  0.001  23.745947  0.017882  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "382       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "383   2688000  1154.561896          15.038119  1101.628394 -1.033492   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "383    -0.187968      0.00034  0.001  23.613754  0.017738  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "383       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "384   2695000  1157.374364          15.048091  1104.430834 -0.94015   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "384     -0.18721     0.000341  0.001  23.72102  0.017845  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "384       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "385   2702000  1160.27178          15.058848  1107.317437 -0.586983   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "385    -0.188313     0.000338  0.001  23.835022  0.017726  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "385       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "386   2709000  1163.045301            15.0699  1110.079847 -0.717966   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "386    -0.189448     0.000335  0.001  23.724979  0.017723  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "386       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "387   2716000  1165.893119          15.081934  1112.915555 -1.025093   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "387    -0.188968     0.000349  0.001  23.804819  0.017975  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "387       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "388   2723000  1168.821915          15.106739  1115.819479 -0.944361   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "388    -0.188081     0.000343  0.001  23.867312  0.017961  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "388       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "389   2730000  1171.711025          15.115435  1118.69983 -0.853612   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "389    -0.187857     0.000343  0.001  23.866833  0.017802  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "389       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "390   2737000  1174.544903          15.125075  1121.524019 -0.750636   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "390    -0.188738     0.000341  0.001  23.728413  0.017812  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "390       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "391   2744000  1177.313721          15.134194  1124.283669 -1.086648   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "391    -0.189405     0.000342  0.001  23.886055  0.017821  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "391       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "392   2751000  1180.015387           15.14347  1126.976007 -1.057272   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "392    -0.189237     0.000333  0.001  23.817164  0.017646  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "392       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "393   2758000  1182.90965          15.153457  1129.860226 -1.212354   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "393    -0.188496     0.000339  0.001  23.829981  0.017718  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "393       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "394   2765000  1185.538118          15.162408  1132.479692 -1.083055   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "394    -0.188054     0.000334  0.001  24.01967   0.01764  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "394       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "395   2772000  1188.261227          15.172094  1135.193056 -0.813324   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "395    -0.187352     0.000346  0.001  23.772749   0.01793  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "395       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "396   2779000  1191.180378          15.182791  1138.101458 -0.954006   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "396    -0.189769     0.000353  0.001  23.766671  0.018131  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "396       27.100271     23.972837  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "397   2786000  1193.99774          15.192974  1140.908586 -0.974652   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "397     -0.18814     0.000345  0.001  24.097901  0.017884  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "397       27.100271     23.972837  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "398   2793000  1196.812851          15.203289  1143.71333 -0.986259   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "398    -0.187801     0.000335  0.001  23.873412  0.017671  0.018342   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "398       27.100271     23.972837  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "399   2800000  1212.378917          15.212761  1146.379683 -1.202088   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "399     -0.18943     0.000341  0.001  23.792093  0.017745  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "399       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "400   2807000  1215.170283          15.224733  1149.158935 -1.011413   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "400    -0.190367     0.000346  0.001  23.603161  0.017748  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "400       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "401   2814000  1217.923084          15.234078  1151.902337 -0.692845   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "401    -0.187988      0.00033  0.001  23.959699  0.017595  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "401       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "402   2821000  1220.899454          15.244519  1154.868191 -0.850559   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "402    -0.187948     0.000339  0.001  23.842362  0.017764  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "402       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "403   2828000  1223.840303          15.254107  1157.799397 -0.795336   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "403    -0.189374     0.000335  0.001  23.809747  0.017628  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "403       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "404   2835000  1226.612364          15.264627  1160.560881 -0.722487   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "404    -0.188582     0.000329  0.001  23.858263  0.017443  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "404       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "405   2842000  1229.528573          15.274251  1163.467416 -1.049251   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "405    -0.187947     0.000327  0.001  23.882549  0.017437  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "405       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "406   2849000  1232.364464           15.28376  1166.293749 -0.729062   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "406     -0.18831     0.000335  0.001  24.014891  0.017623  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "406       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "407   2856000  1235.278234           15.29762  1169.193609 -0.94556   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "407    -0.187668     0.000337  0.001  23.833837  0.017737  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "407       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "408   2863000  1238.097722          15.306677  1172.003988 -0.856413   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "408    -0.190616     0.000339  0.001  23.905978  0.017589  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "408       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "409   2870000  1240.942306          15.317081  1174.838116 -0.850203   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "409    -0.188732     0.000334  0.001  23.97482  0.017767  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "409       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "410   2877000  1243.806234          15.326028  1177.693038 -1.011292   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "410    -0.190416     0.000338  0.001  23.997949  0.017786  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "410       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "411   2884000  1246.663561          15.336719  1180.539621 -0.876324   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "411    -0.188175     0.000334  0.001  23.913105  0.017639  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "411       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "412   2891000  1249.538198          15.361506  1183.389403 -1.02849   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "412    -0.189384     0.000329  0.001  23.874614   0.01748  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "412       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "413   2898000  1252.313509          15.370692  1186.155474 -0.857349   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "413    -0.190062     0.000329  0.001  23.933718  0.017454  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "413       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "414   2905000  1255.138806          15.379864  1188.971546 -0.820816   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "414    -0.190072      0.00033  0.001  24.028918  0.017561  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "414       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "415   2912000  1257.916004          15.389794  1191.738759 -0.739451   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "415    -0.189822     0.000341  0.001  23.97308  0.017623  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "415       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "416   2919000  1260.683117          15.401347  1194.494262 -0.93839   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "416    -0.189758     0.000332  0.001  23.837255  0.017492  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "416       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "417   2926000  1263.545742          15.413352  1197.344822 -0.924396   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "417    -0.189443     0.000332  0.001  24.027673  0.017484  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "417       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "418   2933000  1266.415219          15.424836  1200.202765 -0.421192   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "418    -0.190874     0.000346  0.001  23.885627  0.017785  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "418       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "419   2940000  1269.156626          15.436595  1202.932354 -0.702346   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "419    -0.191688     0.000328  0.001  23.822179  0.017394  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "419       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "420   2947000  1271.90199          15.448817  1205.665436 -0.887614   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "420    -0.191049     0.000315  0.001  23.937858  0.017057  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "420       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "421   2954000  1274.658328          15.461244  1208.409294 -0.82045   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "421    -0.191923     0.000337  0.001  23.989493  0.017543  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "421       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "422   2961000  1277.581553          15.477744  1211.315963 -0.767207   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "422    -0.189441     0.000332  0.001  23.87866  0.017424  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "422       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "423   2968000  1280.306895          15.489912  1214.029086 -0.720601   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "423    -0.189155     0.000339  0.001  23.92521  0.017621  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "423       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "424   2975000  1282.922444          15.501805  1216.632685 -1.01365   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "424    -0.190171     0.000341  0.001  23.816381  0.017631  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "424       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "425   2982000  1285.68135          15.513763  1219.379578 -0.691431   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "425    -0.190711     0.000333  0.001  23.726173  0.017506  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "425       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "426   2989000  1288.561275          15.525774  1222.247439 -0.936664   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "426    -0.192188     0.000334  0.001  23.775191  0.017584  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "426       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "427   2996000  1291.245119          15.540412  1224.916594 -0.80784   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "427    -0.190794     0.000337  0.001  23.798025   0.01756  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "427       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "428   3003000  1293.906745          15.553734  1227.564843 -0.820697   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "428     -0.19206     0.000337  0.001  23.99188  0.017494  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "428       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "429   3010000  1296.661985           15.56196  1230.311805 -0.924429   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "429    -0.192993     0.000327  0.001  23.91194  0.017324  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "429       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "430   3017000  1299.658741          15.570115  1233.300345 -0.719078   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "430    -0.191972     0.000328  0.001  23.954238  0.017335  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "430       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "431   3024000  1302.515241          15.579477  1236.147422 -0.88969   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "431    -0.190602     0.000343  0.001  24.038386  0.017636  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "431       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "432   3031000  1305.28736          15.587304  1238.911663 -0.967019   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "432    -0.191999      0.00034  0.001  23.855478  0.017593  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "432       26.595745     24.020399  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "433   3038000  1308.0634          15.595206  1241.679746 -0.618558   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "433    -0.191169     0.000327  0.001  23.936733  0.017235  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "433       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "434   3045000  1310.831597          15.603327  1244.439752 -0.952235   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "434     -0.19173     0.000341  0.001  23.906219  0.017688  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "434       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "435   3052000  1313.589579          15.610558  1247.190453 -0.831463   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "435    -0.190743     0.000326  0.001  24.04431  0.017283  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "435       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "436   3059000  1316.298554          15.618103  1249.891826 -0.832126   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "436    -0.191949     0.000328  0.001  24.110831  0.017287  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "436       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "437   3066000  1319.09302          15.625728  1252.678616 -0.413541   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "437     -0.19243     0.000328  0.001  24.046269  0.017288  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "437       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "438   3073000  1321.936213          15.635017  1255.512464 -0.639911   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "438     -0.19195     0.000324  0.001  23.974389  0.017347  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "438       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "439   3080000  1324.702529          15.642638  1258.27111 -0.773071   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "439    -0.192007      0.00033  0.001  23.978027  0.017333  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "439       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "440   3087000  1327.425537          15.650458  1260.986246 -0.862842   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "440    -0.191217     0.000325  0.001  24.127478  0.017288  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "440       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "441   3094000  1330.214224          15.659068  1263.766268 -0.924076   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "441    -0.191718     0.000323  0.001  23.993949  0.017218  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "441       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "442   3101000  1333.160025          15.667039  1266.704042 -0.965743   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "442    -0.191529     0.000311  0.001  24.143068  0.016987  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "442       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "443   3108000  1335.992379          15.675257  1269.528119 -0.955164   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "443    -0.194901     0.000317  0.001  24.042648  0.017135  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "443       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "444   3115000  1338.726188          15.683684  1272.253426 -1.060826   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "444    -0.190851     0.000315  0.001  24.061957  0.017188  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "444       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "445   3122000  1341.420366          15.691632   1274.9396 -0.882682   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "445    -0.191504     0.000317  0.001  24.051329  0.017079  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "445       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "446   3129000  1344.36792          15.700436  1277.878295 -1.074948   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "446    -0.193102     0.000315  0.001  23.931686  0.017056  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "446       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "447   3136000  1347.482885          15.707932  1280.985711 -0.420389   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "447    -0.192097     0.000319  0.001  23.755604  0.017216  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "447       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "448   3143000  1350.302989          15.717652  1283.796035 -0.971795   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "448     -0.19231      0.00031  0.001  23.87882  0.016926  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "448       26.595745     24.020399  \n",
      "     timestep     time  collect_wait_time   train_time     score  policy_loss  \\\n",
      "449   3150000  1353.14          15.725346  1286.625295 -1.074304    -0.191966   \n",
      "\n",
      "     qvalue_loss  alpha    entropy  td_error    return  episode_length  \\\n",
      "449     0.000304  0.001  24.069515  0.016833  0.010258       26.595745   \n",
      "\n",
      "     eval_entropy  \n",
      "449     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "450   3157000  1356.026362          15.732565  1289.504384 -1.095031   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "450    -0.193231     0.000321  0.001  24.02487   0.01722  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "450       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "451   3164000  1358.810267          15.740906  1292.279894 -0.803316   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "451    -0.192341     0.000315  0.001  24.073371  0.017072  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "451       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time    score  \\\n",
      "452   3171000  1361.50837          15.749942  1294.968908 -0.81889   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "452    -0.192025     0.000327  0.001  24.10244  0.017123  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "452       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "453   3178000  1364.304161          15.757617  1297.756969 -0.763794   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "453    -0.193439     0.000312  0.001  23.919703  0.016934  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "453       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "454   3185000  1367.178761          15.765044  1300.624085 -0.94875   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "454    -0.193249     0.000318  0.001  23.995154  0.017078  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "454       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "455   3192000  1369.999094          15.772686  1303.436722 -0.659389   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha  entropy  td_error    return  \\\n",
      "455    -0.193752     0.000315  0.001  23.8585  0.017028  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "455       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "456   3199000  1372.821975          15.781655  1306.250583 -0.499596   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "456    -0.194028     0.000304  0.001  23.851744  0.016756  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "456       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "457   3206000  1375.590081          15.789512  1309.010781 -0.916755   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "457    -0.191449     0.000307  0.001  23.823763   0.01685  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "457       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "458   3213000  1378.461213          15.797728  1311.873641 -0.532071   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "458    -0.193902       0.0003  0.001  23.938765  0.016674  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "458       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "459   3220000  1381.284776          15.806788  1314.688087 -0.652385   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "459    -0.193311     0.000309  0.001  23.955585   0.01684  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "459       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "460   3227000  1384.188974          15.814156  1317.584856 -0.799607   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "460    -0.193284     0.000312  0.001  24.078487  0.016834  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "460       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "461   3234000  1387.391218          15.821473  1320.779696 -0.929125   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "461    -0.194468     0.000305  0.001  23.872101  0.016769  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "461       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "462   3241000  1390.591462              15.83  1323.971355 -0.710641   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "462    -0.194114     0.000311  0.001  23.960081  0.016803  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "462       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "463   3248000  1393.63372          15.837949  1327.005609 -0.633722   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "463    -0.194509       0.0003  0.001  24.048244   0.01665  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "463       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "464   3255000  1396.658549          15.846036  1330.02229 -0.694127   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "464    -0.194725     0.000318  0.001  23.907897  0.016976  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "464       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "465   3262000  1399.920742          15.853947  1333.276525 -0.54122   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "465    -0.195174     0.000303  0.001  23.908697  0.016641  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "465       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "466   3269000  1403.023836          15.862119  1336.371392 -0.698645   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "466    -0.195013     0.000302  0.001  24.010273   0.01672  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "466       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "467   3276000  1405.816756          15.868979  1339.157406 -0.818807   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "467    -0.194269     0.000313  0.001  24.01836  0.016843  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "467       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "468   3283000  1408.65046          15.877177  1341.982855 -0.645415   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "468    -0.196721     0.000302  0.001  23.92985  0.016635  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "468       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "469   3290000  1411.411684          15.884549  1344.736657 -0.531937   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "469    -0.193946     0.000306  0.001  24.190162  0.016678  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "469       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "470   3297000  1414.197098          15.892888  1347.513675 -0.67372   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "470    -0.194982      0.00029  0.001  24.040719  0.016415  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "470       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "471   3304000  1416.978215          15.900501  1350.287129 -0.725842   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "471    -0.195531     0.000306  0.001  23.978612  0.016735  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "471       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "472   3311000  1419.724378          15.911752  1353.021985 -0.711096   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "472    -0.197255     0.000299  0.001  24.078492  0.016558  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "472       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "473   3318000  1422.584646           15.91932  1355.874632 -0.755056   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "473    -0.195837     0.000291  0.001  24.096398  0.016456  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "473       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "474   3325000  1425.531981          15.928365  1358.812864 -0.700944   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "474    -0.195007     0.000302  0.001  24.329375  0.016514  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "474       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "475   3332000  1428.238683          15.936087  1361.511797 -0.630465   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "475    -0.196376     0.000284  0.001  24.112196  0.016301  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "475       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "476   3339000  1431.016391          15.943712  1364.281825 -0.800824   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "476    -0.196228     0.000297  0.001  23.984204  0.016479  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "476       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "477   3346000  1433.716037          15.951403  1366.973728 -1.01374   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "477    -0.195715     0.000297  0.001  24.065409  0.016448  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "477       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "478   3353000  1436.58053          15.960143  1369.829429 -0.608803   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "478    -0.195008       0.0003  0.001  24.189867  0.016635  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "478       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "479   3360000  1439.375175          15.968394  1372.61577 -0.841871   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "479      -0.1959       0.0003  0.001  23.957684  0.016584  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "479       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "480   3367000  1442.028725          15.975275  1375.262389 -0.951777   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "480    -0.195186     0.000306  0.001  23.912541  0.016462  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "480       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "481   3374000  1444.768995          15.983318  1377.994563 -0.934908   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "481    -0.194433     0.000297  0.001  24.06915  0.016586  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "481       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "482   3381000  1447.54142          15.991332  1380.758925 -0.816966   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "482    -0.194749     0.000296  0.001  24.198772  0.016487  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "482       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "483   3388000  1450.376057          15.998854  1383.585979 -0.736819   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "483     -0.19483     0.000297  0.001  24.264383  0.016481  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "483       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "484   3395000  1453.050766          16.006457  1386.253036 -0.863088   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "484    -0.197869     0.000296  0.001  23.985889  0.016416  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "484       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "485   3402000  1455.673144          16.013954  1388.867868 -0.750282   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "485    -0.196013     0.000284  0.001  24.102646  0.016213  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "485       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "486   3409000  1458.537949           16.02346  1391.723117 -0.620756   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "486    -0.195996     0.000293  0.001  24.138634  0.016364  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "486       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "487   3416000  1461.30946          16.030982  1394.487055 -0.747068   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "487    -0.196181     0.000293  0.001  24.119372  0.016367  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "487       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "488   3423000  1464.12346          16.038163  1397.293821 -0.741053   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "488    -0.196268     0.000292  0.001  24.223947  0.016418  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "488       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "489   3430000  1466.908531          16.046418  1400.070574 -0.838431   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "489    -0.195981     0.000288  0.001  24.206035  0.016315  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "489       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "490   3437000  1469.716568          16.056396  1402.868582 -0.498498   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "490    -0.197469     0.000288  0.001  24.355875  0.016274  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "490       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "491   3444000  1472.442134          16.064161  1405.586329 -0.688196   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "491    -0.196949     0.000292  0.001  24.157085  0.016204  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "491       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "492   3451000  1475.180243          16.072357  1408.316185 -0.535291   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "492    -0.195917     0.000289  0.001  24.232001  0.016242  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "492       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "493   3458000  1477.998457          16.079407  1411.127301 -0.783562   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "493    -0.196163     0.000297  0.001  24.199854  0.016383  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "493       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "494   3465000  1480.833962          16.087691  1413.954465 -0.740073   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "494    -0.195957     0.000288  0.001  24.208791  0.016189  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "494       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "495   3472000  1483.577308          16.096213  1416.689235 -0.49763   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "495    -0.197677     0.000292  0.001  24.197948  0.016358  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "495       26.595745     24.020399  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "496   3479000  1486.28629          16.104571  1419.389799 -0.896691   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "496    -0.196711     0.000292  0.001  24.241054  0.016333  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "496       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "497   3486000  1489.070303          16.112658  1422.165673 -0.9593   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "497    -0.198093     0.000289  0.001  24.258857  0.016116  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "497       26.595745     24.020399  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "498   3493000  1491.986403          16.119988  1425.074383 -0.923671   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "498    -0.196772     0.000283  0.001  24.152102   0.01612  0.010258   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "498       26.595745     24.020399  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "499   3500000  1507.633051          16.127705  1427.795154 -0.769447   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "499    -0.197645     0.000278  0.001  24.372351  0.016021 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "499       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "500   3507000  1510.871095          16.136635  1431.024214 -0.431446   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "500    -0.198885     0.000285  0.001  24.339123  0.016158 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "500       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "501   3514000  1513.803487          16.149011  1433.944179 -0.971783   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "501    -0.198109     0.000279  0.001  24.128434  0.015932 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "501       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "502   3521000  1516.771588          16.156621  1436.904617 -0.894409   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "502     -0.19784     0.000287  0.001  24.135994  0.016052 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "502       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "503   3528000  1519.572267          16.165207  1439.696654 -0.947203   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "503    -0.197071     0.000277  0.001  24.335539  0.015931 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "503       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "504   3535000  1522.45107          16.173485  1442.567115 -0.760487   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "504    -0.199068     0.000279  0.001  24.211216  0.015922 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "504       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "505   3542000  1525.128589          16.181055  1445.237012 -0.57133   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "505    -0.199201     0.000287  0.001  24.274118  0.016116 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "505       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "506   3549000  1527.888982          16.190008  1447.988395 -0.787689   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "506     -0.19917     0.000282  0.001  24.142548  0.015994 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "506       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "507   3556000  1530.837234          16.198341  1450.928268 -0.793738   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "507    -0.197263      0.00028  0.001  24.164047  0.015947 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "507       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "508   3563000  1533.73028          16.206455  1453.81314 -0.835281   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "508    -0.195981     0.000282  0.001  24.294316  0.016099 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "508       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "509   3570000  1536.505018          16.214952  1456.579329 -1.106247   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "509    -0.197805     0.000268  0.001  24.132488  0.015622 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "509       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "510   3577000  1539.206087          16.223101  1459.272193 -1.011755   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "510    -0.196479     0.000281  0.001  24.154581     0.016 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "510       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "511   3584000  1542.040419          16.230329  1462.099251 -0.716808   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "511    -0.198187     0.000277  0.001  24.184545   0.01589 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "511       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "512   3591000  1544.914182          16.237627  1464.965665 -0.62585   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "512    -0.197439     0.000279  0.001  24.408095  0.015921 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "512       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "513   3598000  1547.658904          16.245653  1467.702311 -0.165658   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "513    -0.198829     0.000276  0.001  24.228496  0.015836 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "513       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "514   3605000  1550.498498          16.253159  1470.534344 -0.553752   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "514     -0.19885     0.000286  0.001  24.328992  0.016053 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "514       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "515   3612000  1553.414987          16.263197  1473.440737 -0.566306   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "515    -0.198479     0.000273  0.001  24.196554   0.01584 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "515       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "516   3619000  1556.262133          16.271799  1476.279221 -0.742927   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "516    -0.199373     0.000275  0.001  24.238919   0.01576 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "516       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "517   3626000  1559.063197          16.280077  1479.071953 -0.326347   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "517     -0.19868     0.000275  0.001  24.172274  0.015788 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "517       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "518   3633000  1562.087997          16.287424  1482.089352 -0.785348   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "518    -0.197856     0.000274  0.001  24.448853  0.015812 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "518       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "519   3640000  1565.05303          16.296339  1485.045421 -0.722732   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "519    -0.198253     0.000269  0.001  24.438113  0.015693 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "519       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "520   3647000  1567.958821          16.303807  1487.943686 -0.396586   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "520    -0.198375     0.000272  0.001  24.361752  0.015781 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "520       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "521   3654000  1570.66451          16.318002  1490.635114 -0.723379   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "521    -0.200084     0.000279  0.001  24.316743  0.015976 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "521       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "522   3661000  1573.471394             16.326  1493.43395 -0.713601   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "522    -0.198811     0.000271  0.001  24.234252  0.015748 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "522       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "523   3668000  1576.251892          16.333151  1496.207242 -0.83322   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "523    -0.198088     0.000271  0.001  24.264558  0.015739 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "523       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "524   3675000  1579.081938          16.341902  1499.028486 -0.609772   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "524    -0.199108     0.000278  0.001  24.252457  0.015857 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "524       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "525   3682000  1581.900845          16.349375  1501.839871 -0.827001   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "525    -0.199519     0.000277  0.001  24.26168  0.015854 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "525       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "526   3689000  1584.761433          16.357148  1504.692636 -0.716234   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "526     -0.19952     0.000288  0.001  24.427566  0.016025 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "526       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "527   3696000  1587.699341          16.364559  1507.623076 -0.565193   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "527    -0.200165     0.000268  0.001  24.538783  0.015563 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "527       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "528   3703000  1590.52791          16.373068  1510.443086 -0.620771   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "528    -0.199449     0.000272  0.001  24.532188  0.015715 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "528       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "529   3710000  1593.54654          16.381237  1513.453492 -0.845117   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "529    -0.197379     0.000281  0.001  24.212654  0.015844 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "529       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "530   3717000  1596.409362          16.389055  1516.308442 -0.748552   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "530    -0.201543     0.000266  0.001  24.382046    0.0155 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "530       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "531   3724000  1599.216178          16.396854  1519.107405 -0.6791   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "531    -0.199469     0.000272  0.001  24.424366  0.015669 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "531       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "532   3731000  1602.052088          16.406442  1521.933671 -0.565132   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "532    -0.200412     0.000277  0.001  24.18645  0.015765 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "532       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "533   3738000  1605.042131          16.414468  1524.915634 -0.439328   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "533    -0.198929     0.000272  0.001  24.383282  0.015706 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "533       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "534   3745000  1608.048169          16.422523  1527.913565 -0.738621   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "534    -0.199826     0.000265  0.001  24.503528  0.015607 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "534       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "535   3752000  1610.747912          16.430983  1530.604796 -0.945476   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "535    -0.199175     0.000272  0.001  24.373228  0.015638 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "535       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "536   3759000  1613.542635          16.439223  1533.391225 -0.554189   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "536    -0.200287     0.000269  0.001  24.301555  0.015546 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "536       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "537   3766000  1616.587716          16.447156  1536.428317 -0.606494   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "537    -0.198604      0.00027  0.001  24.422011  0.015573 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "537       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "538   3773000  1619.473941          16.454833  1539.306808 -0.962859   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "538    -0.201746     0.000273  0.001  24.337381  0.015517 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "538       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "539   3780000  1622.239451          16.463448  1542.063644 -0.475178   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "539    -0.199567     0.000273  0.001  24.481852  0.015617 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "539       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "540   3787000  1625.110148          16.472109  1544.925625 -0.615626   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "540      -0.2005     0.000266  0.001  24.415558  0.015593 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "540       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "541   3794000  1628.007278           16.47933  1547.815472 -0.936662   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "541    -0.201165     0.000267  0.001  24.412193  0.015506 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "541       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "542   3801000  1630.860615          16.489556  1550.658525 -0.716755   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "542    -0.200814     0.000266  0.001  24.461193  0.015465 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "542       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "543   3808000  1633.746614          16.497856  1553.536165 -0.904261   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "543    -0.200237     0.000271  0.001  24.347145  0.015613 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "543       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "544   3815000  1636.542607          16.506292  1556.323672 -0.648073   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "544    -0.200052     0.000262  0.001  24.432508  0.015396 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "544       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "545   3822000  1639.330383          16.514783  1559.102902 -1.09911   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "545     -0.20187     0.000269  0.001  24.608772  0.015522 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "545       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "546   3829000  1642.170419           16.52251  1561.935121 -0.528195   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "546     -0.20142     0.000269  0.001  24.437335  0.015545 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "546       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "547   3836000  1645.065188          16.531386  1564.820918 -0.821727   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "547    -0.200482     0.000275  0.001  24.292585  0.015673 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "547       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "548   3843000  1647.999413          16.540105  1567.746363 -0.443402   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "548    -0.199263     0.000273  0.001  24.248101  0.015695 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "548       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "549   3850000  1650.855805          16.548982  1570.593812 -0.394711   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "549      -0.2004     0.000265  0.001  24.343627  0.015499 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "549       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "550   3857000  1653.615384          16.556629  1573.345693 -0.665717   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "550    -0.200141     0.000267  0.001  24.442365  0.015483 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "550       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "551   3864000  1656.468666          16.565309  1576.190242 -0.572874   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "551    -0.201371     0.000266  0.001  24.43574  0.015343 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "551       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "552   3871000  1659.307395          16.573399  1579.020828 -0.83998   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "552    -0.200844     0.000264  0.001  24.347493   0.01536 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "552       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "553   3878000  1662.349823          16.581556  1582.055042 -0.606272   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "553     -0.19934     0.000258  0.001  24.439447  0.015317 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "553       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "554   3885000  1665.249541          16.589952  1584.946305 -0.636764   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "554     -0.19992     0.000263  0.001  24.327374  0.015453 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "554       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "555   3892000  1668.120145          16.597464  1587.809338 -0.715552   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "555    -0.201184     0.000256  0.001  24.392891  0.015319 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "555       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "556   3899000  1671.050674          16.605426  1590.731847 -0.826478   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "556    -0.202252     0.000262  0.001  24.242897  0.015267 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "556       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "557   3906000  1673.912409          16.613102  1593.585853 -0.378485   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "557    -0.200802     0.000268  0.001  24.362173  0.015366 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "557       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "558   3913000  1676.833171          16.621104  1596.498555 -0.736818   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "558    -0.200628     0.000268  0.001  24.409751  0.015482 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "558       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "559   3920000  1679.589296           16.62959  1599.246138 -0.743467   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "559    -0.199173     0.000264  0.001  24.560935   0.01538 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "559       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "560   3927000  1682.43008          16.637526  1602.078931 -0.559348   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "560     -0.20291      0.00026  0.001  24.316273  0.015228 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "560       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "561   3934000  1685.18282          16.646148  1604.822998 -0.523418   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "561    -0.202449      0.00026  0.001  24.435491  0.015222 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "561       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "562   3941000  1688.096119          16.653631  1607.728758 -0.908288   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "562    -0.200314     0.000263  0.001  24.562549  0.015184 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "562       29.585798     25.903584  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "563   3948000  1690.8905          16.660767  1610.515951 -0.862906   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "563    -0.200283     0.000265  0.001  24.469428   0.01538 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "563       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "564   3955000  1693.663759           16.66851  1613.281418 -0.299016   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "564    -0.202367     0.000254  0.001  24.516563  0.015118 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "564       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "565   3962000  1696.502176          16.675951  1616.112338 -0.919725   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "565    -0.203586     0.000265  0.001  24.363358  0.015286 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "565       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "566   3969000  1699.270192          16.683836  1618.872412 -0.596789   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "566    -0.201671     0.000266  0.001  24.572433  0.015431 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "566       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "567   3976000  1702.050071          16.691189  1621.644881 -0.669691   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "567    -0.203571     0.000264  0.001  24.414753  0.015275 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "567       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "568   3983000  1704.790334          16.698458  1624.377825 -0.694709   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "568    -0.202057     0.000265  0.001  24.379151  0.015268 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "568       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "569   3990000  1707.632731          16.705687  1627.212941 -0.664336   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "569    -0.203532     0.000264  0.001  24.392518  0.015448 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "569       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "570   3997000  1710.414292          16.714285  1629.985783 -0.622869   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "570     -0.20101     0.000267  0.001  24.359264  0.015343 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "570       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "571   4004000  1713.178781          16.723757  1632.740744 -0.747994   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "571    -0.202092     0.000265  0.001  24.31227  0.015295 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "571       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "572   4011000  1715.963956          16.732687  1635.516936 -0.568118   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "572    -0.201163     0.000263  0.001  24.454117  0.015362 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "572       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "573   4018000  1718.845948          16.740648  1638.390906 -0.429353   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "573    -0.202114     0.000262  0.001  24.53425  0.015294 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "573       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "574   4025000  1721.557342          16.749676  1641.093221 -0.585196   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "574    -0.200899     0.000255  0.001  24.480367   0.01514 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "574       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "575   4032000  1724.340824          16.757609  1643.868691 -0.622646   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "575    -0.202127     0.000267  0.001  24.502302  0.015353 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "575       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "576   4039000  1727.099691          16.765141  1646.619966 -0.47006   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "576    -0.203031     0.000263  0.001  24.548682  0.015236 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "576       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "577   4046000  1729.915849          16.773376  1649.427836 -0.877629   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "577    -0.202697     0.000262  0.001  24.391856  0.015208 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "577       29.585798     25.903584  \n",
      "     timestep        time  collect_wait_time   train_time    score  \\\n",
      "578   4053000  1732.68019          16.782508  1652.182986 -0.66349   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "578    -0.202697     0.000265  0.001  24.381386  0.015206 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "578       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "579   4060000  1735.435584          16.789989  1654.930845 -0.84451   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "579    -0.202361      0.00026  0.001  24.493939  0.015144 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "579       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "580   4067000  1738.226151          16.798406  1657.71294 -0.616509   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "580    -0.201947     0.000266  0.001  24.37468  0.015355 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "580       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "581   4074000  1741.072322          16.806237  1660.551226 -0.591859   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "581    -0.201658     0.000257  0.001  24.453541  0.015049 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "581       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "582   4081000  1743.749761          16.813622  1663.221229 -0.859432   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "582    -0.203667     0.000253  0.001  24.419765  0.015003 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "582       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "583   4088000  1746.479606          16.820702  1665.943938 -0.699577   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "583    -0.203213     0.000257  0.001  24.450144  0.015144 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "583       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "584   4095000  1749.181092           16.82812  1668.637953 -0.676418   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "584    -0.203136     0.000265  0.001  24.536222  0.015343 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "584       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "585   4102000  1751.982899          16.835918  1671.431908 -0.502297   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "585    -0.201726     0.000264  0.001  24.407471  0.015186 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "585       29.585798     25.903584  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "586   4109000  1754.7998          16.843126  1674.241547 -0.945234   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "586    -0.202734     0.000259  0.001  24.428118  0.015198 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "586       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "587   4116000  1757.463099          16.850733  1676.897188 -0.561313   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "587    -0.203335     0.000258  0.001  24.256696  0.015129 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "587       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "588   4123000  1760.188678          16.858484  1679.614964 -0.917612   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "588    -0.204131      0.00026  0.001  24.389449  0.015187 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "588       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "589   4130000  1763.033323          16.868487  1682.449548 -0.499556   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "589    -0.202532     0.000265  0.001  24.37381  0.015147 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "589       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "590   4137000  1765.812072          16.876648  1685.220086 -0.344433   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "590    -0.203014     0.000258  0.001  24.29661  0.015129 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "590       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "591   4144000  1768.542529          16.884716  1687.942421 -0.84131   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "591      -0.2032      0.00026  0.001  24.224011  0.015206 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "591       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "592   4151000  1771.386492          16.892658  1690.778344 -0.664593   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "592    -0.204747     0.000258  0.001  24.375845   0.01506 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "592       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "593   4158000  1774.289866          16.900108  1693.674214 -0.836001   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "593    -0.202558     0.000256  0.001  24.557452  0.015117 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "593       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "594   4165000  1777.070225          16.907342  1696.44728 -0.834131   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "594    -0.203282     0.000257  0.001  24.477572  0.015147 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "594       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "595   4172000  1779.737437          16.915076  1699.106705 -0.231516   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "595    -0.201337      0.00026  0.001  24.280281  0.015179 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "595       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "596   4179000  1782.582406          16.922611  1701.944078 -0.84981   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "596    -0.203547     0.000266  0.001  24.571303  0.015276 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "596       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "597   4186000  1785.340717          16.930159  1704.694775 -0.667503   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "597    -0.203023     0.000268  0.001  24.465287  0.015248 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "597       29.585798     25.903584  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "598   4193000  1788.107677           16.93759  1707.454244 -0.829893   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "598    -0.205983     0.000267  0.001  24.462438  0.015199 -0.006959   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "598       29.585798     25.903584  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "599   4200000  1803.80254           16.94651  1710.191517 -0.601308   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "599    -0.202807     0.000263  0.001  24.357749  0.015189  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "599       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "600   4207000  1806.900962          16.954601  1713.281788 -0.770226   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "600    -0.203786     0.000265  0.001  24.425167  0.015259  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "600       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "601   4214000  1809.953996          16.968625  1716.320064 -0.704268   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "601    -0.202773     0.000259  0.001  24.384672  0.015095  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "601       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "602   4221000  1813.356578          16.979155  1719.712034 -0.621855   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "602     -0.20416     0.000257  0.001  24.362133  0.015146  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "602       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "603   4228000  1816.106633          16.987163  1722.45403 -0.261107   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "603    -0.203185      0.00026  0.001  24.448061  0.015167  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "603       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "604   4235000  1818.86766          16.996478  1725.205676 -0.510606   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "604    -0.203312     0.000258  0.001  24.446012  0.015059  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "604       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "605   4242000  1821.697222          17.003991  1728.027647 -0.8453   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "605    -0.203762     0.000259  0.001  24.485753  0.015106  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "605       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "606   4249000  1824.663308          17.012052  1730.985581 -0.777621   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "606    -0.203852     0.000262  0.001  24.330718  0.015097  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "606       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "607   4256000  1827.375891          17.020302  1733.689862 -0.789636   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "607    -0.203446     0.000264  0.001  24.135004  0.015134  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "607       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "608   4263000  1830.192928          17.028785  1736.498365 -0.533641   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "608     -0.20533     0.000251  0.001  24.386422  0.014845  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "608       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "609   4270000  1832.85178          17.036483  1739.149474 -0.640874   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "609     -0.20379     0.000258  0.001  24.459233  0.015093  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "609       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "610   4277000  1835.783272          17.043996  1742.073404 -0.706176   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "610    -0.204319     0.000254  0.001  24.309434  0.014902  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "610       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "611   4284000  1838.546685          17.052409  1744.828355 -0.800203   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "611    -0.203848     0.000258  0.001  24.280042  0.014981  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "611       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "612   4291000  1841.412291          17.060263  1747.686055 -0.87956   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "612    -0.204728     0.000256  0.001  24.276823  0.015012  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "612       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "613   4298000  1844.23824          17.067716  1750.504501 -0.194094   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "613    -0.206234      0.00026  0.001  24.29329  0.015026  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "613       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "614   4305000  1847.194137          17.075528  1753.452534 -0.576315   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "614    -0.206188     0.000258  0.001  24.309918  0.014882  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "614       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "615   4312000  1849.909134          17.084157  1756.158841 -0.515298   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "615     -0.20413     0.000255  0.001  24.376176  0.014949  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "615       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "616   4319000  1852.962749          17.092301  1759.20426 -0.551765   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "616    -0.203142     0.000257  0.001  24.303485  0.015011  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "616       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "617   4326000  1855.790934          17.100554  1762.024136 -0.621769   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "617    -0.205245     0.000257  0.001  24.259985  0.015019  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "617       28.985508     26.355778  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "618   4333000  1858.7642          17.109593  1764.988304 -0.470763   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "618    -0.204151     0.000255  0.001  24.176914  0.015025  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "618       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "619   4340000  1861.504235          17.117498  1767.720382 -0.530737   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "619    -0.204423     0.000262  0.001  24.405673   0.01502  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "619       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "620   4347000  1864.334162          17.125216  1770.542542 -0.573523   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "620    -0.203732     0.000257  0.001  24.503408  0.015015  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "620       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "621   4354000  1867.157921          17.132957  1773.358503 -0.715431   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "621    -0.204175     0.000263  0.001  24.463622  0.015174  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "621       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "622   4361000  1870.105899          17.140458  1776.298925 -0.751144   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "622    -0.205517     0.000254  0.001  24.355966  0.014936  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "622       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "623   4368000  1872.821508          17.147659  1779.007279 -0.48988   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "623    -0.203994     0.000266  0.001  24.570638  0.015267  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "623       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "624   4375000  1875.676016          17.156793  1781.852593 -0.673836   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "624    -0.204162     0.000247  0.001  24.421245  0.014775  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "624       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "625   4382000  1878.532161          17.165076  1784.700404 -0.463345   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "625    -0.205265     0.000259  0.001  24.377239  0.014961  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "625       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "626   4389000  1881.316979          17.173536  1787.476712 -0.459709   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "626    -0.204217     0.000255  0.001  24.494845  0.014994  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "626       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "627   4396000  1884.140903           17.18101  1790.29311 -0.480172   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "627    -0.205195      0.00025  0.001  24.394409  0.014811  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "627       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "628   4403000  1886.952939          17.188231  1793.097875 -0.440933   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "628    -0.203082     0.000251  0.001  24.362894  0.014844  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "628       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "629   4410000  1889.852933          17.195266  1795.990781 -0.71986   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "629    -0.205311     0.000253  0.001  24.344454   0.01487  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "629       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "630   4417000  1892.621542          17.203156  1798.751449 -0.62041   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "630    -0.204802     0.000253  0.001  24.335987  0.014849  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "630       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "631   4424000  1895.444411           17.21035  1801.567075 -0.587943   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "631    -0.204019     0.000258  0.001  24.385883  0.014899  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "631       28.985508     26.355778  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "632   4431000  1898.3077          17.217909  1804.422755 -0.507177   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "632    -0.203998     0.000253  0.001  24.566044  0.014874  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "632       28.985508     26.355778  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "633   4438000  1901.1338          17.225241  1807.241468 -0.826067   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "633    -0.204372     0.000254  0.001  24.477885  0.014865  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "633       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "634   4445000  1903.909341          17.232647  1810.009548 -0.804915   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "634    -0.204653     0.000253  0.001  24.375476  0.014872  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "634       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "635   4452000  1906.741246          17.240725  1812.833319 -0.349887   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "635    -0.203788     0.000259  0.001  24.349943  0.015054  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "635       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "636   4459000  1909.602257          17.249071  1815.68593 -0.805386   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "636    -0.204565     0.000266  0.001  24.18915  0.015052  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "636       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "637   4466000  1912.44314          17.256351  1818.519479 -0.665005   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "637    -0.205566     0.000251  0.001  24.134843  0.014787  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "637       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "638   4473000  1915.30547          17.263797  1821.374313 -0.889361   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "638    -0.205853     0.000254  0.001  24.29318  0.014875  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "638       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "639   4480000  1918.01454          17.272427  1824.074705 -0.599031   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "639    -0.205125      0.00026  0.001  24.632931  0.015029  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "639       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "640   4487000  1920.965534          17.280644  1827.017435 -0.888093   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "640    -0.204695     0.000258  0.001  24.443032  0.015014  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "640       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "641   4494000  1923.768581           17.28811  1829.812963 -0.963507   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "641    -0.205725     0.000258  0.001  24.427564  0.015037  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "641       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "642   4501000  1926.621341           17.29542  1832.658359 -0.496042   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "642    -0.202818     0.000256  0.001  24.617652  0.015016  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "642       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "643   4508000  1929.42244          17.303717  1835.45111 -0.671149   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "643    -0.203426     0.000261  0.001  24.517236   0.01505  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "643       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "644   4515000  1932.221568          17.312141  1838.241755 -0.840943   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "644    -0.204258     0.000257  0.001  24.393427   0.01489  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "644       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "645   4522000  1935.071252          17.320166  1841.083363 -1.052109   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "645    -0.206182     0.000253  0.001  24.381089  0.014811  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "645       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "646   4529000  1937.868591          17.327975  1843.872843 -0.813762   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "646      -0.2038     0.000258  0.001  24.243795  0.014945  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "646       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "647   4536000  1940.608239          17.335923  1846.604489 -0.662506   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "647    -0.206199     0.000247  0.001  24.371951  0.014636  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "647       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "648   4543000  1943.483676          17.343532  1849.472263 -0.702984   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "648    -0.205213     0.000258  0.001  24.392609  0.014898  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "648       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time    score  \\\n",
      "649   4550000  1946.16345          17.350805  1852.144711 -0.48113   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha  entropy  td_error    return  \\\n",
      "649    -0.205959     0.000249  0.001  24.3827  0.014798  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "649       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "650   4557000  1949.025747          17.358257   1854.9995 -0.544114   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "650    -0.205037     0.000265  0.001  24.457155  0.015148  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "650       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "651   4564000  1951.901478           17.36684  1857.866594 -0.564762   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "651    -0.207474     0.000253  0.001  24.386652  0.014884  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "651       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "652   4571000  1954.793514          17.374529  1860.750885 -0.619022   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "652    -0.206671     0.000255  0.001  24.408361  0.014902  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "652       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "653   4578000  1957.536206          17.383154  1863.484905 -0.552337   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "653    -0.206692      0.00026  0.001  24.326579   0.01495  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "653       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "654   4585000  1960.411376          17.390671  1866.352508 -1.030945   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "654    -0.207677     0.000256  0.001  24.268166  0.014851  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "654       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "655   4592000  1963.177909          17.399138  1869.110521 -0.565087   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "655    -0.205367      0.00025  0.001  24.284047  0.014773  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "655       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "656   4599000  1965.908302          17.406808  1871.83319 -0.663796   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "656    -0.206228     0.000259  0.001  24.43997  0.014978  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "656       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "657   4606000  1968.709101          17.414376  1874.626371 -0.833204   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "657    -0.206311     0.000241  0.001  24.557516  0.014569  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "657       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "658   4613000  1971.527955          17.422401  1877.437147 -0.569777   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "658    -0.205813     0.000257  0.001  24.667032  0.014875  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "658       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "659   4620000  1974.363148          17.429987  1880.264698 -0.878566   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "659    -0.206148     0.000257  0.001  24.515254   0.01498  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "659       28.985508     26.355778  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "660   4627000  1977.2899          17.437606  1883.183779 -0.663541   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "660    -0.205678     0.000258  0.001  24.621854  0.014993  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "660       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "661   4634000  1980.100936          17.445341  1885.987029 -0.601744   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "661    -0.205706      0.00027  0.001  24.733715  0.015144  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "661       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "662   4641000  1982.974051          17.453603  1888.851827 -0.817417   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "662    -0.205986      0.00026  0.001  24.600445  0.015004  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "662       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "663   4648000  1985.831084          17.461247  1891.701163 -0.901658   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "663    -0.206737     0.000249  0.001  24.566082  0.014765  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "663       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "664   4655000  1988.595088          17.469829  1894.456529 -0.678813   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "664    -0.206909     0.000263  0.001  24.410131  0.014954  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "664       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "665   4662000  1991.380126          17.478168  1897.233173 -0.82629   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "665    -0.205458     0.000255  0.001  24.337649   0.01485  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "665       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "666   4669000  1994.367607          17.486621  1900.212149 -0.71803   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "666    -0.205204     0.000249  0.001  24.525277  0.014769  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "666       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "667   4676000  1997.325494           17.49568  1903.160919 -0.875695   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "667    -0.206637     0.000264  0.001  24.389501  0.014979  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "667       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "668   4683000  2000.218738          17.504775  1906.045017 -0.66476   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "668    -0.206545     0.000268  0.001  24.389185  0.014957  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "668       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "669   4690000  2002.951575          17.512016  1908.770566 -0.643716   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "669    -0.205335     0.000251  0.001  24.331399  0.014815  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "669       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "670   4697000  2005.791832          17.520934  1911.601847 -0.167608   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "670    -0.206599     0.000249  0.001  24.450639  0.014626  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "670       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "671   4704000  2008.563818           17.52997  1914.364745 -0.474627   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "671     -0.20505     0.000256  0.001  24.626959  0.014893  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "671       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "672   4711000  2011.307818            17.5375  1917.101157 -0.620005   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "672     -0.20583     0.000247  0.001  24.613771  0.014752  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "672       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "673   4718000  2014.00123          17.545021  1919.786994 -0.815897   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "673    -0.206735     0.000257  0.001  24.414334  0.014929  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "673       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "674   4725000  2016.609492          17.552516  1922.387707 -0.804257   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "674       -0.205     0.000253  0.001  24.45497  0.014715  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "674       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "675   4732000  2019.397913          17.560926  1925.16766 -0.266337   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "675    -0.205682      0.00026  0.001  24.571108  0.014893  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "675       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "676   4739000  2022.249236          17.569117  1928.01074 -0.750433   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "676    -0.205745     0.000258  0.001  24.717726  0.014887  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "676       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "677   4746000  2025.129626          17.577252  1930.882941 -0.712764   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "677    -0.206497     0.000252  0.001  24.534823  0.014749  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "677       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "678   4753000  2027.897853          17.585021  1933.643314 -0.891718   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "678    -0.207232     0.000252  0.001  24.510948  0.014715  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "678       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "679   4760000  2030.575315          17.593762  1936.311986 -0.66076   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "679    -0.206974     0.000257  0.001  24.582688  0.014869  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "679       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "680   4767000  2033.385819          17.601269  1939.114926 -0.826285   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "680    -0.205823     0.000249  0.001  24.666542  0.014645  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "680       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "681   4774000  2036.21266          17.608623  1941.934365 -0.677939   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "681    -0.208242     0.000254  0.001  24.65909  0.014814  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "681       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "682   4781000  2039.004761          17.617114  1944.717914 -0.6896   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "682    -0.207786     0.000254  0.001  24.67899  0.014699  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "682       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "683   4788000  2041.795156          17.628221  1947.497149 -0.951569   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "683    -0.204665     0.000252  0.001  24.510739  0.014707  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "683       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "684   4795000  2044.666333          17.635543  1950.360954 -0.813109   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "684    -0.206567      0.00025  0.001  24.256475  0.014675  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "684       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "685   4802000  2047.482672          17.644302  1953.168481 -0.874202   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "685    -0.206143     0.000256  0.001  24.245507  0.014854  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "685       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "686   4809000  2050.212346          17.652111  1955.890297 -0.743474   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "686    -0.206037     0.000251  0.001  24.369042  0.014685  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "686       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "687   4816000  2052.981871          17.660918  1958.650964 -0.586995   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "687    -0.208368     0.000248  0.001  24.577433  0.014722  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "687       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "688   4823000  2055.995887          17.668938  1961.656905 -0.963693   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "688    -0.208562     0.000245  0.001  24.588756  0.014587  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "688       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "689   4830000  2058.777345          17.676466  1964.430777 -0.852604   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "689    -0.207661     0.000249  0.001  24.708479  0.014724  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "689       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "690   4837000  2061.479814          17.683843  1967.12582 -0.470593   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "690    -0.206672      0.00025  0.001  24.542954  0.014677  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "690       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "691   4844000  2064.479592          17.691361  1970.118027 -0.808846   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "691    -0.207967     0.000254  0.001  24.441432  0.014817  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "691       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "692   4851000  2067.319672          17.701169  1972.948244 -0.856097   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "692    -0.206412     0.000246  0.001  24.544538  0.014577  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "692       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "693   4858000  2069.985607          17.708821  1975.606473 -0.88056   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "693    -0.205748     0.000246  0.001  24.47065  0.014612  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "693       28.985508     26.355778  \n",
      "     timestep        time  collect_wait_time   train_time    score  \\\n",
      "694   4865000  2072.72325          17.717308  1978.335576 -0.96081   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "694    -0.206202     0.000248  0.001  24.587496   0.01472  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "694       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "695   4872000  2075.692519          17.724531  1981.297572 -0.525764   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "695    -0.207905     0.000247  0.001  24.396748  0.014537  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "695       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "696   4879000  2078.653786          17.732099  1984.251223 -0.96576   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "696    -0.206489      0.00026  0.001  24.436105  0.014895  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "696       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "697   4886000  2081.416513          17.739147  1987.006854 -0.583307   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "697    -0.206203     0.000248  0.001  24.392035   0.01468  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "697       28.985508     26.355778  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "698   4893000  2084.173983          17.747125  1989.756289 -0.539706   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "698    -0.206651     0.000248  0.001  24.42107  0.014674  0.010355   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "698       28.985508     26.355778  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "699   4900000  2100.547309          17.755755  1992.819153 -0.775177   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "699    -0.206305     0.000253  0.001  24.364127  0.014751  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "699        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "700   4907000  2103.383443          17.764089  1995.646897 -0.70845   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "700    -0.207647      0.00025  0.001  24.40233  0.014718  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "700        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "701   4914000  2107.002554          17.777694  1999.252351 -0.478672   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "701    -0.207937     0.000255  0.001  24.60194  0.014765  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "701        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "702   4921000  2110.267003          17.785616  2002.508825 -0.412369   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "702    -0.206242     0.000253  0.001  24.629975  0.014718  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "702        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "703   4928000  2113.497446          17.792943  2005.731888 -0.497691   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "703    -0.208032     0.000244  0.001  24.478658   0.01462  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "703        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "704   4935000  2117.018542           17.80166  2009.244201 -0.541114   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "704    -0.207633     0.000247  0.001  24.535832  0.014583  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "704        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "705   4942000  2120.22096          17.811576  2012.436644 -0.906307   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "705    -0.207279     0.000239  0.001  24.601676  0.014365  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "705        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "706   4949000  2123.177958          17.820055  2015.385108 -0.717066   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "706    -0.206602     0.000252  0.001  24.50314  0.014695  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "706        30.30303      26.45713  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "707   4956000  2126.3742          17.828405  2018.572943 -0.590026   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "707    -0.206908     0.000247  0.001  24.497632  0.014518  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "707        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "708   4963000  2129.560866          17.837708  2021.750224 -0.647961   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "708    -0.207236     0.000253  0.001  24.583799  0.014717  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "708        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "709   4970000  2132.619612          17.846351  2024.800273 -0.812446   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "709    -0.209082     0.000262  0.001  24.533193  0.014902  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "709        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "710   4977000  2135.47274          17.854073  2027.645627 -1.010389   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "710    -0.207055     0.000254  0.001  24.592599   0.01482  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "710        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "711   4984000  2138.556423          17.862271  2030.72106 -0.644722   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "711    -0.206333     0.000251  0.001  24.559378  0.014741  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "711        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "712   4991000  2141.626076          17.870033  2033.782894 -0.777631   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "712    -0.207924     0.000245  0.001  24.331201   0.01445  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "712        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "713   4998000  2144.584162          17.877702  2036.733258 -0.870522   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "713    -0.207798     0.000245  0.001  24.520036  0.014558  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "713        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "714   5005000  2147.707935          17.885087  2039.849599 -0.695372   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "714    -0.209703     0.000249  0.001  24.513963  0.014648  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "714        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "715   5012000  2150.816676          17.893027  2042.95035 -0.635099   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "715    -0.207751     0.000254  0.001  24.605512  0.014643  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "715        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "716   5019000  2153.528031          17.901382  2045.653296 -0.543139   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha  entropy  td_error    return  \\\n",
      "716    -0.207905     0.000252  0.001  24.5325  0.014702  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "716        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "717   5026000  2156.121315          17.908477  2048.23943 -0.708534   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "717    -0.208298     0.000248  0.001  24.651396  0.014603  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "717        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "718   5033000  2159.159714          17.916081  2051.270172 -0.577723   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha  entropy  td_error    return  \\\n",
      "718    -0.208224     0.000253  0.001   24.608  0.014736  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "718        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "719   5040000  2162.004586           17.92456  2054.106507 -0.533567   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "719     -0.20879     0.000246  0.001  24.73349  0.014632  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "719        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "720   5047000  2164.79645          17.932213  2056.890657 -0.603961   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "720    -0.207467     0.000249  0.001  24.712265  0.014661  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "720        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "721   5054000  2167.493744          17.939658  2059.580458 -0.747797   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "721    -0.207161     0.000253  0.001  24.642906  0.014797  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "721        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "722   5061000  2170.466614          17.950108  2062.54283 -0.909988   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "722    -0.207684     0.000256  0.001  24.350553  0.014836  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "722        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "723   5068000  2173.306113          17.958641  2065.373744 -0.79529   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "723    -0.207976     0.000257  0.001  24.606328  0.014797  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "723        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "724   5075000  2176.081633          17.967293  2068.140562 -0.91824   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "724    -0.206939     0.000249  0.001  24.630805  0.014602  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "724        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "725   5082000  2178.779746          17.975264  2070.83065 -1.036205   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "725    -0.207703     0.000252  0.001  24.43752  0.014544  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "725        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "726   5089000  2181.70856          17.982835  2073.751842 -0.800879   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "726      -0.2105     0.000247  0.001  24.44176  0.014614  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "726        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "727   5096000  2184.640513           17.99084  2076.675733 -0.393246   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "727     -0.20774      0.00024  0.001  24.500173   0.01433  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "727        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "728   5103000  2187.432194          17.999575  2079.458622 -0.731067   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "728    -0.208227      0.00025  0.001  24.434193   0.01446  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "728        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "729   5110000  2190.211557           18.00694  2082.230564 -0.73805   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "729    -0.208902     0.000246  0.001  24.492423  0.014478  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "729        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "730   5117000  2193.156571          18.015536  2085.166918 -0.816249   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "730    -0.209115     0.000246  0.001  24.568413  0.014509  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "730        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "731   5124000  2196.497907          18.024308  2088.499433 -0.810601   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "731    -0.206599     0.000241  0.001  24.603843  0.014496  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "731        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "732   5131000  2199.33585          18.034236  2091.327395 -0.897372   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "732    -0.210021     0.000249  0.001  24.565059  0.014584  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "732        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "733   5138000  2202.060391          18.042505  2094.043612 -1.00667   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "733    -0.208095     0.000245  0.001  24.505172  0.014463  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "733        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "734   5145000  2204.957715          18.050131  2096.933263 -0.588751   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "734    -0.208141     0.000246  0.001  24.659187   0.01456  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "734        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "735   5152000  2207.890185          18.057899  2099.857912 -0.848382   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "735    -0.208364     0.000255  0.001  24.551282   0.01466  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "735        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "736   5159000  2210.609789          18.066264  2102.569103 -0.874459   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "736    -0.207626     0.000242  0.001  24.596656    0.0143  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "736        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "737   5166000  2213.511354          18.074512  2105.462371 -0.639561   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "737    -0.207915      0.00025  0.001  24.525402  0.014691  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "737        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "738   5173000  2216.392396          18.082252  2108.335621 -0.7877   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "738    -0.209109     0.000247  0.001  24.496472  0.014463  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "738        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "739   5180000  2219.390389          18.091261  2111.324556 -0.725143   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "739    -0.207737     0.000251  0.001  24.574784  0.014698  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "739        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "740   5187000  2222.124802          18.098508  2114.051674 -0.785098   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "740    -0.209472     0.000241  0.001  24.594921  0.014295  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "740        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "741   5194000  2224.769097          18.106926  2116.687476 -0.88364   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "741     -0.20802     0.000252  0.001  24.58747  0.014674  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "741        30.30303      26.45713  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "742   5201000  2227.5408          18.114394  2119.451656 -0.608895   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "742    -0.209183     0.000244  0.001  24.556641  0.014444  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "742        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "743   5208000  2230.322733          18.122275  2122.225647 -0.750605   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "743    -0.210102     0.000255  0.001  24.394783  0.014662  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "743        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "744   5215000  2233.048165          18.130073  2124.943228 -0.670134   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "744    -0.208671     0.000249  0.001  24.403631  0.014585  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "744        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "745   5222000  2235.864347          18.137411  2127.752022 -1.019131   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "745    -0.209102     0.000247  0.001  24.397023  0.014531  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "745        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "746   5229000  2238.826815          18.145244  2130.706606 -0.623455   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "746    -0.208556     0.000258  0.001  24.578875  0.014714  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "746        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "747   5236000  2241.612334          18.153045  2133.484273 -0.525236   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "747    -0.207913     0.000256  0.001  24.595566  0.014763  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "747        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "748   5243000  2244.427854           18.16034  2136.29244 -0.749439   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "748    -0.210128      0.00026  0.001  24.468532  0.014663  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "748        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "749   5250000  2247.11449          18.167837  2138.971525 -0.849461   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "749    -0.209292     0.000248  0.001  24.637407  0.014561  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "749        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "750   5257000  2250.014648          18.177169  2141.862304 -0.508046   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "750    -0.207444     0.000256  0.001  24.689307  0.014799  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "750        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "751   5264000  2252.736697          18.184698  2144.576771 -0.845318   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "751    -0.208454     0.000251  0.001  24.592601   0.01472  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "751        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "752   5271000  2255.473483           18.19185  2147.306353 -0.945858   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "752    -0.208975     0.000251  0.001  24.52444  0.014668  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "752        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "753   5278000  2258.40904          18.200475  2150.23323 -1.139303   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "753    -0.207388     0.000245  0.001  24.631127  0.014522  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "753        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "754   5285000  2261.233975          18.208357  2153.050228 -0.870573   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "754    -0.209867     0.000258  0.001  24.503061  0.014874  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "754        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "755   5292000  2264.134351          18.215629  2155.943279 -0.874064   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "755    -0.209223     0.000259  0.001  24.494544  0.014773  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "755        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "756   5299000  2266.796741          18.223446  2158.597803 -0.710824   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "756    -0.208646     0.000247  0.001  24.487689  0.014648  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "756        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "757   5306000  2269.634581          18.232494  2161.426464 -0.612289   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "757    -0.206982     0.000246  0.001  24.65919  0.014505  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "757        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "758   5313000  2272.477238            18.2404  2164.261156 -0.834442   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "758    -0.208953     0.000246  0.001  24.491745  0.014529  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "758        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "759   5320000  2275.273621          18.248403  2167.049485 -0.604962   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "759    -0.207229     0.000255  0.001  24.428373  0.014839  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "759        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "760   5327000  2278.157845          18.255961  2169.926098 -0.809542   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "760    -0.208332     0.000261  0.001  24.501655  0.014826  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "760        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "761   5334000  2281.047911           18.26404  2172.808033 -1.011075   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "761    -0.208188     0.000252  0.001  24.520221  0.014585  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "761        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "762   5341000  2283.762261          18.271534  2175.514833 -0.56963   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "762    -0.209199     0.000253  0.001  24.48885  0.014571  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "762        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "763   5348000  2286.554581          18.279667  2178.298966 -0.713482   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "763    -0.210828     0.000241  0.001  24.524049  0.014346  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "763        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "764   5355000  2289.331806          18.289339  2181.066472 -0.629846   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "764    -0.209727     0.000251  0.001  24.575329  0.014675  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "764        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "765   5362000  2292.232349          18.297702  2183.958557 -0.674588   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "765    -0.208387     0.000255  0.001  24.497861  0.014621  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "765        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "766   5369000  2295.067925          18.307331  2186.784448 -0.521721   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "766    -0.210141     0.000252  0.001  24.404679  0.014646  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "766        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "767   5376000  2297.893115          18.315039  2189.601879 -0.524997   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "767    -0.208404     0.000253  0.001  24.45959  0.014579  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "767        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "768   5383000  2300.674048           18.32348  2192.374316 -0.690069   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "768    -0.210469     0.000247  0.001  24.682305  0.014539  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "768        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "769   5390000  2303.478538          18.332318  2195.169919 -0.789298   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "769    -0.209716      0.00025  0.001  24.720322  0.014634  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "769        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "770   5397000  2306.369514          18.339641  2198.05352 -0.606088   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "770    -0.209898     0.000263  0.001  24.447065  0.014771  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "770        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "771   5404000  2309.255549          18.347709  2200.931441 -0.798051   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "771    -0.208782     0.000242  0.001  24.640041  0.014367  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "771        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "772   5411000  2312.036979           18.35544  2203.705085 -0.597101   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "772    -0.209261      0.00024  0.001  24.721402  0.014346  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "772        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "773   5418000  2315.039987          18.364358  2206.699107 -0.659239   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "773     -0.21003      0.00024  0.001  24.478476  0.014279  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "773        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "774   5425000  2317.896125          18.374002  2209.545541 -0.602544   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "774    -0.208344     0.000245  0.001  24.540473  0.014534  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "774        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time   train_time    score  \\\n",
      "775   5432000  2320.65382          18.381874  2212.295297 -0.62528   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "775    -0.208322     0.000245  0.001  24.493302  0.014489  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "775        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "776   5439000  2323.355808          18.389327  2214.98978 -0.667595   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "776    -0.209778     0.000243  0.001  24.441452   0.01436  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "776        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "777   5446000  2326.158801          18.398013  2217.784027 -0.859174   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "777    -0.209886     0.000244  0.001  24.577523  0.014451  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "777        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "778   5453000  2328.980882          18.407111  2220.596954 -0.540059   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "778     -0.20565     0.000244  0.001  24.614064  0.014473  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "778        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "779   5460000  2331.689889          18.414343  2223.298679 -0.783571   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "779    -0.209864     0.000243  0.001  24.646437  0.014383  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "779        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "780   5467000  2334.477022          18.422092  2226.078013 -0.650483   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "780    -0.210667      0.00024  0.001  24.545789  0.014326  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "780        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "781   5474000  2337.361611          18.429213  2228.955428 -0.441559   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "781    -0.209122     0.000242  0.001  24.732871  0.014323  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "781        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "782   5481000  2340.127779          18.437867  2231.712895 -0.626475   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "782    -0.209205     0.000243  0.001  24.579678  0.014439  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "782        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "783   5488000  2342.816036          18.445319  2234.393645 -0.806291   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "783    -0.208564     0.000244  0.001  24.739077  0.014444  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "783        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "784   5495000  2345.623854          18.452607  2237.194125 -0.877491   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "784    -0.209628     0.000247  0.001  24.595534   0.01443  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "784        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "785   5502000  2348.612242          18.460257  2240.174816 -0.783253   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "785     -0.20882     0.000243  0.001  24.646574  0.014371  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "785        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "786   5509000  2351.404251          18.468165  2242.958865 -0.557781   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "786    -0.210268     0.000246  0.001  24.622263  0.014414  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "786        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "787   5516000  2354.143316          18.476664  2245.689378 -0.930339   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "787     -0.20947     0.000243  0.001  24.660379  0.014343  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "787        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "788   5523000  2356.916999          18.484608  2248.455064 -0.799685   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "788    -0.209128     0.000246  0.001  24.641476  0.014562  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "788        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "789   5530000  2359.753013          18.492544  2251.283087 -0.684218   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "789    -0.209595     0.000244  0.001  24.681927  0.014249  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "789        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "790   5537000  2362.649246          18.502481  2254.16933 -0.584718   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "790    -0.209636     0.000246  0.001  24.63279  0.014471  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "790        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "791   5544000  2365.214671          18.510175  2256.727008 -0.684658   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "791     -0.21116     0.000242  0.001  24.49637   0.01435  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "791        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "792   5551000  2368.135533          18.518659  2259.639309 -0.788635   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "792    -0.210678     0.000238  0.001  24.485259  0.014296  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "792        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "793   5558000  2370.991743          18.527499  2262.486625 -0.559433   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "793    -0.209463     0.000244  0.001  24.417151  0.014407  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "793        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "794   5565000  2373.851754          18.534929  2265.339155 -0.579926   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "794     -0.21166     0.000247  0.001  24.687361  0.014383  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "794        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "795   5572000  2376.567765          18.542379  2268.047665 -0.723341   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "795     -0.20989     0.000237  0.001  24.627025  0.014187  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "795        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "796   5579000  2379.305112          18.550016  2270.777318 -0.833705   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "796    -0.210703     0.000237  0.001  24.600853  0.014277  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "796        30.30303      26.45713  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "797   5586000  2382.26272          18.556965  2273.727925 -0.473545   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "797    -0.209579      0.00025  0.001  24.499857  0.014512  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "797        30.30303      26.45713  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "798   5593000  2385.088453          18.564899  2276.545663 -0.88127   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "798    -0.210402     0.000244  0.001  24.547751  0.014476  0.012556   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "798        30.30303      26.45713  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "799   5600000  2401.076548            18.5731  2279.464277 -0.590985   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "799    -0.210101     0.000241  0.001  24.665252  0.014298  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "799        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "800   5607000  2403.933929          18.581455  2282.313216 -0.594764   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "800    -0.210638     0.000249  0.001  24.454633   0.01448  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "800        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "801   5614000  2407.15357          18.589662  2285.524595 -0.682103   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "801    -0.209748     0.000246  0.001  24.671191  0.014365  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "801        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "802   5621000  2410.255023          18.603464  2288.61219 -0.797892   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "802    -0.211463     0.000253  0.001  24.705195  0.014478  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "802        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "803   5628000  2413.001019          18.610931  2291.35066 -0.944726   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "803     -0.20843     0.000252  0.001  24.805667  0.014631  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "803        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "804   5635000  2415.664856          18.619512  2294.005862 -0.997421   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "804    -0.211195     0.000246  0.001  24.724026  0.014408  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "804        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "805   5642000  2418.621018          18.626799  2296.954673 -0.788513   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "805    -0.208652     0.000252  0.001  24.671748  0.014542  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "805        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "806   5649000  2421.448515          18.635119  2299.773791 -0.492045   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "806    -0.210889     0.000239  0.001  24.408542  0.014263  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "806        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "807   5656000  2424.246209          18.642279  2302.564274 -0.966233   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "807    -0.211044     0.000243  0.001  24.423122  0.014443  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "807        28.40909     27.149452  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "808   5663000  2427.1432          18.650033  2305.453458 -0.899754   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "808    -0.209648     0.000244  0.001  24.670793  0.014386  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "808        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "809   5670000  2429.941863           18.65933  2308.242766 -0.8152   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "809    -0.209807     0.000248  0.001  24.75292  0.014474  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "809        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "810   5677000  2432.766317          18.667045  2311.059453 -0.506569   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "810    -0.208744     0.000246  0.001  24.684913  0.014397  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "810        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "811   5684000  2435.430261          18.674932  2313.715452 -0.667924   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "811    -0.210097     0.000257  0.001  24.69024  0.014465  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "811        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "812   5691000  2438.421202          18.682811  2316.698467 -0.852062   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "812     -0.21029     0.000246  0.001  24.718969  0.014508  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "812        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "813   5698000  2441.239449          18.690472    2319.509 -0.566103   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "813    -0.209568     0.000247  0.001  24.55684  0.014358  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "813        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time    score  \\\n",
      "814   5705000  2443.821871           18.69815  2322.08369 -0.73883   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "814    -0.209425     0.000247  0.001  24.585014   0.01442  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "814        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "815   5712000  2446.568042          18.706044  2324.821915 -0.836872   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "815     -0.20973     0.000252  0.001  24.495696  0.014423  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "815        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "816   5719000  2449.539745          18.714055  2327.78554 -0.422867   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "816    -0.208058     0.000246  0.001  24.407802   0.01435  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "816        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "817   5726000  2452.579309          18.723616  2330.815466 -0.688626   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "817    -0.210696     0.000242  0.001  24.634136  0.014355  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "817        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "818   5733000  2455.396867          18.731711  2333.624867 -0.474448   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "818    -0.208926      0.00025  0.001  24.682638  0.014497  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "818        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "819   5740000  2458.335553          18.740745  2336.554462 -0.869316   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "819    -0.210009     0.000253  0.001  24.562297  0.014598  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "819        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "820   5747000  2461.23918          18.749428  2339.449305 -0.710674   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "820    -0.208555     0.000245  0.001  24.570283  0.014363  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "820        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "821   5754000  2464.09052          18.757537  2342.29249 -0.731347   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "821    -0.210904     0.000241  0.001  24.606796  0.014296  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "821        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "822   5761000  2466.764704          18.765393  2344.958762 -0.828285   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "822    -0.208645     0.000247  0.001  24.539296  0.014388  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "822        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "823   5768000  2469.664641          18.775248  2347.848793 -0.854347   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "823    -0.210314     0.000244  0.001  24.729021  0.014329  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "823        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "824   5775000  2472.761528          18.782997  2350.937847 -0.62859   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "824    -0.210124     0.000246  0.001  24.632168  0.014473  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "824        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "825   5782000  2475.563137          18.791744  2353.730639 -0.623363   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "825    -0.210535     0.000244  0.001  24.720432  0.014373  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "825        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "826   5789000  2478.36005          18.798986  2356.520259 -0.873306   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "826    -0.210136     0.000247  0.001  24.76643  0.014497  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "826        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time   train_time    score  \\\n",
      "827   5796000  2481.20896          18.807529  2359.360571 -0.76839   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "827    -0.211046     0.000241  0.001  24.584272  0.014352  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "827        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "828   5803000  2484.206401          18.817064  2362.348416 -0.637202   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "828    -0.211045     0.000248  0.001  24.730514  0.014382  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "828        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "829   5810000  2487.118627          18.824712  2365.252942 -0.822382   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "829    -0.210064     0.000239  0.001  24.668863  0.014332  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "829        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "830   5817000  2489.879242          18.832552  2368.005668 -0.683915   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "830    -0.209808     0.000245  0.001  24.561383  0.014272  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "830        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "831   5824000  2492.756716          18.840773  2370.87487 -0.737213   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "831    -0.211418     0.000246  0.001  24.666574   0.01448  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "831        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "832   5831000  2495.605554          18.848229  2373.716205 -0.715627   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "832    -0.209896     0.000249  0.001  24.728599  0.014555  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "832        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "833   5838000  2498.372635          18.856664  2376.474793 -0.395241   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "833    -0.211033     0.000241  0.001  24.594478  0.014318  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "833        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "834   5845000  2501.100838          18.864287  2379.195311 -0.367051   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "834    -0.209851     0.000237  0.001  24.533071  0.014109  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "834        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "835   5852000  2503.822446           18.87201  2381.909142 -0.481261   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "835    -0.209868     0.000241  0.001  24.550505  0.014406  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "835        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "836   5859000  2506.644583          18.879709  2384.723525 -0.710635   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "836    -0.212028     0.000247  0.001  24.612474  0.014423  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "836        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "837   5866000  2509.439156          18.887811  2387.509945 -0.467551   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "837     -0.20892     0.000242  0.001  24.526976  0.014346  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "837        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "838   5873000  2512.363294          18.896291  2390.42555 -0.643201   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "838    -0.209177     0.000243  0.001  24.613838  0.014384  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "838        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "839   5880000  2515.006404          18.903283  2393.061614 -0.593561   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "839    -0.210482     0.000238  0.001  24.587513  0.014145  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "839        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "840   5887000  2517.844723          18.910422  2395.892736 -0.85737   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "840    -0.211441     0.000248  0.001  24.582172  0.014394  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "840        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "841   5894000  2520.681764          18.917928  2398.722222 -0.75332   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "841    -0.208978      0.00024  0.001  24.512922  0.014257  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "841        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "842   5901000  2523.52052           18.92675  2401.552104 -0.819971   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "842    -0.210963      0.00024  0.001  24.616631  0.014232  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "842        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "843   5908000  2526.264106          18.935047  2404.287337 -0.649105   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "843    -0.209451     0.000237  0.001  24.649866  0.014282  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "843        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "844   5915000  2529.026218          18.942811  2407.041633 -0.330559   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "844    -0.211399     0.000236  0.001  24.672532   0.01411  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "844        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "845   5922000  2531.690259          18.950309  2409.698092 -0.521312   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "845    -0.210504     0.000234  0.001  24.65491  0.014083  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "845        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "846   5929000  2534.425427          18.957953  2412.425566 -0.664736   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "846    -0.212027     0.000242  0.001  24.650801  0.014298  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "846        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "847   5936000  2537.380749          18.965837  2415.37295 -0.760444   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "847    -0.211778     0.000244  0.001  24.586389  0.014427  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "847        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time  train_time     score  \\\n",
      "848   5943000  2540.34038          18.975108  2418.32325 -0.792738   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "848    -0.209918     0.000247  0.001  24.604786   0.01444  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "848        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "849   5950000  2543.125765          18.984678  2421.099005 -0.918072   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "849    -0.210439      0.00024  0.001  24.628835  0.014137  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "849        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "850   5957000  2546.001186          18.993058  2423.965983 -0.665077   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "850    -0.213192     0.000251  0.001  24.59162  0.014435  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "850        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "851   5964000  2548.752568          19.000732  2426.709639 -0.276266   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "851    -0.210685     0.000244  0.001  24.695331  0.014319  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "851        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "852   5971000  2551.580664          19.009456  2429.528954 -0.628883   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "852    -0.210979      0.00024  0.001  24.526377  0.014304  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "852        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "853   5978000  2554.458464          19.017474  2432.398688 -0.865737   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "853    -0.210722     0.000244  0.001  24.532042  0.014398  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "853        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "854   5985000  2557.196949          19.025272  2435.129326 -0.483873   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "854    -0.209897     0.000245  0.001  24.630195  0.014397  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "854        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "855   5992000  2560.007534          19.032289  2437.932839 -0.970945   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "855    -0.211227     0.000241  0.001  24.700593   0.01428  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "855        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "856   5999000  2562.852885          19.040198  2440.770226 -0.654922   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "856    -0.209626     0.000245  0.001  24.574051  0.014298  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "856        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "857   6006000  2565.702253          19.047968  2443.611774 -0.483412   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "857    -0.211401     0.000249  0.001  24.385422  0.014385  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "857        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "858   6013000  2568.452173          19.055645  2446.353966 -0.376768   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "858    -0.212209     0.000238  0.001  24.451047  0.014195  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "858        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "859   6020000  2571.221313           19.06437  2449.114301 -0.749022   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "859    -0.210166     0.000241  0.001  24.680522  0.014318  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "859        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "860   6027000  2574.022523          19.072135  2451.907694 -0.354955   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "860    -0.209686     0.000248  0.001  24.685472  0.014511  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "860        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "861   6034000  2576.795794          19.081466  2454.671578 -0.741205   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "861    -0.211899     0.000246  0.001  24.637748  0.014311  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "861        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "862   6041000  2579.521702          19.088593  2457.390307 -0.614738   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "862    -0.209787     0.000244  0.001  24.530132  0.014316  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "862        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "863   6048000  2582.189788          19.097018  2460.049913 -0.433999   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "863    -0.211874     0.000237  0.001  24.324683  0.014198  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "863        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "864   6055000  2585.065555          19.104693  2462.917953 -0.32148   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "864    -0.210223     0.000246  0.001  24.560972  0.014303  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "864        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "865   6062000  2587.815318          19.111363  2465.660997 -0.42899   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "865    -0.210069     0.000248  0.001  24.556831  0.014274  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "865        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "866   6069000  2590.691328          19.117705  2468.530618 -0.357883   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "866    -0.211057     0.000238  0.001  24.700966  0.014222  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "866        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "867   6076000  2593.461931          19.124848  2471.294026 -0.843955   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "867     -0.21142     0.000242  0.001  24.591671   0.01425  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "867        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "868   6083000  2596.238501          19.132904  2474.062479 -0.421524   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "868     -0.21027     0.000243  0.001  24.726749  0.014237  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "868        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "869   6090000  2598.97371          19.141968  2476.788567 -0.762325   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "869    -0.209087     0.000245  0.001  24.659554  0.014364  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "869        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "870   6097000  2601.857674          19.148697  2479.665732 -0.538971   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "870    -0.209272     0.000243  0.001  24.516839  0.014258  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "870        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "871   6104000  2604.640355          19.156027  2482.441029 -0.716769   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "871     -0.21076     0.000243  0.001  24.510306  0.014188  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "871        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "872   6111000  2607.472784          19.163567  2485.265864 -0.845063   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "872    -0.210576     0.000247  0.001  24.529697  0.014338  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "872        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "873   6118000  2610.176053          19.171104  2487.961547 -0.985948   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "873    -0.211096      0.00024  0.001  24.550503  0.014245  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "873        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "874   6125000  2612.971669          19.178863  2490.749349 -0.49131   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "874    -0.211241     0.000239  0.001  24.604644  0.014321  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "874        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "875   6132000  2615.717281           19.18645  2493.487318 -0.667406   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "875    -0.210167      0.00024  0.001  24.670389  0.014225  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "875        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "876   6139000  2618.508236          19.194363  2496.270309 -0.420448   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "876    -0.210519     0.000235  0.001  24.434037  0.014072  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "876        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "877   6146000  2621.260687          19.200216  2499.016863 -0.433666   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "877    -0.209223     0.000237  0.001  24.473099  0.014085  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "877        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "878   6153000  2624.124386           19.20727  2501.873452 -0.766964   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "878    -0.210354     0.000248  0.001  24.640272   0.01436  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "878        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "879   6160000  2627.087877          19.215033  2504.829131 -0.559221   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "879    -0.210129     0.000251  0.001  24.618114  0.014489  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "879        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "880   6167000  2630.040957          19.222561  2507.774634 -0.296597   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "880    -0.210128     0.000243  0.001  24.616977  0.014365  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "880        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "881   6174000  2633.038739          19.234798  2510.760123 -0.643058   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "881    -0.210785     0.000242  0.001  24.733297  0.014374  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "881        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "882   6181000  2636.255157          19.242409  2513.96888 -0.946958   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "882    -0.212149     0.000242  0.001  24.68977  0.014198  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "882        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "883   6188000  2639.565544          19.249742  2517.271884 -0.651203   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "883    -0.211889     0.000246  0.001  24.724415   0.01435  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "883        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "884   6195000  2642.787503           19.25766  2520.485869 -0.552964   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "884    -0.210339     0.000244  0.001  24.764925  0.014376  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "884        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "885   6202000  2646.016404          19.265707  2523.706659 -0.897768   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "885    -0.209603      0.00024  0.001  24.752729  0.014272  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "885        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "886   6209000  2649.127965          19.273869  2526.810007 -0.797691   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "886    -0.211477      0.00024  0.001  24.608864  0.014218  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "886        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "887   6216000  2652.176774          19.280846  2529.851786 -0.669429   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "887     -0.21084     0.000241  0.001  24.322558  0.014198  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "887        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "888   6223000  2655.308664          19.288891  2532.975578 -0.69394   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "888     -0.21106     0.000236  0.001  24.524892  0.014177  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "888        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "889   6230000  2658.394779          19.296977  2536.053546 -0.755182   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "889    -0.209983     0.000242  0.001  24.801353  0.014175  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "889        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "890   6237000  2661.218762          19.305045  2538.869386 -0.397973   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "890    -0.210162     0.000243  0.001  24.639172  0.014334  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "890        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "891   6244000  2664.154056          19.312394  2541.797252 -0.847721   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha  entropy  td_error  return  \\\n",
      "891    -0.210881     0.000247  0.001  24.6158  0.014348  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "891        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "892   6251000  2667.137172          19.321654  2544.771061 -0.571263   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "892     -0.21188     0.000231  0.001  24.548941  0.013936  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "892        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "893   6258000  2670.111907          19.331707  2547.73567 -0.393614   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "893    -0.211382      0.00024  0.001  24.541014  0.014245  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "893        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "894   6265000  2673.024477          19.340839  2550.639051 -0.692857   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "894    -0.209797     0.000233  0.001  24.546861  0.014208  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "894        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "895   6272000  2676.044708          19.350331  2553.649731 -0.748187   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "895    -0.211069      0.00023  0.001  24.803785  0.014004  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "895        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "896   6279000  2679.065536          19.358483  2556.662356 -0.47618   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error  return  \\\n",
      "896    -0.210203     0.000244  0.001  24.72584   0.01436  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "896        28.40909     27.149452  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "897   6286000  2681.97574           19.36544  2559.565531 -0.730132   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "897    -0.212244     0.000245  0.001  24.554471  0.014272  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "897        28.40909     27.149452  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "898   6293000  2684.850474          19.372725  2562.432922 -0.5973   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error  return  \\\n",
      "898     -0.21115     0.000235  0.001  24.626464  0.014123  0.0138   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "898        28.40909     27.149452  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "899   6300000  2701.660437          19.380404  2565.411603 -0.723169   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "899    -0.212316     0.000232  0.001  24.605127  0.013933  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "899       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "900   6307000  2704.680769          19.389917  2568.422358 -0.26645   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "900    -0.209495      0.00023  0.001  24.637315  0.014082  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "900       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time   score  \\\n",
      "901   6314000  2707.591922          19.400791  2571.322587 -0.6982   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "901    -0.212432     0.000234  0.001  24.674427   0.01411  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "901       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "902   6321000  2710.351729          19.411409  2574.071718 -1.058934   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "902    -0.210159     0.000236  0.001  24.738364  0.014029  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "902       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "903   6328000  2713.406007          19.423562  2577.113781 -0.931741   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "903     -0.21158      0.00024  0.001  24.682242   0.01423  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "903       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "904   6335000  2716.144596          19.433928  2579.841949 -0.669593   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "904    -0.212519     0.000236  0.001  24.620425  0.014086  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "904       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "905   6342000  2718.988472          19.443856  2582.675845 -0.422954   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "905    -0.211141     0.000245  0.001  24.677917  0.014358  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "905       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "906   6349000  2722.032157          19.454617  2585.708712 -0.393187   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "906    -0.212239     0.000237  0.001  24.566306  0.014146  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "906       30.030029     26.989843  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "907   6356000  2725.18995          19.465036  2588.856033 -0.661668   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "907    -0.212196     0.000237  0.001  24.658066  0.014246  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "907       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "908   6363000  2728.046442          19.475978  2591.701531 -0.832809   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "908    -0.213103      0.00024  0.001  24.619855  0.014164  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "908       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "909   6370000  2730.871233          19.485539  2594.516709 -0.695925   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "909    -0.210825     0.000242  0.001  24.721605  0.014205  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "909       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "910   6377000  2733.841779          19.495185  2597.477534 -0.697486   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "910    -0.211887      0.00024  0.001  24.612598  0.014159  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "910       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "911   6384000  2736.889772          19.506985  2600.51367 -0.677992   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "911    -0.211176      0.00024  0.001  24.622926  0.014279  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "911       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "912   6391000  2739.829076          19.516683  2603.443224 -0.67714   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "912    -0.210718      0.00024  0.001  24.691808  0.014325  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "912       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "913   6398000  2742.836022          19.526302  2606.440487 -0.554778   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "913    -0.210513      0.00024  0.001  24.698176  0.014222  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "913       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "914   6405000  2745.790163          19.537006  2609.383863 -0.884255   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "914    -0.210537     0.000237  0.001  24.751836  0.014079  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "914       30.030029     26.989843  \n",
      "     timestep        time  collect_wait_time   train_time     score  \\\n",
      "915   6412000  2748.72726          19.546626  2612.311279 -0.598973   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "915    -0.210697      0.00024  0.001  24.672668  0.014263  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "915       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "916   6419000  2751.706038          19.556206  2615.280424 -0.561992   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "916     -0.21166     0.000242  0.001  24.582773  0.014219  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "916       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "917   6426000  2754.654556          19.565288  2618.219811 -0.641893   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "917    -0.211324      0.00024  0.001  24.726981  0.014189  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "917       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "918   6433000  2757.713625          19.573302  2621.270808 -0.645088   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "918    -0.212138     0.000246  0.001  24.84061  0.014313  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "918       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "919   6440000  2760.677977           19.58198  2624.226428 -0.549478   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "919    -0.212449     0.000241  0.001  24.761075  0.014235  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "919       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "920   6447000  2763.684332          19.590922  2627.223788 -0.549752   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "920    -0.211435     0.000234  0.001  24.663627  0.013999  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "920       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "921   6454000  2766.559036          19.599935  2630.089428 -0.588509   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "921    -0.212805     0.000239  0.001  24.499978  0.014131  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "921       30.030029     26.989843  \n",
      "     timestep       time  collect_wait_time   train_time     score  \\\n",
      "922   6461000  2769.5837          19.608483  2633.105487 -0.583188   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "922    -0.210858     0.000231  0.001  24.539385  0.013992  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "922       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "923   6468000  2772.686318          19.616637  2636.199875 -0.466679   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "923    -0.210644     0.000234  0.001  24.559874  0.014081  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "923       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "924   6475000  2775.674881          19.624919  2639.180103 -0.674587   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "924    -0.211063     0.000242  0.001  24.584745  0.014264  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "924       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "925   6482000  2778.954165           19.63263  2642.451622 -0.295241   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "925    -0.211853     0.000233  0.001  24.736861  0.014139  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "925       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "926   6489000  2781.891595          19.641564  2645.380062 -0.336782   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "926    -0.210461      0.00024  0.001  24.803171  0.014234  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "926       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "927   6496000  2784.897385          19.649718  2648.377651 -0.300734   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "927     -0.21135     0.000242  0.001  24.721685  0.014185  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "927       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "928   6503000  2787.862884          19.657337  2651.33548 -0.357261   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "928    -0.213017      0.00024  0.001  24.777105  0.014151  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "928       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "929   6510000  2790.669843          19.664531  2654.135186 -0.623804   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "929    -0.212068     0.000242  0.001  24.830241  0.014233  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "929       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "930   6517000  2793.870662          19.673639  2657.326755 -0.833853   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "930    -0.212224     0.000239  0.001  24.660133  0.014191  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "930       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "931   6524000  2797.116828          19.683304  2660.563179 -0.741454   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "931    -0.211227     0.000238  0.001  24.652546  0.014051  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "931       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "932   6531000  2800.160948          19.695257  2663.595273 -0.558952   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "932    -0.209927     0.000234  0.001  24.624388  0.014043  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "932       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "933   6538000  2803.498079          19.706268  2666.92133 -0.751706   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "933    -0.212204     0.000232  0.001  24.550008  0.013967  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "933       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "934   6545000  2806.884487          19.714874  2670.299083 -0.671799   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "934    -0.212374     0.000238  0.001  24.56643   0.01409  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "934       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "935   6552000  2810.096322          19.725582  2673.500132 -0.275938   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "935    -0.210783     0.000233  0.001  24.622038   0.01407  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "935       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "936   6559000  2813.312274          19.734806  2676.706798 -0.480564   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "936    -0.211649     0.000238  0.001  24.773243  0.014171  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "936       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "937   6566000  2816.289119          19.743614  2679.67477 -0.552355   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha   entropy  td_error    return  \\\n",
      "937     -0.21066     0.000236  0.001  24.86164  0.014041  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "937       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "938   6573000  2819.302212          19.754415  2682.677005 -0.712498   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "938    -0.212876     0.000238  0.001  24.760943  0.014184  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "938       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "939   6580000  2822.198174          19.762603  2685.564723 -0.865199   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "939    -0.211679     0.000232  0.001  24.655533  0.013973  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "939       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "940   6587000  2825.166517          19.770779  2688.524835 -0.710378   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "940    -0.209129     0.000236  0.001  24.751256  0.014134  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "940       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "941   6594000  2828.160346           19.77879  2691.510592 -0.573576   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "941     -0.21089     0.000236  0.001  24.739333  0.013988  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "941       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "942   6601000  2831.436182          19.786664  2694.778498 -0.678256   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "942    -0.212201     0.000237  0.001  24.693131  0.014183  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "942       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "943   6608000  2834.549988          19.794769  2697.88414 -0.690179   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "943    -0.210832     0.000238  0.001  24.699039  0.014163  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "943       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "944   6615000  2837.288782          19.803094  2700.614555 -0.679443   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "944      -0.2125     0.000229  0.001  24.613742  0.013916  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "944       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time    score  \\\n",
      "945   6622000  2840.616407          19.810251  2703.934973 -0.71272   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "945    -0.213951     0.000233  0.001  24.577224  0.013977  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "945       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "946   6629000  2843.997155          19.816624  2707.309293 -0.567733   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "946     -0.21181     0.000232  0.001  24.674965  0.014062  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "946       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time   train_time     score  \\\n",
      "947   6636000  2847.138185          19.827917  2710.438943 -0.512505   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "947    -0.212458      0.00023  0.001  24.680905  0.013919  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "947       30.030029     26.989843  \n",
      "     timestep         time  collect_wait_time  train_time     score  \\\n",
      "948   6643000  2850.238627          19.838622  2713.52863 -0.595062   \n",
      "\n",
      "     policy_loss  qvalue_loss  alpha    entropy  td_error    return  \\\n",
      "948    -0.210886     0.000236  0.001  24.709151  0.014104  0.012597   \n",
      "\n",
      "     episode_length  eval_entropy  \n",
      "948       30.030029     26.989843  \n"
     ]
    }
   ],
   "source": [
    "eval_env = create_env(graphics=False, time_scale=TIME_SCALE)\n",
    "short_watch.start(); long_watch.start()\n",
    "for i, data in enumerate(collector):\n",
    "    # 1. Extend\n",
    "    replay_buffer.extend(data)\n",
    "    logger.add({\"collect_wait_time\": short_watch.end(), \"timestep\": ENV_STEPS})\n",
    "    avg_reward = data[\"next\", \"reward\"].mean().cpu().item()\n",
    "    score = (avg_reward / BENCH_AVG_REWARD) * 100\n",
    "    logger.acc({\"score\": score})\n",
    "\n",
    "    # 2. Train Loop\n",
    "    short_watch.start()\n",
    "    for j in range(TRAIN_STEPS):\n",
    "        batch = replay_buffer.sample().to(device)\n",
    "\n",
    "        # 1. SAC Loss\n",
    "        loss_data = loss_module(batch)\n",
    "        loss = loss_data[\"loss_actor\"] + loss_data[\"loss_qvalue\"] + loss_data[\"loss_alpha\"]\n",
    "        \n",
    "        # 2. Update\n",
    "        optimizer.zero_grad(); loss.backward()\n",
    "        optimizer.step(); target_updater.step()\n",
    "\n",
    "        # 3. Log Metrics\n",
    "        logger.acc({\n",
    "            \"policy_loss\":  loss_data[\"loss_actor\"].detach().cpu().item(),\n",
    "            \"qvalue_loss\":  loss_data[\"loss_qvalue\"].detach().cpu().item(),\n",
    "            \"entropy\":      loss_data[\"entropy\"].detach().cpu().item(),\n",
    "            \"td_error\":     batch[\"td_error\"].detach().mean().cpu().item(),\n",
    "            \"alpha\":        loss_data[\"alpha\"].detach().mean().cpu().item(),\n",
    "        }, mode='ema')\n",
    "    logger.add({\"train_time\": short_watch.end()})\n",
    "\n",
    "\n",
    "    # 3. Sync Policy\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # 4. Checkpoint and eval\n",
    "    if (i+1) % CKPT_EVAL_INTERVAL == 0:\n",
    "        eval_env.reset()\n",
    "        with torch.no_grad():\n",
    "            data = eval_env.rollout(EVAL_STEPS, policy=policy, break_when_any_done=False, auto_cast_to_device=True)\n",
    "        metrics = metric_module(data)\n",
    "        metrics[\"eval_entropy\"] = metrics[\"entropy\"]\n",
    "        del metrics[\"entropy\"]\n",
    "        logger.acc(metrics)\n",
    "        checkpointer.save_progress(state_obj={\n",
    "            \"timestep\": logger.last()[\"timestep\"],\n",
    "            \"loss_module\": loss_module.state_dict(),\n",
    "        })\n",
    "    else:\n",
    "        logger.add({key: 0 for key in [\"return\", \"episode_length\", \"eval_entropy\"]})\n",
    "\n",
    "    # 5. Log\n",
    "    logger.add({\"time\": long_watch.end()})\n",
    "    short_watch.start(); long_watch.start()\n",
    "    logger.next(print_row=True)\n",
    "collector.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "def plot(name, alpha=0):\n",
    "    # Get dataframe\n",
    "    logger = Logger(keys = LOG_KEYS, log_path=LOG_PATH, name=name)\n",
    "    logger.revert()\n",
    "    df = logger.dataframe()\n",
    "    df = df.set_index(LOG_INDEX)\n",
    "    if alpha>0:\n",
    "        df = df.ewm(alpha=alpha).mean()\n",
    "\n",
    "    # Plot it\n",
    "    rows = (len(df.columns) + 3) // 4\n",
    "    df.plot(subplots=True, layout=(rows,4), figsize=(15, int(rows* 7/3)))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_PATH, f\"{name}.png\"))\n",
    "plot(\"run0\", alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/transforms/transforms.py:587: FutureWarning: The key 'continuous_action' is unaccounted for by the transform (expected keys ['VectorSensor_size243', 'done', 'terminated', 'truncated', 'group_reward', 'reward']). Every new entry in the tensordict resulting from a call to a transform must be registered in the specs for torchrl rollouts to be consistently built. Make sure transform_output_spec/transform_observation_spec/... is coded correctly. This warning will trigger a KeyError in v0.9, make sure to adapt your code accordingly.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        done: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 243]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([10]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = create_env(graphics=True, time_scale=5)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    data = env.rollout(1000, policy=policy, break_when_any_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'return': 4.451180934906006,\n",
       " 'episode_length': 909.0908813476562,\n",
       " 'entropy': 14.313319206237793}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleMetricModule(mode=\"approx\")(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
