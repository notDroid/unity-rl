{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Walker SAC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.util import MultiVersionCheckpointer, Checkpointer, Logger, SimpleMetricModule\n",
    "from config import *\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from env import create_env\n",
    "\n",
    "# Import Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Models and Loss\n",
    "from rlkit.models import MLP\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "# Util\n",
    "from torchrl.objectives import SACLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(model_config):\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"out_features\"] *= 2\n",
    "    model = MLP(**model_config)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        model,\n",
    "        NormalParamExtractor()\n",
    "    )\n",
    "    model = TensorDictModule(model, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "    \n",
    "    policy = ProbabilisticActor(\n",
    "        module=model,  \n",
    "        distribution_class=TanhNormal,\n",
    "\n",
    "        in_keys=[\"loc\", \"scale\"],\n",
    "        out_keys=[\"action\"],\n",
    "\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=\"log_prob\",\n",
    "        cache_dist=True,\n",
    "    )\n",
    "\n",
    "    return policy\n",
    "\n",
    "def create_qvalue(model_config):\n",
    "    # Remove out_features from config\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"in_features\"] = model_config[\"in_features\"] + model_config[\"out_features\"]\n",
    "    model_config[\"out_features\"] = 1\n",
    "\n",
    "    model = MLP(**model_config)\n",
    "    qvalue = TensorDictModule(model, in_keys=[\"observation\", \"action\"], out_keys=[\"state_action_value\"])\n",
    "    return qvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = create_policy(MODEL_CONFIG)\n",
    "qvalue = create_qvalue(MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = SACLoss(\n",
    "    actor_network=policy, qvalue_network=qvalue, value_network=None,\n",
    "    num_qvalue_nets=2,\n",
    "    alpha_init=0.1, fixed_alpha=True, \n",
    "    delay_actor=False, delay_qvalue=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/var/folders/l_/gtffst3j133f47568b_fd9zr0000gn/T/ml-agents-binaries/binaries/Walker-0b2d628333f5f2f77e17caa072661a50/Startup/Startup.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/var/folders/l_/gtffst3j133f47568b_fd9zr0000gn/T/ml-agents-binaries/binaries/Walker-0b2d628333f5f2f77e17caa072661a50/Startup/Startup.app/Contents/MonoBleedingEdge/etc'\n",
      "New input system (experimental) initialized\n",
      "Initialize engine version: 2022.3.4f1 (35713cd46cd7)\n",
      "[Subsystems] Discovering subsystems at path /var/folders/l_/gtffst3j133f47568b_fd9zr0000gn/T/ml-agents-binaries/binaries/Walker-0b2d628333f5f2f77e17caa072661a50/Startup/Startup.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; threaded=0; jobified=0\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "- Loaded All Assemblies, in  0.172 seconds\n",
      "- Finished resetting the current domain, in  0.003 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "UnloadTime: 1.005792 ms\n",
      "Command line arguments passed: /var/folders/l_/gtffst3j133f47568b_fd9zr0000gn/T/ml-agents-binaries/binaries/Walker-0b2d628333f5f2f77e17caa072661a50/Startup/Startup.app/Contents/MacOS/UnityEnvironment -nographics -batchmode --mlagents-port 11000 --mlagents-scene-name Assets/ML-Agents/Examples/Walker/Scenes/Walker.unity\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "Unloading 7 Unused Serialized files (Serialized files now loaded: 0)\n",
      "UnloadTime: 0.247292 ms\n",
      "Registered Communicator in Agent.\n",
      "Unloading 6 unused Assets to reduce memory usage. Loaded Objects now: 4054.\n",
      "Total: 1.324250 ms (FindLiveObjects: 0.206583 ms CreateObjectMapping: 0.030625 ms MarkObjects: 1.053291 ms  DeleteObjects: 0.033416 ms)\n",
      "\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "Setting up 7 worker threads for Enlighten.\n",
      "Memory Statistics:\n",
      "[ALLOC_TEMP_TLS] TLS Allocator\n",
      "  StackAllocators : \n",
      "    [ALLOC_TEMP_MAIN]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 1 frames\n",
      "      Initial Block Size 4.0 MB\n",
      "      Current Block Size 4.0 MB\n",
      "      Peak Allocated Bytes 2.1 MB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 4]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.PreloadManager]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 127.6 KB\n",
      "      Overflow Count 9\n",
      "    [ALLOC_TEMP_Background Job.worker 3]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 11]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 7]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 6]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 11]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 12]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 2]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 9]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 0]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 4]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 5]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 8]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 10]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 1]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_EnlightenWorker] x 7\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 1]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 8.3 KB\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 3]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 5]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 12]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_AssetGarbageCollectorHelper] x 13\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 13]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 15]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 2]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 10]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 7]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_OSX HID Input]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 9]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 0]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 6]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Job.worker 8]\n",
      "      Initial Block Size 256.0 KB\n",
      "      Current Block Size 256.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Background Job.worker 14]\n",
      "      Initial Block Size 32.0 KB\n",
      "      Current Block Size 32.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_Loading.AsyncRead]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 349 B\n",
      "      Overflow Count 0\n",
      "    [ALLOC_TEMP_BatchDeleteObjects]\n",
      "      Initial Block Size 64.0 KB\n",
      "      Current Block Size 64.0 KB\n",
      "      Peak Allocated Bytes 0 B\n",
      "      Overflow Count 0\n",
      "[ALLOC_DEFAULT] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 29\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.4 MB\n",
      "    [ALLOC_DEFAULT_MAIN]\n",
      "      Peak usage frame count: [16.0 MB-32.0 MB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 29.6 MB\n",
      "      Peak Large allocation bytes 16.0 MB\n",
      "    [ALLOC_DEFAULT_THREAD]\n",
      "      Peak usage frame count: [2.0 MB-4.0 MB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 2.2 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TEMP_JOB_1_FRAME]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_2_FRAMES]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 0\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_4_FRAMES (JobTemp)]\n",
      "  Initial Block Size 2.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_TEMP_JOB_ASYNC (Background)]\n",
      "  Initial Block Size 1.0 MB\n",
      "  Used Block Count 1\n",
      "  Overflow Count (too large) 0\n",
      "  Overflow Count (full) 0\n",
      "[ALLOC_GFX] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.4 MB\n",
      "    [ALLOC_GFX_MAIN]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 65.8 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_GFX_THREAD]\n",
      "      Peak usage frame count: [64.0 KB-128.0 KB]: 1 frames\n",
      "      Requested Block Size 16.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 82.5 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_CACHEOBJECTS] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 31\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.4 MB\n",
      "    [ALLOC_CACHEOBJECTS_MAIN]\n",
      "      Peak usage frame count: [0.5 MB-1.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 0.6 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_CACHEOBJECTS_THREAD]\n",
      "      Peak usage frame count: [4.0 MB-8.0 MB]: 1 frames\n",
      "      Requested Block Size 4.0 MB\n",
      "      Peak Block count 2\n",
      "      Peak Allocated memory 4.5 MB\n",
      "      Peak Large allocation bytes 0 B\n",
      "[ALLOC_TYPETREE] Dual Thread Allocator\n",
      "  Peak main deferred allocation count 0\n",
      "    [ALLOC_BUCKET]\n",
      "      Large Block size 4.0 MB\n",
      "      Used Block count 1\n",
      "      Peak Allocated bytes 1.4 MB\n",
      "    [ALLOC_TYPETREE_MAIN]\n",
      "      Peak usage frame count: [0-1.0 KB]: 1 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 112 B\n",
      "      Peak Large allocation bytes 0 B\n",
      "    [ALLOC_TYPETREE_THREAD]\n",
      "      Peak usage frame count: [4.0 KB-8.0 KB]: 1 frames\n",
      "      Requested Block Size 2.0 MB\n",
      "      Peak Block count 1\n",
      "      Peak Allocated memory 7.9 KB\n",
      "      Peak Large allocation bytes 0 B\n",
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = create_env(graphics=False, time_scale=10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = env.rollout(100, policy=policy, break_when_any_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/common.py:40: UserWarning: No target network updater has been associated with this loss module, but target parameters have been found. While this is supported, it is expected that the target network updates will be manually performed. You can deactivate this warning by turning the RL_WARNINGS env variable to False.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/common.py:456: UserWarning: No target network updater has been associated with this loss module, but target parameters have been found. While this is supported, it is expected that the target network updates will be manually performed. You can deactivate this warning by turning the RL_WARNINGS env variable to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MLP.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: TensorDictModule failed with operation\n    MLP(\n      (proj_in): Linear(in_features=282, out_features=512, bias=True)\n      (mlp_blocks): ModuleList(\n        (0-2): 3 x MLPBlock(\n          (norm): RMSNorm((512,), eps=None, elementwise_affine=True)\n          (geglu): GeGLU(\n            (linear): Linear(in_features=512, out_features=3072, bias=True)\n            (gelu): GELU(approximate='none')\n          )\n          (proj_down): Linear(in_features=1536, out_features=512, bias=True)\n        )\n      )\n      (proj_out): Sequential(\n        (0): RMSNorm((512,), eps=None, elementwise_affine=True)\n        (1): Linear(in_features=512, out_features=1, bias=True)\n      )\n    )\n    in_keys=['observation', 'action']\n    out_keys=['state_action_value'].",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/walimirza/unity-rl/experiments/Walker/sac/rl.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/walimirza/unity-rl/experiments/Walker/sac/rl.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/walimirza/unity-rl/experiments/Walker/sac/rl.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loss_module(data)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1879\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[39mreturn\u001b[39;00m inner()\n\u001b[1;32m   1878\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1879\u001b[0m     \u001b[39mreturn\u001b[39;00m inner()\n\u001b[1;32m   1880\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1881\u001b[0m     \u001b[39m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1882\u001b[0m     \u001b[39m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1883\u001b[0m     \u001b[39m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1884\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m _global_forward_hooks\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1827\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1824\u001b[0m     bw_hook \u001b[39m=\u001b[39m BackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1825\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1827\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1828\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1829\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1830\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1831\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1832\u001b[0m     ):\n\u001b[1;32m   1833\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/common.py:54\u001b[0m, in \u001b[0;36m_forward_wrapper.<locals>.new_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m rm\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m     53\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     55\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     em\u001b[39m.\u001b[39m\u001b[39m__exit__\u001b[39m(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py:328\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m _self \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mreturn\u001b[39;00m func(tensordict, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/sac.py:620\u001b[0m, in \u001b[0;36mSACLoss.forward\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    618\u001b[0m     loss_value, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_loss(tensordict)\n\u001b[1;32m    619\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 620\u001b[0m     loss_qvalue, value_metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_qvalue_v2_loss(tensordict)\n\u001b[1;32m    621\u001b[0m     loss_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    622\u001b[0m loss_actor, metadata_actor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actor_loss(tensordict)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/sac.py:918\u001b[0m, in \u001b[0;36mSACLoss._qvalue_v2_loss\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_qvalue_v2_loss\u001b[39m(\n\u001b[1;32m    915\u001b[0m     \u001b[39mself\u001b[39m, tensordict: TensorDictBase\n\u001b[1;32m    916\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[Tensor, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Tensor]]:\n\u001b[1;32m    917\u001b[0m     \u001b[39m# we pass the alpha value to the tensordict. Since it's a scalar, we must erase the batch-size first.\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     target_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_target_v2(tensordict)\n\u001b[1;32m    920\u001b[0m     tensordict_expand \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vmap_qnetworkN0(\n\u001b[1;32m    921\u001b[0m         tensordict\u001b[39m.\u001b[39mselect(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqvalue_network\u001b[39m.\u001b[39min_keys, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m    922\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqvalue_network_params,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m     pred_val \u001b[39m=\u001b[39m tensordict_expand\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor_keys\u001b[39m.\u001b[39mstate_action_value)\u001b[39m.\u001b[39msqueeze(\n\u001b[1;32m    925\u001b[0m         \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m    926\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/sac.py:895\u001b[0m, in \u001b[0;36mSACLoss._compute_target_v2\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    890\u001b[0m         next_sample_log_prob \u001b[39m=\u001b[39m compute_log_prob(\n\u001b[1;32m    891\u001b[0m             next_dist, next_action, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor_keys\u001b[39m.\u001b[39mlog_prob\n\u001b[1;32m    892\u001b[0m         )\n\u001b[1;32m    894\u001b[0m \u001b[39m# get q-values\u001b[39;00m\n\u001b[0;32m--> 895\u001b[0m next_tensordict_expand \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vmap_qnetworkN0(\n\u001b[1;32m    896\u001b[0m     next_tensordict, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_qvalue_network_params\n\u001b[1;32m    897\u001b[0m )\n\u001b[1;32m    898\u001b[0m state_action_value \u001b[39m=\u001b[39m next_tensordict_expand\u001b[39m.\u001b[39mget(\n\u001b[1;32m    899\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor_keys\u001b[39m.\u001b[39mstate_action_value\n\u001b[1;32m    900\u001b[0m )\n\u001b[1;32m    901\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    902\u001b[0m     state_action_value\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(next_sample_log_prob\u001b[39m.\u001b[39mshape) :]\n\u001b[1;32m    903\u001b[0m     \u001b[39m!=\u001b[39m next_sample_log_prob\u001b[39m.\u001b[39mshape\n\u001b[1;32m    904\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/_functorch/apis.py:202\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mreturn\u001b[39;00m vmap_impl(\n\u001b[1;32m    203\u001b[0m         func, in_dims, out_dims, randomness, chunk_size, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    204\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/_functorch/vmap.py:334\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    324\u001b[0m         func,\n\u001b[1;32m    325\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    331\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[39m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m \u001b[39mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    335\u001b[0m     func,\n\u001b[1;32m    336\u001b[0m     batch_size,\n\u001b[1;32m    337\u001b[0m     flat_in_dims,\n\u001b[1;32m    338\u001b[0m     flat_args,\n\u001b[1;32m    339\u001b[0m     args_spec,\n\u001b[1;32m    340\u001b[0m     out_dims,\n\u001b[1;32m    341\u001b[0m     randomness,\n\u001b[1;32m    342\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    343\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/_functorch/vmap.py:484\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[39mas\u001b[39;00m vmap_level:\n\u001b[1;32m    481\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    482\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    483\u001b[0m     )\n\u001b[0;32m--> 484\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mbatched_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/objectives/utils.py:532\u001b[0m, in \u001b[0;36m_vmap_func.<locals>.decorated_module\u001b[0;34m(*module_args_params)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mwith\u001b[39;00m params\u001b[39m.\u001b[39mto_module(module):\n\u001b[1;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m         r \u001b[39m=\u001b[39m module(\u001b[39m*\u001b[39;49mmodule_args)\n\u001b[1;32m    533\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m         r \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, func)(\u001b[39m*\u001b[39mmodule_args)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py:328\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m _self \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    329\u001b[0m \u001b[39mreturn\u001b[39;00m func(tensordict, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/utils.py:372\u001b[0m, in \u001b[0;36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev \u001b[39m=\u001b[39m _skip_existing\u001b[39m.\u001b[39mget_mode()\n\u001b[1;32m    371\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     result \u001b[39m=\u001b[39m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    373\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     _skip_existing\u001b[39m.\u001b[39mset_mode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py:1218\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1216\u001b[0m in_keys \u001b[39m=\u001b[39m indent(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min_keys=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1217\u001b[0m out_keys \u001b[39m=\u001b[39m indent(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mout_keys=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1218\u001b[0m \u001b[39mraise\u001b[39;00m err \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1219\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensorDictModule failed with operation\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00min_keys\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mout_keys\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1220\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py:1190\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m   1185\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSome tensors that are necessary for the module call may \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1186\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnot have not been found in the input tensordict: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1187\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe following inputs are None: \u001b[39m\u001b[39m{\u001b[39;00mnone_set\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1188\u001b[0m         ) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39merr\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1190\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors_out, (\u001b[39mdict\u001b[39m, TensorDictBase)) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m   1192\u001b[0m     key \u001b[39min\u001b[39;00m tensors_out \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_keys\n\u001b[1;32m   1193\u001b[0m ):\n\u001b[1;32m   1194\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors_out, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py:1174\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(  \u001b[39m# type: ignore[unreachable]\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m         tensordict\u001b[39m.\u001b[39m_get_tuple_maybe_non_tensor(\n\u001b[1;32m   1167\u001b[0m             _unravel_key_to_tuple(in_key),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[39mfor\u001b[39;00m in_key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\n\u001b[1;32m   1172\u001b[0m     )\n\u001b[1;32m   1173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1174\u001b[0m     tensors_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_module(tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1175\u001b[0m     \u001b[39mif\u001b[39;00m tensors_out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1176\u001b[0m         tensors_out \u001b[39m=\u001b[39m ()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/tensordict/nn/common.py:1133\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[0;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m kwargs\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod_kwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49mtensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmethod)(\u001b[39m*\u001b[39mtensors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1786\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: MLP.forward() takes 2 positional arguments but 3 were given"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data\n",
    "loss_module(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
