{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Crawler PPO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Common Util\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment\n",
    "from env import create_env\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Model\n",
    "from rlkit.models import MLP\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "### Training\n",
    "# Collection\n",
    "from torchrl.collectors import SyncDataCollector, MultiSyncDataCollector\n",
    "from torchrl.data import ReplayBuffer, LazyMemmapStorage, SliceSamplerWithoutReplacement, SamplerWithoutReplacement\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "# Util\n",
    "from torchrl.collectors.utils import split_trajectories\n",
    "from rlkit.util import Checkpointer, Logger, Stopwatch, round_up, compute_categorical_entropy, compute_traj_reward_info, reduce_dim\n",
    "\n",
    "\n",
    "# Config\n",
    "from config import (\n",
    "    ENV_PATH, \n",
    "    N_ENVS, OBSERVATION_DIM, ACTION_DIM,\n",
    "    LOG_KEYS, LOG_INDEX, BEST_METRIC_KEY,\n",
    "    MODEL_PATH, CKPT_PATH, LOG_PATH, RESULTS_PATH,\n",
    ")\n",
    "\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if str(device).startswith(\"cuda\") else \"cpu\"\n",
    "amp_dtype   = torch.float16 if device_type == \"cuda\" else torch.float32\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: env.close()\n",
    "# except: pass\n",
    "# env = create_env(graphics=False, time_scale=5)\n",
    "# check_env_specs(env)\n",
    "# td = env.rollout(1000, break_when_any_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_specs(env):\n",
    "#     print(\"action_spec:\", env.action_spec)\n",
    "#     print(\"reward_spec:\", env.reward_spec)\n",
    "#     print(\"done_spec:\", env.done_spec)\n",
    "#     print(\"observation_spec:\", env.observation_spec)\n",
    "    \n",
    "# print_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()\n",
    "# act_data = pd.DataFrame(td[\"action\"].reshape(-1))\n",
    "# obs_data = pd.DataFrame(td[\"observation\"].reshape(-1))\n",
    "# obs_data = obs_data.clip(float(obs_data.quantile(0.01).iloc[0]), float(obs_data.quantile(0.99).iloc[0]))\n",
    "# rew_data = pd.DataFrame(td[\"next\", \"reward\"].reshape(-1))\n",
    "\n",
    "# plt.violinplot(obs_data, positions=[0], showmedians=True, showextrema=True, widths=0.9)\n",
    "# plt.violinplot(act_data, positions=[1], showmedians=True, showextrema=True, widths=0.9)\n",
    "# plt.violinplot(rew_data, positions=[2], showmedians=True, showextrema=True, widths=0.9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(model_config):\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"out_features\"] *= 2\n",
    "    model = MLP(**model_config)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        model,\n",
    "        NormalParamExtractor()\n",
    "    )\n",
    "    model = TensorDictModule(model, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "    \n",
    "    policy = ProbabilisticActor(\n",
    "        module=model,  \n",
    "        distribution_class=TanhNormal,\n",
    "\n",
    "        in_keys=[\"loc\", \"scale\"],\n",
    "        out_keys=[\"action\"],\n",
    "\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=\"log_prob\",\n",
    "        cache_dist=True,\n",
    "    )\n",
    "\n",
    "    return policy\n",
    "\n",
    "def create_value(model_config):\n",
    "    # Remove out_features from config\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"out_features\"] = 1\n",
    "\n",
    "    model = MLP(**model_config)\n",
    "    value = TensorDictModule(model, in_keys=[\"observation\"], out_keys=[\"state_value\"])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss_module(policy, value, epsilon, entropy_coef, gamma, lmbda):\n",
    "    loss_module = ClipPPOLoss(\n",
    "        actor_network=policy,\n",
    "        critic_network=value,\n",
    "        clip_epsilon=epsilon,\n",
    "        loss_critic_type=\"smooth_l1\", # Default\n",
    "        entropy_coeff=entropy_coef,\n",
    "    )\n",
    "\n",
    "    loss_module.make_value_estimator(\n",
    "        ValueEstimators.GAE, \n",
    "        time_dim=0,\n",
    "        gamma=gamma, lmbda=lmbda, \n",
    "        shifted=True, average_gae=True,\n",
    "        auto_reset_env=True,\n",
    "    )\n",
    "\n",
    "    # All defaults\n",
    "    loss_module.set_keys(\n",
    "        # From value estimator\n",
    "        advantage='advantage',\n",
    "        value_target='value_target', \n",
    "        value='state_value', \n",
    "        # From policy (should match ProbabilisticActor, this is correctly set automatically key not specified)\n",
    "        sample_log_prob='log_prob',\n",
    "        action='action', \n",
    "        # For value estimator\n",
    "        reward='reward', \n",
    "        done='done', \n",
    "        terminated='terminated',\n",
    "    )\n",
    "\n",
    "    return loss_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV Params\n",
    "TIME_SCALE = 10\n",
    "\n",
    "# PPO Params\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "EPSILON = 0.2\n",
    "ENTROPY_COEF = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"hidden_dim\": 256,\n",
    "    \"n_blocks\": 3,\n",
    "    \"in_features\": OBSERVATION_DIM,\n",
    "    \"out_features\": ACTION_DIM,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection Phase Params\n",
    "WORKERS = os.cpu_count() // 2\n",
    "STORAGE_DEVICE = \"cpu\"\n",
    "GENERATION_SIZE = round_up(64_000, WORKERS*N_ENVS)\n",
    "GENERATIONS = 200\n",
    "COLLECTOR_BUFFER_SIZE = round_up(WORKERS*N_ENVS*128)\n",
    "\n",
    "# Advantage Phase Params\n",
    "SLICE_LEN = 128 # GAE Window\n",
    "ADV_MINIBATCH_SIZE = round_up(10_000, SLICE_LEN)\n",
    "\n",
    "# Train Phase Params\n",
    "EPOCHS = 2\n",
    "MINIBATCH_SIZE = 1024\n",
    "LR = 3e-4\n",
    "MAX_GRAD_NORM = 0.5\n",
    "KL_TARGET = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intervals\n",
    "LOG_INTERVAL = 1\n",
    "CHECKPOINT_INTERVAL = 1\n",
    "\n",
    "# Run Info\n",
    "NAME = 'run0'\n",
    "CONTINUE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers = 7 \n",
      "parallel envs = 70 \n",
      "generation_size = 64050 \n",
      "generations = 200 \n",
      "timesteps = 12810000 \n",
      "device = cpu \n"
     ]
    }
   ],
   "source": [
    "def summary():\n",
    "    s = [\n",
    "        (\"workers\", WORKERS), (\"parallel envs\", WORKERS*N_ENVS),\n",
    "        (\"generation_size\", GENERATION_SIZE), (\"generations\", GENERATIONS), (\"timesteps\", GENERATIONS*GENERATION_SIZE),\n",
    "        (\"device\", device),\n",
    "    ]\n",
    "    for key, value in s:\n",
    "        print(f\"{key} = {value} \")\n",
    "summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-main=262144\"\n",
      "    \"memorysetup-allocator-temp-initial-block-size-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "Mono path[0] = '/Users/walimirza/unity-rl/envs/Crawler.app/Contents/Resources/Data/Managed'\n",
      "Mono config path = '/Users/walimirza/unity-rl/envs/Crawler.app/Contents/MonoBleedingEdge/etc'\n",
      "[Physics::Module] Initialized MultithreadedJobDispatcher with 13 workers.\n",
      "New input system (experimental) initialized\n",
      "Initialize engine version: 2023.2.13f1 (70197a359f36)\n",
      "[Subsystems] Discovering subsystems at path /Users/walimirza/unity-rl/envs/Crawler.app/Contents/Resources/Data/UnitySubsystems\n",
      "Forcing GfxDevice: Null\n",
      "GfxDevice: creating device client; kGfxThreadingModeNonThreaded\n",
      "NullGfxDevice:\n",
      "    Version:  NULL 1.0 [1.0]\n",
      "    Renderer: Null Device\n",
      "    Vendor:   Unity Technologies\n",
      "Begin MonoManager ReloadAssembly\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Loaded All Assemblies, in  0.051 seconds\n",
      "icall.c:1842:\n",
      "icall.c:1842:\n",
      "- Finished resetting the current domain, in  0.001 seconds\n",
      "ERROR: Shader Sprites/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Sprites/Mask shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "ERROR: Shader Legacy Shaders/VertexLit shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Legacy Shaders/Diffuse shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Legacy Shaders/Diffuse' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader ML-Agents/GridPattern shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'ML-Agents/GridPattern' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Standard shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Standard' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "ERROR: Shader Autodesk Interactive shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n",
      "WARNING: Shader Unsupported: 'Autodesk Interactive' - All subshaders removed\n",
      "WARNING: Shader Did you use #pragma only_renderers and omit this platform?\n",
      "WARNING: Shader If subshaders removal was intentional, you may have forgotten turning Fallback off?\n",
      "UnloadTime: 0.276583 ms\n",
      "Registered Communicator in Agent.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/transforms/transforms.py:587: FutureWarning: The key 'continuous_action' is unaccounted for by the transform (expected keys ['PhysicsBodySensor:Body', 'VectorSensor_size32', 'done', 'terminated', 'truncated', 'group_reward', 'reward']). Every new entry in the tensordict resulting from a call to a transform must be registered in the specs for torchrl rollouts to be consistently built. Make sure transform_output_spec/transform_observation_spec/... is coded correctly. This warning will trigger a KeyError in v0.9, make sure to adapt your code accordingly.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlagents3/lib/python3.10/site-packages/torchrl/envs/libs/unity_mlagents.py:373: DeprecationWarning: In future, it will be an error for 'np.bool' scalars to be interpreted as an index\n",
      "  source[group_name][agent_name][\"truncated\"] = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Shader UI/Default shader is not supported on this GPU (none of subshaders/fallbacks are suitable)\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = create_env(time_scale=5)\n",
    "\n",
    "policy, value = create_policy(MODEL_CONFIG).to(device), create_value(MODEL_CONFIG).to(device)\n",
    "loss_module = make_loss_module(policy, value, epsilon=0.1, entropy_coef=0.01, gamma=0.99, lmbda=0.95).to(device)\n",
    "\n",
    "data = None\n",
    "with torch.no_grad():\n",
    "    data = env.rollout(20, policy=policy, auto_cast_to_device=True, break_when_any_done=False).to(device)\n",
    "    env.close()\n",
    "    loss_module.value_estimator(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([10, 20, 20]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        advantage: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        loc: Tensor(shape=torch.Size([10, 20, 20]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        log_prob: Tensor(shape=torch.Size([10, 20]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([10, 20, 158]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                state_value: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                truncated: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([10, 20]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([10, 20, 158]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        scale: Tensor(shape=torch.Size([10, 20, 20]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        state_value: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        truncated: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        value_target: Tensor(shape=torch.Size([10, 20, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "    batch_size=torch.Size([10, 20]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1240315437316895, 18.18181800842285)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_data = split_trajectories(data)\n",
    "compute_traj_reward_info(traj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(create_env, policy, value, generations=GENERATIONS):\n",
    "    # Loss + Optimizer\n",
    "    loss_module = make_loss_module(policy, value, epsilon=EPSILON, entropy_coef=ENTROPY_COEF, gamma=GAMMA, lmbda=GAE_LAMBDA)\n",
    "    optimizer = optim.Adam(loss_module.parameters(), lr=LR)\n",
    "    scaler = torch.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
    "\n",
    "    # Logger + Checkpointer\n",
    "    logger = Logger(keys = LOG_KEYS, log_path=LOG_PATH, name=NAME)\n",
    "    checkpointer = Checkpointer(ckpt_path=CKPT_PATH, name=NAME)\n",
    "\n",
    "    # Continue/Reset\n",
    "    start_generation = 0\n",
    "    if not CONTINUE:\n",
    "        logger.reset()\n",
    "        checkpointer.reset()\n",
    "    else:\n",
    "        checkpoint = checkpointer.load_progress()\n",
    "        if checkpoint:\n",
    "            start_generation = int(checkpoint[\"generation\"])\n",
    "            policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "            value.load_state_dict(checkpoint[\"value_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            if \"scaler_state_dict\" in checkpoint:\n",
    "                scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "            print(\"CHECKPOINT FOUND, STARTING FROM GENERATION:\", start_generation)\n",
    "        else:\n",
    "            print(\"CHECKPOINT NOT FOUND, STARTING FROM SCRATCH\")\n",
    "\n",
    "    # Watches\n",
    "    short_watch = Stopwatch()\n",
    "    long_watch = Stopwatch()\n",
    "\n",
    "    # Replay Buffers\n",
    "    collect_replay_buffer = ReplayBuffer(\n",
    "        storage=LazyMemmapStorage(GENERATION_SIZE, device=STORAGE_DEVICE, ndim=2 + int(WORKERS > 1)),\n",
    "        sampler=SliceSamplerWithoutReplacement(\n",
    "            slice_len = SLICE_LEN,\n",
    "            shuffle=False, strict_length=False, \n",
    "            end_key=(\"next\", \"done\")\n",
    "        ),\n",
    "        batch_size=ADV_MINIBATCH_SIZE,\n",
    "    )\n",
    "    train_replay_buffer = ReplayBuffer(storage=LazyMemmapStorage(GENERATION_SIZE, device=STORAGE_DEVICE), sampler=SamplerWithoutReplacement(), batch_size=MINIBATCH_SIZE)\n",
    "\n",
    "    # Collectors\n",
    "    if WORKERS > 1:\n",
    "        collector = MultiSyncDataCollector([create_env]*WORKERS, policy, \n",
    "            frames_per_batch=COLLECTOR_BUFFER_SIZE, \n",
    "            total_frames=GENERATION_SIZE*(generations - start_generation), \n",
    "            env_device=\"cpu\", device=device, storing_device=STORAGE_DEVICE, \n",
    "            update_at_each_batch=False, # Manually update\n",
    "        )\n",
    "    else:\n",
    "        collector = SyncDataCollector(\n",
    "            create_env, policy, \n",
    "            frames_per_batch=COLLECTOR_BUFFER_SIZE, \n",
    "            total_frames=GENERATION_SIZE*(generations - start_generation), \n",
    "            env_device=\"cpu\", device=device, storing_device=STORAGE_DEVICE,\n",
    "        )\n",
    "\n",
    "\n",
    "    collector_iters_per_gen = int(np.ceil(GENERATION_SIZE / COLLECTOR_BUFFER_SIZE))\n",
    "    long_watch.start()\n",
    "\n",
    "    ### TRAINING LOOP\n",
    "    for i in range(start_generation, generations):\n",
    "        # 1. COLLECT TRAJECTORY DATASET\n",
    "        policy.eval(); value.eval()\n",
    "        if WORKERS > 1: collector.update_policy_weight_()\n",
    "\n",
    "        short_watch.start(); \n",
    "        collect_replay_buffer.empty()\n",
    "        # Buffer in memory then move to memory mapped storage in loop\n",
    "        for j in range(collector_iters_per_gen):\n",
    "            data = collector.next()\n",
    "            collect_replay_buffer.extend(data)\n",
    "        \n",
    "        collection_time = short_watch.end()\n",
    "        logger.add({\"collection_time\": collection_time})\n",
    "\n",
    "\n",
    "        # 2. Compute Advantages, Value Target, and Metrics (Iterate Along Trajectories)\n",
    "        train_replay_buffer.empty()\n",
    "        for j, batch in enumerate(collect_replay_buffer):\n",
    "            batch = batch.to(device)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                loss_module.value_estimator(batch)\n",
    "                metrics = compute_trajectory_metrics(split_trajectories(batch))\n",
    "            \n",
    "            logger.acc(metrics, mode='avg')\n",
    "            train_replay_buffer.extend(batch.reshape(-1).cpu())\n",
    "        collect_replay_buffer.empty() # A bit inefficient to only delete here\n",
    "\n",
    "        # 3. Minibatch Gradient Descent Loop (Iterate along random timesteps)\n",
    "        short_watch.start()\n",
    "        policy.train(); value.train()\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            for j, batch in enumerate(train_replay_buffer):\n",
    "                # 2.1 Optimization Step\n",
    "                batch = batch.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=amp_dtype, enabled=(amp_dtype==torch.float16)):\n",
    "                    loss_data = loss_module(batch)\n",
    "                    loss = loss_data[\"loss_objective\"] + loss_data[\"loss_critic\"] + loss_data[\"loss_entropy\"]\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(loss_module.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # 2.2 Accumulate Metric\n",
    "                weight = float(batch.batch_size[0])\n",
    "                logger.acc(ppo_loss_td_to_dict(loss_data, weight), mode='ema')\n",
    "\n",
    "        train_replay_buffer.empty()\n",
    "        policy.eval(); value.eval()\n",
    "        train_time = short_watch.end()\n",
    "        logger.add({\"train_time\": train_time})\n",
    "\n",
    "        # 4. Log results\n",
    "        logger.add({\"generation\": 1})\n",
    "        logger.add({\"timestep\": GENERATION_SIZE})\n",
    "        if (i % LOG_INTERVAL) == 0:\n",
    "            logger.add({\"time\": long_watch.end()})\n",
    "            long_watch.start()\n",
    "            logger.next(print_row=True)\n",
    "        \n",
    "        # 5. Checkpoint model\n",
    "        if (i % CHECKPOINT_INTERVAL) == 0:\n",
    "            gen = i + 1\n",
    "            metric = metrics[BEST_METRIC_KEY]\n",
    "            checkpointer.save_progress(metric_key=BEST_METRIC_KEY,\n",
    "            state_obj={\n",
    "                \"generation\": gen,\n",
    "                \"policy_state_dict\": policy.state_dict(),\n",
    "                \"value_state_dict\": value.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scaler_state_dict\": scaler.state_dict(),\n",
    "                BEST_METRIC_KEY: metric,\n",
    "            })\n",
    "\n",
    "    checkpointer.copy_model('latest', MODEL_PATH, ('policy_state_dict', 'value_state_dict'))\n",
    "    return logger.dataframe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
