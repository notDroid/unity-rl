{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "# Common Util\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment\n",
    "from env import create_env\n",
    "\n",
    "# Model\n",
    "from rlkit.models import MLP\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torchrl.modules import ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Training\n",
    "from rlkit.util import Checkpointer, Logger, Stopwatch, round_up\n",
    "\n",
    "# Config\n",
    "from config import (\n",
    "    ENV_PATH, \n",
    "    N_ENVS, OBSERVATION_DIM, ACTION_DIM,\n",
    "    LOG_KEYS, LOG_INDEX, BEST_METRIC_KEY,\n",
    "    MODEL_PATH, CKPT_PATH, LOG_PATH, RESULTS_PATH,\n",
    ")\n",
    "\n",
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_specs(env):\n",
    "    print(\"action_spec:\", env.action_spec)\n",
    "    print(\"reward_spec:\", env.reward_spec)\n",
    "    print(\"done_spec:\", env.done_spec)\n",
    "    print(\"observation_spec:\", env.observation_spec)\n",
    "\n",
    "# env = create_env(graphics=False, time_scale=5)\n",
    "# td = env.rollout(1000, break_when_any_done=False)\n",
    "# print_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_data = pd.DataFrame(td[\"action\"].reshape(-1))\n",
    "# obs_data = pd.DataFrame(td[\"observation\"].reshape(-1))\n",
    "# obs_data = obs_data.clip(float(obs_data.quantile(0.01).iloc[0]), float(obs_data.quantile(0.99).iloc[0]))\n",
    "# rew_data = pd.DataFrame(td[\"next\", \"reward\"].reshape(-1))\n",
    "\n",
    "# plt.violinplot(obs_data, positions=[0], showmedians=True, showextrema=True, widths=0.9)\n",
    "# plt.violinplot(act_data, positions=[1], showmedians=True, showextrema=True, widths=0.9)\n",
    "# plt.violinplot(rew_data, positions=[2], showmedians=True, showextrema=True, widths=0.9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_policy(model_config):\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"out_features\"] *= 2\n",
    "    model = MLP(**model_config)\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        model,\n",
    "        NormalParamExtractor()\n",
    "    )\n",
    "    model = TensorDictModule(model, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"])\n",
    "    \n",
    "    policy = ProbabilisticActor(\n",
    "        module=model,  \n",
    "        distribution_class=TanhNormal,\n",
    "\n",
    "        in_keys=[\"loc\", \"scale\"],\n",
    "        out_keys=[\"action\"],\n",
    "\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=\"log_prob\",\n",
    "        cache_dist=True,\n",
    "    )\n",
    "\n",
    "    return policy\n",
    "\n",
    "def create_value(model_config):\n",
    "    # Remove out_features from config\n",
    "    model_config = model_config.copy()\n",
    "    model_config[\"out_features\"] = 1\n",
    "\n",
    "    model = MLP(**model_config)\n",
    "    value = TensorDictModule(model, in_keys=[\"observation\"], out_keys=[\"state_value\"])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"hidden_dim\": 256,\n",
    "    \"n_blocks\": 3,\n",
    "    \"in_features\": OBSERVATION_DIM,\n",
    "    \"out_features\": ACTION_DIM,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMP + Scaler?\n",
    "device_type = \"cuda\" if str(device).startswith(\"cuda\") else \"cpu\"\n",
    "amp_dtype   = torch.float16 if device_type == \"cuda\" else torch.float32\n",
    "\n",
    "### Training Loop Params\n",
    "WORKERS = os.cpu_count() // 2\n",
    "STORAGE_DEVICE = \"cpu\"\n",
    "GENERATION_SIZE = round_up(64_000, WORKERS*N_ENVS)\n",
    "GENERATIONS = 200\n",
    "COLLECTOR_BUFFER_SIZE = round_up(WORKERS*N_ENVS*128)\n",
    "\n",
    "# Advantage Comp Params\n",
    "SLICE_LEN = 128 # GAE Window\n",
    "ADV_MINIBATCH_SIZE = round_up(10_000, SLICE_LEN)\n",
    "\n",
    "# GD Params\n",
    "EPOCHS = 2\n",
    "MINIBATCH_SIZE = 1024\n",
    "LR = 3e-4\n",
    "MAX_GRAD_NORM = 0.5\n",
    "KL_TARGET = 0.01\n",
    "\n",
    "### RL Params\n",
    "\n",
    "# ENV Params\n",
    "TIME_SCALE = 10\n",
    "\n",
    "# PPO Params\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "EPSILON = 0.2\n",
    "ENTROPY_COEF = 1e-4\n",
    "\n",
    "NAME = 'run0'\n",
    "\n",
    "LOG_INTERVAL = 1\n",
    "CHECKPOINT_INTERVAL = 1\n",
    "\n",
    "CONTINUE=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers = 7 \n",
      "parallel envs = 70 \n",
      "generation_size = 64050 \n",
      "generations = 200 \n",
      "timesteps = 12810000 \n",
      "device = cpu \n"
     ]
    }
   ],
   "source": [
    "def summary():\n",
    "    s = [\n",
    "        (\"workers\", WORKERS), (\"parallel envs\", WORKERS*N_ENVS),\n",
    "        (\"generation_size\", GENERATION_SIZE), (\"generations\", GENERATIONS), (\"timesteps\", GENERATIONS*GENERATION_SIZE),\n",
    "        (\"device\", device),\n",
    "    ]\n",
    "    for key, value in s:\n",
    "        print(f\"{key} = {value} \")\n",
    "summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(create_env, policy, value, generations=GENERATIONS):\n",
    "    # Loss + Optimizer\n",
    "    loss_module = make_loss_module(policy, value, epsilon=EPSILON, entropy_coef=ENTROPY_COEF, gamma=GAMMA, lmbda=GAE_LAMBDA)\n",
    "    optimizer = optim.Adam(loss_module.parameters(), lr=LR)\n",
    "    # only need scaler with float16, float32 and bfloat16 have wider exponent ranges.\n",
    "    scaler = torch.amp.GradScaler(enabled=(amp_dtype == torch.float16))\n",
    "\n",
    "    # Logger + Checkpointer\n",
    "    logger = Logger(keys = LOG_KEYS, log_path=LOG_PATH, name=NAME)\n",
    "    checkpointer = Checkpointer(ckpt_path=CKPT_PATH, name=NAME)\n",
    "\n",
    "    # Continue/Reset\n",
    "    start_generation = 0\n",
    "    if not CONTINUE:\n",
    "        logger.reset()\n",
    "        checkpointer.reset()\n",
    "    else:\n",
    "        checkpoint = checkpointer.load_progress()\n",
    "        if checkpoint:\n",
    "            start_generation = int(checkpoint[\"generation\"])\n",
    "            policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "            value.load_state_dict(checkpoint[\"value_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            if \"scaler_state_dict\" in checkpoint:\n",
    "                scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "            print(\"CHECKPOINT FOUND, STARTING FROM GENERATION:\", start_generation)\n",
    "        else:\n",
    "            print(\"CHECKPOINT NOT FOUND, STARTING FROM SCRATCH\")\n",
    "\n",
    "    # Watches\n",
    "    short_watch = Stopwatch()\n",
    "    long_watch = Stopwatch()\n",
    "\n",
    "    # Replay Buffers\n",
    "    collect_replay_buffer = ReplayBuffer(\n",
    "        storage=LazyMemmapStorage(GENERATION_SIZE, device=STORAGE_DEVICE, ndim=2 + int(WORKERS > 1)),\n",
    "        sampler=SliceSamplerWithoutReplacement(\n",
    "            slice_len = SLICE_LEN,\n",
    "            shuffle=False, strict_length=False, \n",
    "            end_key=(\"next\", \"done\")\n",
    "        ),\n",
    "        batch_size=ADV_MINIBATCH_SIZE,\n",
    "    )\n",
    "    train_replay_buffer = ReplayBuffer(storage=LazyMemmapStorage(GENERATION_SIZE, device=STORAGE_DEVICE), sampler=SamplerWithoutReplacement(), batch_size=MINIBATCH_SIZE)\n",
    "\n",
    "    # Collectors\n",
    "    if WORKERS > 1:\n",
    "        collector = MultiSyncDataCollector([create_env]*WORKERS, policy, \n",
    "            frames_per_batch=COLLECTOR_BUFFER_SIZE, \n",
    "            total_frames=GENERATION_SIZE*(generations - start_generation), \n",
    "            env_device=\"cpu\", device=device, storing_device=STORAGE_DEVICE, \n",
    "            update_at_each_batch=True,\n",
    "        )\n",
    "    else:\n",
    "        collector = SyncDataCollector(\n",
    "            create_env, policy, \n",
    "            frames_per_batch=COLLECTOR_BUFFER_SIZE, \n",
    "            total_frames=GENERATION_SIZE*(generations - start_generation), \n",
    "            env_device=\"cpu\", device=device, storing_device=STORAGE_DEVICE,\n",
    "        )\n",
    "\n",
    "\n",
    "    collector_iters_per_gen = int(np.ceil(GENERATION_SIZE / COLLECTOR_BUFFER_SIZE))\n",
    "    long_watch.start()\n",
    "\n",
    "    ### TRAINING LOOP\n",
    "    for i in range(start_generation, generations):\n",
    "        # 1. COLLECT TRAJECTORY DATASET\n",
    "        policy.eval(); value.eval()\n",
    "        short_watch.start(); \n",
    "        collect_replay_buffer.empty()\n",
    "\n",
    "        # Buffer in memory then move to memory mapped storage in loop\n",
    "        for j in range(collector_iters_per_gen):\n",
    "            data = collector.next()\n",
    "            collect_replay_buffer.extend(data)\n",
    "        \n",
    "        collection_time = short_watch.end()\n",
    "        logger.sum({\"collection_time\": collection_time})\n",
    "\n",
    "\n",
    "        # 2. Compute Advantages, Value Target, and Metrics (Iterate Along Trajectories)\n",
    "        train_replay_buffer.empty()\n",
    "        for j, batch in enumerate(collect_replay_buffer):\n",
    "            batch = batch.to(device)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                loss_module.value_estimator(batch)\n",
    "                metrics = compute_trajectory_metrics(split_trajectories(batch))\n",
    "            \n",
    "            logger.accumulate(metrics)\n",
    "            train_replay_buffer.extend(batch.reshape(-1).cpu())\n",
    "        collect_replay_buffer.empty() # A bit inefficient to only delete here\n",
    "\n",
    "        # 3. Minibatch Gradient Descent Loop (Iterate along random timesteps)\n",
    "        short_watch.start()\n",
    "        policy.train(); value.train()\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            for j, batch in enumerate(train_replay_buffer):\n",
    "                # 2.1 Optimization Step\n",
    "                batch = batch.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=amp_dtype, enabled=(amp_dtype==torch.float16)):\n",
    "                    loss_data = loss_module(batch)\n",
    "                    loss = loss_data[\"loss_objective\"] + loss_data[\"loss_critic\"] + loss_data[\"loss_entropy\"]\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(loss_module.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # 2.2 Accumulate Metric\n",
    "                weight = float(batch.batch_size[0])\n",
    "                logger.accumulate(loss_dict(loss_data, weight))\n",
    "\n",
    "                # Check for end\n",
    "                if logger.last()[\"kl_target\"] > KL_TARGET: break\n",
    "            if logger.last()[\"kl_target\"] > KL_TARGET: break\n",
    "\n",
    "        train_replay_buffer.empty()\n",
    "        policy.eval(); value.eval()\n",
    "        train_time = short_watch.end()\n",
    "        logger.sum({\"train_time\": train_time})\n",
    "\n",
    "        # 4. Log results\n",
    "        logger.sum({\"generation\": 1})\n",
    "        logger.sum({\"timestep\": GENERATION_SIZE})\n",
    "        if (i % LOG_INTERVAL) == 0:\n",
    "            logger.sum({\"time\": long_watch.end()})\n",
    "            long_watch.start()\n",
    "            logger.next(print_row=True)\n",
    "        \n",
    "        # 5. Checkpoint model\n",
    "        if (i % CHECKPOINT_INTERVAL) == 0:\n",
    "            gen = i + 1\n",
    "            metric = metrics[BEST_METRIC_KEY]\n",
    "            checkpointer.save_progress(metric_key=BEST_METRIC_KEY,\n",
    "            state_obj={\n",
    "                \"generation\": gen,\n",
    "                \"policy_state_dict\": policy.state_dict(),\n",
    "                \"value_state_dict\": value.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scaler_state_dict\": scaler.state_dict(),\n",
    "                BEST_METRIC_KEY: metric,\n",
    "            })\n",
    "\n",
    "    checkpointer.copy_model('latest', MODEL_PATH, ('policy_state_dict', 'value_state_dict'))\n",
    "    return logger.dataframe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
